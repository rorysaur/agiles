Working solo on your own project—"scratching your own itch"—can be a lot of fun. 
There are no questions about which features to work on, how things ought to work, if the software works correctly, or whether stakeholders are happy. 
All the answers are right there in one brain.Team software development is different. 
The same information is spread out among many members of the team. 
Different people know:All of this knowledge is necessary for success. 
XP acknowledges this reality by creating cross-functional teams composed of diverse people who can fulfill all of the team's roles.XP teams sit together in an open workspace. 
At the beginning of each iteration, the team meets for a series of activities: an iteration demo, a retrospective, and iteration planning. 
These typically take two to four hours in total. 
The team also meets for daily stand-up meetings, which usually take five to ten minutes each.Other than these scheduled activities, everyone on the team plans his own work. 
That doesn't mean everybody works independently; they just aren't on an explicit schedule. 
Team members work out the details of each meeting when they need to. 
Sometimes it's as informal as somebody standing up and announcing across the shared workspace that he would like to discuss an issue. 
This self-organization is a hallmark of agile teams.On-site customers—often just called customers—are responsible for defining the software the team builds. 
The rest of the team can and should contribute suggestions and ideas, but the customers are ultimately responsible for determining what stakeholders will find valuable.Customers' most important activity is release planning. 
This is a multi-faceted activity. 
Customers need to evangelize the project's vision; identify features and stories; determine how to group features into small, frequent releases; manage risks; and create an achievable plan by coordinating with programmers and playing the planning game.On-site customers may or may not be real customers, depending on the type of project. 
Regardless, customers are responsible for refining their plans by soliciting feedback from real customers and other stakeholders. 
One of the venues for this feedback is the weekly iteration demo, which customers lead.In addition to planning, customers are responsible for providing programmers with requirements details upon request. 
XP uses requirements documents only as memory aids for customers. 
Customers themselves act as living requirements documents, researching information in time for programmer use and providing it as needed. 
Customers also help communicate requirements by creating mock-ups, reviewing work-in-progress and creating detailed customer tests that clarify complex business rules. 
The entire team must sit together for this communication to take place effectively.Typically, product managers, domain experts, interaction designers, and business analysts play the role of the on-site customer. 
One of the most difficult aspects of creating a cross-functional team is finding people qualified and willing to be on-site customers. 
Don't neglect this role; it's essential to increasing the value of the product you deliver. 
A great team will produce technically excellent software without on-site customers, but to truly succeed, your software must also bring value to its investors. 
This requires the perspective of on-site customers.[Coffin] describes an experience with two nearly identical teams, one that did not have on-site customers and one that did. 
The team without on-site customers took fifteen months to produce a product with mediocre value:A team composed of many of the same developers, at the same company, using the same process, later produced a product with compelling value in less than three months:One of the primary reasons for this success was customer involvement.Customer involvement makes a huge difference in product success. 
Make an extra effort to include customers. 
One way to do so is to move to their offices rather than asking them to move to your office. 
Make sure the customers agree and that there's adequate space available.The product manager only has one job on an XP project, but it's a doozy. 
That job is to maintain and promote the product vision. 
In practice, that means documenting the vision, sharing it with stakeholders, incorporating feedback, generating features and stories, setting priorities for release planning, providing direction for the team's on-site customers, reviewing work in progress, leading iteration demos, involving real customers, and dealing with organizational politics.The best product managers have deep understandings of their markets, whether the market is one organization (as with custom software) or many (as with commercial software). 
Good product managers have an intuitive understanding of what the software will provide and why it's the most important thing their project teams can do with their time.A great product manager also has a rare combination of skills. 
In addition to vision, she must have the authority to make difficult trade-off decisions about what goes into the product and what stays out. 
She must have the political savvy to align diverse stakeholder interests, consolidate them into the product vision, and to effectively say "no" to wishes that can't be accommodated.Product managers of this caliber often have a lot of demands on their time. 
You may have trouble getting enough attention. 
Persevere. 
Theirs is one of the most crucial roles on the team. 
Enlist the help of your project manager and remind people that software development is very expensive. 
If the software isn't valuable enough to warrant the time of a good product manager—a product manager who could mean the difference between success and failure—perhaps it isn't worth developing in the first place.Make sure your product manager is committed to the project full-time. 
Once a team is running smoothly, the product manager might start cutting back on his participation. 
Although domain experts and other on-site customers can fill in for the product manager for a time, the project is likely to start drifting off-course unless the product manager participates in every iteration. 
[Rooney] experienced that problem, with regrettable results:In a predictable environment, and by delegating to a solid set of on-site customers, a product manager might be able to spend most of her time on other things, but she should still participate in every retrospective, iteration demo, and most release planning sessions.Some companies have a committee play the role of product manager, but I advise against this approach. 
The team needs a consistent vision to follow, and I've found that committees have trouble creating consistent, compelling visions. 
When I've seen committees succeed, it's been because one committee member acted as de facto product manager. 
I recommend that you explicitly find a product manager. 
Her role may be nothing more than consolidating the ideas of the committee into a single vision, and that's likely to keep her hands full. 
Be sure to choose a product manager with plenty of political acumen in this case.Most software operates in a particular industry, such as finance, that has its own specialized rules for doing business. 
To succeed in that industry, the software must implement those rules faithfully and exactly. 
These rules are domain rules and knowledge of these rules is domain knowledge.Most programmers have gaps in their domain knowledge, even if they've worked in an industry for years. 
In many cases, the industry itself doesn't clearly define all its rules. 
The basics may be clear, but there are nitpicky details where domain rules are implicit or even contradictory.The team's domain experts are responsible for figuring out these details and having the answers at their fingertips. 
Domain experts, also known as subject matter experts, are experts in their field. 
Examples include financial analysts and PhD chemists.Domain experts spend most of their time with the team, figuring out the details of upcoming stories and standing ready to answer questions when programmers ask. 
For complex rules, they create customer tests (often with the help of testers) to help convey nuances.The user interface is the public face of the product. 
For many users, the UI is the product. 
They judge the product's quality solely on their perception of the UI.Interaction designers help define the product UI. 
Their job focuses on understanding users, their needs, and how they will interact with the product. 
They perform such tasks as interviewing users, creating user personas, reviewing paper prototypes with users, and observing usage of actual software.You may not have a professional interaction designer on staff. 
Some companies fill this role with a graphic designer, the product manager, or a programmer.Interaction designers divide their time between working with the team and working with users. 
They contribute to release planning by advising the team on user needs and priorities. 
During each iteration, they help the team create mock-ups of UI elements for that iteration's stories. 
As each story approaches completion, they review the look and feel of the UI and confirm that it works as they expected.The fast, iterative, feedback-oriented nature of XP development leads to a different environment than interaction designers may be used to. 
Rather than spending time researching users and defining behaviors before development begins, interactions designers must iteratively refine their models concurrently with iterative refinement of the program itself.Although interaction design is different under XP than in other methods, it is not necessarily diminished. XP produces working software every week, which provides a rich grist for the interaction designer's mill. 
Designers have the opportunity to take real software to users, observe their usage patterns, and use that feedback to effect changes as soon as one week later.On non agile teams, business analysts typically act as liaisons between the customers and developers, by clarifying and refining customer needs into a functional requirements specification.On an XP team, business analysts augment a team that already contains a product manager and domain experts. 
The analyst continues to clarify and refine customer needs, but the analyst does so in support of the other on-site customers, not as a replacement for them. 
Analysts help customers think of details they might otherwise forget and help programmers express technical tradeoffs in business terms.A great product vision requires solid execution. 
The bulk of the XP team consists of software developers in a variety of specialties. 
Each of these developers contributes directly to creating working code. 
To emphasize this, XP calls all developers programmers.If the customers' job is to maximize the value of the product, then the programmers' job is to minimize its cost. 
Programmers are responsible for finding the most effective way of delivering the stories in the plan. 
To this end, programmers provide effort estimates, suggest alternatives, and help customers create an achievable plan by playing the planning game.Programmers spend most of their time pair programming. 
Using test-driven development, they write tests, implement code, refactor, and incrementally design and architect the application. 
They pay careful attention to design quality, and they're keenly aware of technical debt (for an explanation of technical debt, see XP Concepts later in this chapter) and its impact on development time and future maintenance costs.Programmers also ensure that the customers can choose release the software at the end of any iteration. 
With the help of the whole team, the programmers strive to produce no bugs in completed software. 
They maintain a ten-minute build that can build a complete release package at any time. 
They use version control and practice continuous integration, keeping all but the most recent few hours'  work integrated and passing its tests.This work is a joint effort of all of the programmers. 
At the beginning of the project, the programmers establish coding standards that allow them to collectively share responsibility for the code. 
Programmers have the right and responsibility to fix any problems they see, no matter which part of the application it touches.Programmers rely on customers for information about the software to build. 
Rather than guessing when they have a question, they ask one of the on-site customers. 
To enable these conversations, programmers build their software to use a ubiquitous language. 
They assist in customer testing by automating the customers' examples.Finally, programmers provide for the long-term maintainability of the product by providing documentation at appropriate times.Everybody codes on an XP team, and everybody designs. 
Test-driven development combines design, tests, and coding into a single, ongoing activity.Expert designers and architects are still necessary. 
They contribute by guiding the team's incremental design and architecture efforts and helping team members see ways of simplifying complex designs. 
They act as peers—that is, as programmers—rather than teachers, guiding rather dictating.In addition to the obvious titles (programmer, developer, software engineer), the XP "programmer" role includes other software development roles. 
The programmers could include a database designer, a security expert, or a network architect. 
XP programmers are generalizing specialists. 
While each person has his own area of expertise, everybody is expected to work on any part of the system that needs attention. 
(See Collective Code Ownership in Chapter 7 for more.)Testers help XP teams produce quality results from the beginning. 
Testers apply their critical thinking skills to help customers consider all possibilities when envisioning the product. 
They help customers identify holes in the requirements and assist in customer testing.11This discussion of tester responsibilities is part of my variant of XP (see the sidebar "A Little Lie," earlier in this chapter). 
Classic XP doesn't include testers as a distinct role.Testers also act as technical investigators for the team. 
They use exploratory testing to help the team identify whether it is successfully preventing bugs from reaching finished code. 
Testers also provide information about the software's nonfunctional characteristics, such as performance, scalability, and stability, by using both exploratory testing and long-running automated tests.However, testers don't exhaustively test the software for bugs. 
Rather than relying on testers to find bugs for programmers to fix, the team should produce nearly bug-free code on their own. 
When testers find bugs, they help the rest of team figure out what went wrong so that the team as a whole can prevent those kinds of bugs from occurring in the future.These responsibilities require creative thinking, flexibility, and experience defining test plans. 
Because XP automates repetitive testing rather than performing manual regression testing, testers who are used to self-directed work are the best fit.Some XP teams don't include dedicated testers. 
If you don't have testers on your team, programmers and customers should share this role.XP teams self-organize, which means each member of the team figures out how he can best help the team move forward at any given moment. 
XP teams eschew traditional management roles.Instead, leaders lead by example, helping the team reach its potential rather than creating jobs and assigning tasks. 
To emphasize this difference, XP leaders are called coaches. 
Over time, as the team gains experience and self-organizes, explicit leadership becomes less necessary and leadership roles dynamically switch from person to person as situations dictate.A coach's work is subtle; it enables the team to succeed. 
Coaches help the team start their process well by arranging for a shared workspace and making sure that the team includes the right people. 
They help set up conditions for energized work, and they assist the team in creating an informative workspace.One of the most important things the coaches can do is to help the team interact with the rest of the organization. 
They help the team generate organizational trust and goodwill, and they often take responsibility for any reporting needed.Coaches also help team members remember to maintain their self-discipline, helping them remain in control of challenging practices such as risk management, test-driven development, slack, and incremental design and architecture.Every team needs a programmer-coach to help them with XP's technical practices. 
Programmer-coaches are often senior developers and may have titles such as "technical lead" or "architect". 
They can even be functional managers. 
While some programmer-coaches make good all-around coaches, others require the assistance of a project manager.Programmer-coaches also act as normal programmers and participate fully in software development.Project managers help the team work with the rest of the organization. 
They are usually good at coaching nonprogramming practices. 
Some functional managers fit into this role as well. 
However, most project managers lack the technical expertise to coach XP's programming practices which necessitates the assistance of a programmer-coach.Project managers may also double as customers.The preceding roles are a few of the most common team roles, but this list is by no means comprehensive. 
The absence of a role does not mean the expertise is inappropriate for an XP team; an XP team should include exactly the expertise necessary to complete the project successfully and cost-effectively. 
For example, one team I worked with included a technical writer and an ISO 9001 analyst.Projects don't live in a vaccuum; every team has an ecosystem surrounding it. 
This ecosystem extends beyond the team to the project community, which includes everyone who affects or is affected by the project.2  Keep this community in mind as you begin your XP project, as everybody within it can have an impact on your success.2Thanks to David Schmaltz and Amy Schwartz of True North pgs, Inc., for this term.Two members of your project community that you may forget to consider are your organization's Human Resources and Facilities departments. 
Human Resources often handles performance reviews and compensation. 
Their mechanisms may not be compatible with XP's team-based effort (see Trust in Chapter 6). 
Similarly, in order to use XP, you'll need the help of Facilities to create an open workspace (see Sit Together in Chapter 6).Stakeholders form a large subset of your project community. 
Not only are they affected by your project; they have an active interest in its success. 
Stakeholders may include end users, purchasers, managers, and executives. 
Although they don't participate in day-to-day development, do invite them to attend each iteration demo. 
The on-site customers—particularly the product manager—are responsible for understanding the needs of your stakeholders, deciding which needs are most important, and knowing how to best meet those needs.The executive sponsor is particularly important: he holds the purse strings for your project. 
Take extra care to identify your executive sponsor and understand what he wants from your project. 
He's your ultimate customer. 
Be sure to provide him with regular demos and confirm that the project is proceeding according to his expectations.The exact structure of your team isn't that important as long as it has all the knowledge it needs. 
The makeup of your team will probably depend more on your organization's traditions than on anything else.In other words, if project managers and testers are typical for your organization, include them. 
If they're not, you don't necessarily need to hire them. 
You don't have to have one person for each role—some people can fill multiple roles. 
Just keep in mind that someone has to perform those duties even if no one has a specific job title saying so.At a minimum, however, I prefer to see one person clearly designated as "product manager" (who may do other customer-y things) and one person clearly defined as "programmer-coach" (who also does programmer-y things).The other roles may blend together. 
Product managers are usually domain experts and can often fill the project manager's shoes, too. 
One of the customers may be able to play the role of interaction designer, possibly with the help of a UI programmer. 
On the programming side, many programmers are generalists and understand a variety of technologies. 
In the absence of testers, both programmers and customers should pick up the slack.The guidelines in this book assume teams with four to ten programmers (five to 20 total team members). 
For new teams, four to six programmers is a good starting point.Applying the staffing guidelines to a team of six programmers produces a team that also includes four customers, one tester, and a project manager, for a total team size of 12 people. 
Twelve people turns out to be a natural limit for team collaboration.XP teams can be as small as one experienced programmer and one product manager, but full XP might be overkill for such a small team. 
The smallest team I would use with full XP consists of five people: four programmers (one acting as coach) and one product manager (who also acts as project manager, domain expert, and tester). 
A team of this size might find that the product manager is overburdened; if so, the programmers will have to pitch in. 
Adding a domain expert or tester will help.On the other end of the spectrum, starting with ten programmers produces a 20-person team that includes six customers, three testers, and a project manager. 
You can create even larger XP teams, but they require special practices that are out of the scope of this book.Before you scale your team above twelve people, however, remember that large teams incur extra communication and process overhead, reducing individual productivity. 
The combined overhead might even reduce overall productivity. 
If possible, hire more experienced, more productive team members rather than scaling to a large team.A 20-person team is advanced XP. 
Avoid creating a team of this size until your organization has had extended success with a smaller team. 
If you're working with a team of this size, continuous review, adjustment, and an experienced coach are critical.All of the team members should sit with the team full-time and give the project their complete attention. 
This particularly applies to customers, who are often surprised at the level of involvement XP requires of them.Some organizations like to assign people to multiple projects simultaneously. 
This fractional assignment is particularly common in matrix-managed organizations. 
(If team members have two managers, one for their project and one for their function, you're probably in a matrixed organization.)If your company practices fractional assignment, I have some good news. 
You can instantly improve productivity by reassigning people to only one project at a time. 
Fractional assignment is dreadfully counterproductive: fractional workers don't bond with their teams, they often aren't around to hear conversations and answer questions. and they must task switch, which incurs a significant hidden penalty. 
"[T]he minimum penalty is 15 percent... Fragmented knowledge workers may look busy, but a lot of their busyness is just thrashing."  [DeMarco 2002] (p.19-20)That's not to say that everyone needs to work with the team for the entire duration of the project. 
You can bring someone in to consult on a problem temporarily. 
However, while she works with the team, she should be fully engaged and available.Question that assumption. 
I've helped a wide variety of teams adopt XP: 20-person teams and one-person teams; huge corporations and small startups; shrinkwrap, in-house, and outsourced software vendors; proprietary and open source developers. 
Through these experiences, I've learned that software teams are more similar than they are different. 
XP's applicability has far more to do with your organization and the people involved than the type of project you're working on.The following text is excerpted from The Art of Agile Development by James Shore and Shane Warden, published by O'Reilly. Copyright &copy 2008 the authors. All rights reserved.You can adopt XP in a wide variety of conditions, although the practices you use will vary depending on your situation. 
The practices in this book were chosen to give you the greatest chance of success. 
That leads to some prerequisites and recommendations about your team's environment. 
You don't have to meet these criteria exactly, but it's worth trying to change your environment so that you do. 
That will give you the best chance of succeeding.As Martin Fowler said:11http://martinfowler.com/bliki/EnterpriseRails.html.In other words, if your organization puts a barrier between your work and success, don't just put up with it... find a way to remove it. 
It's your best path to success.Similarly, if you want to practice XP, do everything you can to meet the following prerequisites and recommendations. 
It's a lot more effective than working around limitations.It's very difficult to use XP in the face of opposition from management. 
Active support is best. 
To practice XP as described in this book, you will need the following:You will often need management's help to get the previous three items. 
In addition, the more management provides the following things, the better:In order for management to support your adoption of XP, they need to believe in its benefits. 
Think about what the decision-makers care about. 
What does an organizational success mean to your management?  What does a personal success mean?  How will adopting XP help them achieve those successes?  What are the risks of trying XP, how will you mitigate those risks, and what makes XP worth the risks?  Talk in terms of your managers' idea of success, not your own success.If you have a trusted manager you can turn to, ask for her help and advice. 
If not, talk to your mentor (see "Find a Mentor" in Chapter 2). 
Fearless Change: Patterns for Introducing New Ideas [Manns & Rising] is another good resource.If management refuses your overtures, then XP probably isn't appropriate for your team. 
You may be able to demonstrate XP's value incrementally by adopting some standalone practices (see "Extremeties: Applying Bits and Pieces of XP," later in this chapter).Just as important as management support is the team's agreement to use XP. 
If team members don't want to use XP, it's not likely to work. 
XP assumes good faith on the part of team members—there's no way to force the process on somebody who's resisting it.It's never a good idea to force someone to practice XP against his will. 
In the best case, he'll find some way to leave the team, quitting if necessary. 
In the worst case, he'll remain on the team and silently sabotage your efforts.Reluctant skeptics are okay. 
If somebody says, "I don't want to practice XP, but I see that the rest of you do, so I'll give it a fair chance for a few months," that's fine. 
She may end up liking it. 
If not, after a few months have gone by, you'll have a better idea of what you can do to meet the whole team's needs.If only one or two people refuse to use XP and they're interested in working on another project, let them transfer so the rest of the team can use XP. 
If no such project is available, or if a significant portion of the team is against using XP, don't use it.XP relies on fast, high-bandwidth communication for many of its practices. 
In order to achieve that communication, your team needs to sit together in the same room.Colocation makes a big difference in team effectiveness. 
Don't assume that your team can't sit together; be sure to include the possibility of bringing the team together is your first option.With that said, it's okay if one or two noncentral team members are off-site some of the time. 
You'll be surprised, though, at how much more difficult it is to interact with them. 
(Actually, they're no more difficult to interact with than before; it's the rest of the team that's improved.)  Talk with your mentor about how to best deal with the problem.If a lot of people are off-site, if a central figure is often absent, or if your team is split across multiple locations, you need help beyond this book. 
You can use XP or another agile method with a distributed team, but it's a complicated problem that's outside of the scope of our discussion. 
Ask your mentor for help and see Sit Together in Chapter 6 for more ideas.On-site customers are critical to the success of an XP team. 
They, led by the product manager, determine which features the team will develop. 
In other words, their decisions determine the value of the software.Of the on-site customers, the product manager is likely the most important. 
She makes the final determination of value. 
A good product manager will choose features that provide value to your organization. 
A poor product manager will dither time away on inconsequential features.Domain experts, and possibly interaction designers, are also important. 
They take the place of an upfront requirements phase, sitting with the team to plan upcoming features and answering questions about what the software needs to do.If you have an experienced product manager who makes high-level decisions about features and priorities, but who isn't available to sit with the team full-time, you may be able to ask a business analyst or one of the other on-site customers to act as a proxy. 
The proxy's job is to act in the product manager's stead to make decisions about details while following the actual product manager's high-level decisions.This can work well if your proxy has the authority to act in place of the product manager. 
If the proxy is unable to answer questions on his own and needs to confirm every decision with the real product manager, he will introduce too many delays for this book's approach to XP to work well.This may be okay as long as she has a more experienced colleague she turns to for advice.Although good product managers are in high demand, the absence of a product manager is a big danger sign. 
The right person for the job may not have the title of "product manager" (see Real Customer Involvement in Chapter 6), but XP requires that somebody with business expertise take responsibility for determining and prioritizing features.Remind your organization of the cost of development (presumably, hundreds of thousands of dollars) and the value the software will bring to them (hopefully, millions of dollars). 
That value hinges on the participation of a good product manager. 
Is that really something they want to scrimp on?If you can't find a product manager, someone from the development team can play the part. 
However, this may be a dangerous approach, because this person is unlikely to have the business expertise to deliver an organizational success. 
If you can't get a product manager, talk with your mentor about how to compensate.Because XP doesn't have an up-front requirements phase, the work of figuring out requirements happens concurrently with software development. 
This compresses the overall schedule, but it means that at least one person—and usually several—needs to work on requirements full-time.Unless you have a small team, this work is probably more than a product manager can handle alone. 
Typically, the product manager delegates the details to a set of domain experts. 
In applications that involve a sophisticated user interface, a interaction designer may be involved as well. 
This allows the product manager to focus on coordinating with stakeholders and resolving questions of value and priorities.Some business analysts may be domain experts. 
Be careful of using business analysts that aren't already experts in the domain; although they can relay information from true experts, this process invariably introduces misunderstandings and delays.As long as somebody is playing the on-site customer role, you can use XP. 
However, the less expertise your on-site customers have, the more risk there is to the value of your software.I wrote this book for teams as large as 20 people and as small as one person. 
For teams new to XP, however, I recommend four to six programmers and no more than 12 people on the team (see The XP Team in Chapter 3). 
I also recommend having an even number of programmers so that everyone can pair program (see Pair Programming in Chapter 5). 
If you have ongoing support needs, add one more programmer for a total of five or seven so the team can have a batman (see Iteration Planning in Chapter 8).Teams with fewer than four programmers are less likely to have the intellectual diversity they need. 
They'll also have trouble using pair programming, an important support mechanism in XP. 
Large teams introduce coordination challenges. 
Although experienced teams will handle those challenges smoothly, a new XP team will struggle.The easiest solution is to add or drop one programmer so you have even pairs. 
If you can't do that, the XP practices are still appropriate for you, but try to find useful nonproduction code work for the programmer who isn't pairing. 
This will help the team consistently apply XP's technical practices and will improve code quality.The coordination challenges of a large team can make learning XP more difficult. 
Consider hiring an experienced XP coach to lead the team through the transition. 
You may also benefit from hiring another experienced XP programmer to assist the coach in mentoring the team.If your team is larger than ten programmers, you need guidance that's outside of the scope of this book. 
Hire a coach with experience in scaling XP to large teams.Most of the XP practices are still appropriate for you, but you probably won't be able to pair program much. 
In this situation, it's best if your team members are conscientious programmers who are passionate about producing high-quality code. 
That passion will help them apply the technical practices with discipline.You may have trouble getting on-site customers to sit with you full-time. 
Instead, sit close to them so that you can get their attention when you need it.Some organizations—particularly IT organizations—have a lot of small projects rather than one big project. 
They structure their work to assign one programmer to each project.Although this approach has the advantage of connecting programmers directly with projects, it has several disadvantages. 
It's high risk: every project is the responsibility of one programmer, so that any programmer who leaves orphans a project. 
Her replacement may have to learn it from first principles.Code quality can also be a challenge. 
Projects don't benefit from peer review, so the code is often idiosyncratic. 
Stovepipe systems, in which each programmer solves the same problem in different ways, appear. 
Junior programmers, lacking the guidance of their more senior peers, create convoluted, kludgey systems and have few opportunities to learn better approaches. 
Senior programmers, not realizing the inexperience of their more junior peers, create overly sophisticated code that others have trouble understanding.You may be able to combine four to seven of these programmers into a single XP team that works on one project at a time, which allows it to complete projects more quickly (see Release Planning in Chapter 8). 
By working together, senior developers have the opportunity to mentor junior developers, and the team can eliminate stovepipe systems.Combining your programmers into a single team has some drawbacks. 
The biggest is likely to be a perceived lack of responsiveness. 
Although projects will be finished more quickly, customers will no longer have a dedicated programmer to talk to about the status of their projects. 
The team will only work on one project at a time, so other customers may feel they are being ignored.To resolve these problems, consider dedicating one programmer to deal with customer requests and minor changes (see Iteration Planning in Chapter 8). 
You'll also need an influential, unbiased business person to play the product manager role, addressing conflicts between customers and making prioritization decisions.You may be tempted to ignore or remove some practices, particularly the ones that make team members uncomfortable. 
Be careful of this. 
XP is designed to have very little waste. 
Nearly every practice directly contributes to the production of valuable software.For example, pair programming supports collective code ownership, which is necessary for refactoring. 
Refactoring allows incremental design and architecture. 
Incremental design and architecture enables customer-driven planning and frequent releases, which are the key to XP's ability to increase value and deliver successful software.XP doesn't require perfection—it's okay if you accidentally misapply a practice from time to time—but it rarely works well if you arbitrarily remove pieces.You may feel that some XP practices aren't appropriate for your organization. 
That may be true, but it's possible you just feel uncomfortable or unfamiliar with a practice. 
Are you sure that practice won't work, or do you just not want to do it?  XP will work much better for you if you give all of the practices a fair chance rather than picking and choosing the ones you like.If you're sure a practice won't work, you need to replace it. 
For example, in order to achieve the benefits of collective code ownership without pair programming, you must provide another way for people to share knowledge about the codebase. 
(You'll also have to find ways to replace the other benefits of pairing.)Replacing practices requires continuous refinement and an in-depth understanding of XP. 
Ask your mentor for help and consider hiring an experienced XP coach.Easily-changed code is vital to XP. 
If your code is cumbersome to change, you'll have difficulty with XP's technical practices, and that difficulty will spill over into XP's planning practices.XP teams put a lot of effort into keeping their code clean and easy to change. 
If you have a brand-new codebase, this is easy to do. 
If you have to work with existing code, you can still practice XP, but it will be more difficult. 
Even well-maintained code is unlikely to have the simple design and suite of automated unit tests that XP requires (and produces). 
New XP teams often experience an epiphany between the second and fourth months. 
"This is the best code I've ever worked with!" they say, and start to see the power of XP.To understand and appreciate XP's technical practices fully, you need to experience the practices meshing together to give you complete confidence in your code, tests, and build. 
You need to feel the delight of making big improvements with small changes. 
You're unlikely to have that experience when working with existing code. 
If you can, save preexisting code for experienced XP teams.You can dig your way out of this hole. 
See "Applying XP to an Existing Project," later in this chapter.Simple, easily-changed design is XP's core enabler. 
This means at least one person on the team—preferably a natural leader—needs to have strong design skills.It's hard to tell if somebody has strong design skills unless you have strong design skills yourself. 
One clue to look for is an understanding and appreciation of domain-driven design. 
It requires a crucial shift in thinking—from imperative procedural design to declarative object-oriented design—that programmers with poor design skills can have difficulty grasping.You'll probably do as well with XP as you would be with any method—perhaps more, because XP includes specific technology practices and advice. 
However, that doesn't mean you'll be successful. 
Take it slow and steady, and seek out as much experienced help as you can get.Meanwhile, start learning!  [Evans]' Domain-Driven Design is a good place to start, as is [Fowler 2002a]'s Patterns of Enterprise Application Architecture. 
Consider taking a course or hiring somebody to join the team as a mentor. 
Be careful, though—strong design skills, while essential, are surprisingly rare. 
Ask someone with good design skills to help you vet your choice.XP relies on refactoring to continuously improve existing designs, so any language that makes refactoring difficult will make XP difficult. 
Of the currently popular languages, object-oriented and dynamic languages with garbage collection are the easiest to refactor. 
C and C++, for example, are more difficult to refactor.You can still use XP, but be sure to include someone on your team who has experience with refactoring in your language, if you can.Some people are natural leaders. 
They're decisive, but appreciate others' views; competent, but respectful of others' abilities. 
Team members respect and trust them. 
You can recognize a leader by her influence—regardless of her title, people turn to a leader for advice.XP relies on self-organizing teams. 
Such a team doesn't have a predefined hierarchy; instead, the team decides for itself who is in charge of what. 
These roles are usually informal. 
In fact, in a mature XP team, there is no one leader. 
Team members seamlessly defer leadership responsibilities from one person to the next, moment to moment, depending on the task at hand and the expertise of those involved.When your team first forms, though, it won't work together so easily. 
Somebody will need to help the team remember to follow the XP practices consistently and rigorously. 
This is particularly important for programmers, who have the most difficult practices to learn.In other words, your team needs a coach. 
The best coaches are natural leaders—people who remind others to do the right thing by virtue of who they are rather than the orders they give. 
Your coach also needs to be an experienced programmer so she can help the team with XP's technical practices.Explain the situation to the team and ask them to choose a coach by consensus. 
In other words, ask them to pick one person that they can all agree would be a good coach.If you can't pick a coach by consensus, your team may be too fractured to use XP. 
If there's someone you can hire that the team would trust, that may help. 
Be sure to tell whoever you hire that you weren't able to reach consensus on this issue—an experienced XP coach will see it as a danger sign and should speak to team members before accepting.Good leaders aren't always experienced developers, but a good coach should look for subtle cues that indicate upcoming problems, which does require experience. 
An experienced developer is your best coach.If your leaders are inexperienced, you may want to try pair coaching. 
Pick one person who's a good leader and one person who has a lot of experience. 
Make sure they get along well. 
Ask the two coaches to work together to help the team remember to practice XP consistently and rigorously.Your organization may assign somebody to be coach who isn't a good leader. 
In this case, if the assigned coach recognizes the problem, pair coaching may work for you.If the assigned coach doesn't recognize the problem and he's damaging the team's ability to function, discuss the situation with your mentor or a manager you trust. 
This is a delicate situation that requires context-specific advice.XP requires that everybody work together to meet team goals. 
There's no provision for someone to work in isolation, so it's best if team members enjoy working together.XP requires people to work together. 
Combined with the pressure of weekly deliveries, this can help team members learn to trust and respect each other. 
However, it's possible for a team to implode from the pressure. 
Try including a team member who is level-headed and has a calming influence.If team members won't even attempt to work together, don't use XP. 
If there's just one person whose behavior encourages other people's bad behavior, you might be able to solve the problem by moving him to a different team.Are you ready to adopt XP?  Great!  Your first step is to arrange for your open workspace (see Sit Together in Chapter 6). 
Start solving this problem now. 
It will probably take longer than you expect.Next, find an appropriate project for the team to work on. 
Look for a project that's valuable, but be wary of projects that will be under intense scrutiny. 
You need room to make mistakes as you learn.At the same time, figure out who will be on your team. 
The XP Team in Chapter 3 provides some suggestions for team structure. 
Talk with your project's executive sponsor and other stakeholders about who to include as your on-site customers. 
(See Real Customer Involvement in Chapter 6 for ideas.)  Be sure your team members want to try XP.As you're forming your team, consider hiring an experienced XP coach to work with the team full-time. 
Although an outside coach isn't necessary—I learned XP by reading about it and trying it—a good coach will make things go more smoothly.1Avoid permanent markers; they bleed through the paper and will damage your whiteboard if you use the wrong pen by mistake.It's a fact of life: change makes people uncomfortable. 
XP is probably a big change for your team. 
If you previously used a rigid, document-centric process, XP will seem loose and informal. 
If you previously had no process, XP will seem strict and disciplined. 
Either way, expect team members to be uncomfortable. 
This discomfort can extend into the larger organization.Discomfort and a feeling of chaos is normal for any team undergoing change, but that doesn't make it less challenging. 
Expect the chaotic feeling to continue for at least two months. 
Give yourselves four to nine months to feel truly comfortable with your new process. 
If you're adopting XP incrementally, it will take longer.To survive the transformation, you need to know why you are making this change. 
What benefits does it provide to the organization?  To the team?  Most importantly, what benefits does it provide to each individual?  As you struggle with the chaos of change, remember these benefits.A supportive work environment is also important. 
Team members are likely to experience defense reactions to the lack of familiar structure. 
Expect mood swings and erratic behavior. 
Some team members may lash out or refuse to cooperate. 
Acknowledge the discomfort people are experiencing, and help team members find constructive outlets for their frustration.Your stakeholders may be uncomfortable with your new approach to planning and reporting progress. 
Managers and executives may see the team's initial chaos as a sign that XP won't work for your team. 
To help everyone feel more comfortable, consider giving them this pledge:Before starting XP, it's a good idea to discuss working agreements—that is, which practices your team will follow and how your practice of XP will differ from what I describe in this book. 
(I recommend following the book as closely as you can until you've had several months of experience.)  Discuss your roles and what you expect from each other. 
It's best to hold these conversations as a collaborative team discussions. 
Try to avoid assigning roles or giving people orders.In the final weeks before starting your new XP project, review the practices in Part II. 
Try some of the practices in your current work and consider taking courses on the practices that seem challenging.When you've finished these preparations, if you have a greenfield project—meaning your team is creating a new codebase from scratch—you're ready to go. 
Review the practices in Part II one more time, take a deep breath, and start your first iteration."Wait!" you may wonder. 
"Isn't there a way we can ease into this?"Well... yes. 
You can follow the incremental approach that legacy projects use, but if you have a greenfield project, it's actually easier and faster to adopt all the practices at once. 
It's the chaos and uncertainty of change that makes adopting XP difficult, not the practices themselves. 
If you adopt XP incrementally, every new practice will disrupt the equilabrium you'll be fighting to achieve. 
You'll actually extend the period of chaos and uncertainty, making the transition all the more difficult. 
In my experience, teams that adopt XP incrementally make substantial improvements, but it's the team that adopt it all at once that really excel.Be bold. 
You have the right people, the right workplace, and the will to succeed. 
Do it!When starting a brand-new XP project, expect the first three or four weeks to be pretty chaotic as everyone gets up to speed. 
During the first month, on-site customers will be working out the release plan, programmers will be establishing their technical infrastructure, and everyone will be learning how to work together.Some people think the best way to overcome this chaos is to take a week or two at the beginning of the project to work on planning and technical infrastructure before starting the first iteration. 
Although there's some merit to this idea, an XP team should plan and build technical infrastructure incrementally and continuously throughout the project as needed. 
Starting with a real iteration on the first day helps establish this good habit.Your very first activity will be to plan your first iteration. 
Normally, this involves selecting stories from the release plan, but you won't have a release plan yet. 
Instead, think of one feature that will definitely be part of your first release. 
Brainstorm a few must-have stories for that feature. 
These first few stories should sketch out a "vertical slice" (see Figure 8-3) of your application. 
If the application involves user interaction, create a story to display the initial screen or web page. 
If it includes reporting, create a story for a bare-bones report. 
If it requires installation, create a story for a bare-bones installer.Don't expect much from these initial stories. 
The programmers' estimates for them will be fairly high because they need to establish some technical infrastructure. 
As a result, the stories should do very little. 
The report might display headers and footers, but no line items. 
The installer might just be a .zip file. 
The initial screen could have nothing more than your logo on it.These basic stories will give you ideas for more stories that will add missing details. 
Brainstorm ten to twenty in the first planning session and have the programmers estimate them. 
These should keep the programmers busy for several iterations. 
Try to choose stories that the programmers already understand well; this will reduce the amount of time customers need to spend answering programmer questions so they can focus on creating the release plan.Iteration planning is a little more difficult during the first iteration because you haven't established a velocity yet. 
Just make your best guess about what your velocity might be. 
(Some teams add up the available programmer-days and divide by π.)  During the iteration, work on just one or two stories at a time and check your progress every day. 
This will help you deliver completed stories even if your initial plan is wildly inaccurate.After you've finished planning, programmers should start establishing their technical infrastructure. 
Set up an integration machine, create your version control repository, and so forth. 
(I recommend creating engineering tasks for these items during iteration planning. 
See Iteration Planning in Chapter 8 for more about the role of engineering tasks in iteration planning.)  Once that's set up, start working on your stories.During the first iteration, it's a good idea to have all the programmers work on the first few stories as a group. 
Set up a projector so the whole team navigates while one person drives. 
(See Pair Programming in Chapter 5 for an explanation of driving and navigating.)  Sometimes individual programmers (or pairs) peel off to take care of some necessary issue, such as installing a version control system or setting up the programmers' workstations, but for the most part you should work as a team. 
This reduces the chaos that occurs when multiple people work on a tiny project and allows you to jointly establish initial conventions, such as project structure, filenames and namespaces, and basic design choices.After the first few days, the fundamentals should be well-established and the project should be large enough for people to work on separate parts without unduly interfering with each other. 
At this point, you can break into pairs and work normally. 
It's also a good time to schedule your first coding standards discussion. 
For that first meeting, you can usually just document what you agreed on while working as a group.While the programmers are working on stories, customers and testers should work on the vision and release plan. 
First, work with stakeholders to create the product vision. 
You probably already have an idea what the vision for the project is; now formalize it. 
Finalizing the vision could take a few weeks, so while that's in progress, brainstorm the stories for your first feature. 
Start thinking about other features that you want to include and pick a date for your first release. 
Decide on your planning horizons as well. 
(See Release Planning in Chapter 8 for more about planning horizons.)Each subsequent iteration will be a little easier to plan. 
The programmers' estimates will stabilize and your velocity will become predictable. 
You'll be able to estimate the scope of your next release and fill out your planning horizons. 
The feeling of chaos will begin to subside as the team works in a steady, predictable rhythm.Greenfield projects can adopt all the XP practices at once. 
You'll experience some bumps along the way, but you'll typically have things figured out in four to nine months.If you're working with an existing codebase that has no tests, particularly one that's been around for a year or more—in other words, if you have a legacy project—you can achieve the same results, but it will take more time. 
In this case, adopt XP incrementally.Other than change itself, the biggest challenge in applying XP to an existing project is not writing tests, refactoring, or cleaning up your bug database. 
The biggest challenge is setting aside enough time to pay down debt.If you have a typical legacy project, your current velocity is a polite fiction based on shortcuts. 
In other words, you incur new technical debt in order to meet your deadlines. 
To improve productivity and reduce bug production, not only do you need to stop incurring new technical debt, you need to set aside extra slack (see Slack in Chapter 8) for paying down the existing technical debt. 
This double hit will cause your velocity to go down. 
It might go down a lot.Fortunately, as your technical debt decreases, your velocity will rise again. 
Eventually it will surpass your current velocity. 
This can take a while. 
Depending on the amount of technical debt you have and how much slack you set aside for paying it down, expect your velocity to remain low for at least a quarter, probably more.Setting aside slack is a painful decision. 
However, if you don't stop accumulating technical debt, your velocity will continue to decrease and your defect production rate will increase. 
Eventually, the cost of development will exceed the value of even simple changes. 
Your organization will either shelve the product or rewrite it at great expense.Product managers, avoid this fate by acting decisively now. 
This is your best option for turning a debt-ridden legacy project into a long-term asset.The first thing you need to do is bring structure to your project. 
Many legacy projects have a chaotic approach to planning, even if they started out well.Start by introducing XP's structural practices. 
Move the team, including customers and testers, into a shared workspace, start pair-programming, conduct iteration planning and retrospectives, and so forth. 
Apply:Two-, three-, or even four-week iterations may be best for you. 
Start with two-week iterations. 
In particularly challenging environments, you may have trouble making your stories both small and customer-valued (see Stories in Chpater 8). 
Consider increasing your iteration length, but talk to your mentor (see "Find a Mentor" in Chapter 2) before doing so.Other than working more closely together, the biggest changes will be to planning. 
Take your existing project plan and convert each line item into a story card. 
If the stories aren't customer-centric, that's okay for now; once the team is used to working in iterations, the customers and the project manager should start revising the stories to make them more customer-centric.When you are comfortable with the structural practices, begin introducing technical practices.The biggest problem facing legacy projects is usually excessive technical debt. 
You need to stop the bleeding by preventing more technical debt from occurring. 
First, create a ten-minute build. 
Follow up with continuous integration. 
Introduce test-driven development.Meanwhile, reduce existing technical debt by introducing extra slack into your iterations (see Slack in Chapter 8). 
Use it to pay down technical debt as described in that section. 
At first, your clean-up efforts will seem fruitless, but over time, you'll see greater and greater benefits to quality and productivity. 
As your code quality improves, introduce the remaining practices in Releasing and Developing.These first steps will allow you to steadily pay down technical debt while continuing to make progress on new stories. 
As the bug rate for new code drops, you can start organizing your bug backlog.If your team is like most teams, your bug database is full of to-dos, questions, feature requests, and genuine defects. 
Customers and testers, go through the database and eliminate duplicates and unimportant issues. 
Close feature requests by turning them into stories or rejecting them. 
Find another way to address to-dos and questions. 
When you're done, the only items remaining should be genuine defects.2Thanks to Erik Petersen for this insight.Depending on the size of your bug database, you may not be able to do this work in a single session. 
Chip away at it every iteration, just as the programmers do with technical debt.If your bug database is in use by stakeholders, support personnel, or other people outside of the team, find a way to keep new entries clean. 
You may be able to institute new policies for using the database, but your best approach is probably to review, clean up, and categorize new entries every day.Either way, as your bug database becomes a reliable bug repository, make a fix or don't fix decision for each bug. 
You should probably involve the product manager at some level and you may need the programmers to estimate the cost of fixing some of the bugs.Close or defer all of the bugs that you decide not to fix in this release. 
You can revisit them when you plan the next release. 
At this point, all that remains in the database is bugs that you will fix. 
Turn these bugs into stories, have the programmers estimate any that remain unestimated, and put them in the release plan.Over the remainder of the release, fix the bugs and work on preventing their causes as described in No Bugs in Chapter 7. 
Continue to pay down technical debt and start applying a bit of root-cause analysis as well.When you start this process, your testers will probably spend their time testing each release prior to delivery. 
A large part of their workload is likely to be manual regression testing. 
The programmers' focus on test-driven development will slowly create an automated regression suite and reduce the pressure on the testers.As time passes, productivity improves, and programmers have less need to pay down technical debt, use your iteration slack to automate the remaining manual regression tests. 
You may need to create end-to-end tests at first. 
Over time, refactor the end-to-end tests into more focused unit and integration tests.With the regression testing burden eliminated and the team producing few new bugs, the testers will have time available for other work. 
Take advantage of this opportunity to finish integrating the testers into the team. 
Move them forward in the process so that, rather than testing after a development phase, they help the team produce higher quality code from the beginning. 
Have them work with customers to find holes in requirements (see Customer Tests in Chapter 9) and begin conducting exploratory testing (see Exploratory Testing in Chapter 9).This process will allow you to reduce technical debt, increase code quality, and remove defects. 
As you do, productivity will increase. 
At first, your progress will be imperceptible. 
Depending on the amount of technical debt you face, it could take many months to get to the ideal of nearly zero new bugs each month. 
It will take months more to finish your regression test suite, eliminate the need for a separate pre-release testing phase, and integrate your testers.As long as each iteration has less debt than the previous, however, you will get there. 
It will take time and hard work but it will be well worth it. 
After the first few months, you should start seeing progress in the form of more reliable estimates and more enjoyable programming.XP assumes that you use iterations, not phases, which makes using XP in a phase-based environment difficult. 
If your organization uses a phase-based approach to development, you may be able to use the XP development practices (see Releasing and Developing) even if you can't use the other practices.Your organization may want to try XP within your existing phase-based structure. 
Your best course of action is to convince your organization to let you try XP's simultaneous phases. 
If that doesn't work, you may be able to shoehorn XP into a phase-based structure. 
It's difficult and the exact approach depends on your organization. 
The following suggestions are a starting point; talk to your mentor for more specific advice.Your organization may have a planning phase or planning gate which expects you to deliver a detailed plan. 
If you can, allocate a month for the planning phase and use it to run four actual iterations. 
(You may be able to combine the planning phase and analysis phase to get more time.)  Use the approach described in Release Planning in Chapter 8 to create your release plan during those first iterations. 
You'll end up with a good plan and you will have finished some actual software, too.If you can't use this approach, whatever approach your organization currently uses for planning will be fine, although it probably won't be as accurate as conducting actual iterations.If your organization conducts an upfront analysis phase, you may receive a requirements document as a fait accompli. 
In this case, decompose the requirements document into stories. 
One starting point is to create a story out of each sentence including the words "must", "shall", or "should".If instead you need to create your own requirements document, XP doesn't have much to add. 
Use traditional requirements-gathering techniques in this situation, perhaps using iterations and requirements gathering stories for structure.Requirements documents aren't a replacement for a good product manager or on-site customers. 
Without those people, you will have difficulty filling in missing details in the requirements documents. 
You will also have more trouble making good schedule/scope trade-offs.XP assumes the use of incremental design and architecture that is intimately tied to programming with test-driven development. 
An up-front design phase has little to add to this approach.If you can, conduct actual XP iterations during the design phase and work on the first stories in your release plan. 
Use the time to create an initial design and architecture incrementally. 
Document the results in your design document.XP focuses on improving and adapting the design throughout the project. 
Simple design is central to doing so. 
Dedicated design phases often lead to complex designs, so minimize the amount of time you spend on upfront design if you can.XP fits well into the coding phase. 
Break your coding phase into one-week iterations and conduct XP as normal.XP performs a lot of testing every iteration. 
A phase-based organization that considers XP to be the coding phase and expects a long testing phase might schedule too little time for coding and too much time for testing. 
However, testing is an important part of XP and should remain integrated.With a good build, you should be ready to deploy at the end of any iteration. 
You can schedule XP's wrap-up activities for the deployment phase.What if your team doesn't meet the conditions for using XP?  What then?Although you won't be able to use all of XP, you may be able to add some XP practices to your existing method. 
Several practices are easy to adopt and are likely to make an immediate difference:If you struggle with frequent interruptions, try adopting day-long iterations (see Iteration Planning in Chapter 8). 
Use the planning game (see The Planning Game in Chapter 8) and the team's measured velocity (discussed in Estimating in Chapter 8) to conduct a joint planning session at the beginning of each day, then defer all interruptions until the next planning meeting, which will be less than a day away. 
Be sure to have programmers estimate their own tasks.If you aren't interrupted frequently, but still feel a sense of chaos in your planning, try using weekly iterations. 
In this case, you may also benefit from daily stand-up meetings (see Stand Up Meetings in Chapter 6) and weekly iteration demos (see Iteration Demo in Chapter 6). 
As time goes on, consider using index cards for planning and a big chart to show upcoming work, as shown in Release Planning in Chapter 8.Frequent retrospectives (see Retrospectives in Chapter 5) are an excellent way for your team to adapt and improve its process. 
If your team has the authority to make any improvements to its process, try scheduling weekly or biweekly retrospectives.A fast, automated build will make a big difference to your quality of life, and it will open up opportunities for other improvements as well. 
See Ten-Minute Build in Chapter 7 for more.Continuous integration not only decreases integration problems, it also drives improvements to your build and tests. 
See Continuous Integration in Chapter 7 for more.Although test-driven development (see Test-Driven Development in Chapter 9) isn't as easy to adopt as the other practices, it's very powerful. 
Test-driven development is the basis for reducing bugs, increasing development speed, improving your ability to refactor, and decreasing technical debt. 
It can take some time to master, so be patient.Other XP practices might help, so review Part II. 
Many of the practices there require the support of other practices, so be sure to read each practice's "Contraindications" section carefully before trying it.What's wrong with this sentence?That's a quote from a manager I once worked with. 
In a way, he was right: you will never give your customer what she wants without typing on a keyboard.That wasn't our problem, though. 
I later realized that our progress had a single bottleneck: the availability of our staging environment. 
More keyboards wouldn't have helped, even if we had more programmers sitting at them. 
If we had realized this sooner, we would have been much more productive.Sometimes the biggest gains in productivity come from stopping to think about what you're doing, why you're doing it, and whether it's a good idea. 
The best developers don't just find something that works and use it; they also question why it works, try to understand it, and then improve it.XP doesn't require experts. 
It does require a habit of mindfulness. 
This chapter contains five practices to help mindful developers excel.The following text is excerpted from The Art of Agile Development by James Shore and Shane Warden, published by O'Reilly. Copyright &copy 2008 the authors. All rights reserved.We help each other succeed.Do you want somebody to watch over your shoulder all day?  Do you want to waste half of your time sitting in sullen silence watching somebody else code?Of course not. 
Nobody does—especially not people who pair program.Pair programming is one of the first things people notice about XP. 
Two people work at the same keyboard?  It's weird. 
It's also extremely powerful and, once you get used to it, tons of fun. 
Most programmers I know who tried pairing for a month find that they prefer it to programming alone.This chapter is called Thinking, yet I included pair programming as the first practice. 
That's because pair programming is all about increasing your brainpower.When you pair, one person codes—the driver. 
The other person is the navigator, whose job is to think. 
As navigator, sometimes you think about what the driver is typing. 
(Don't rush to point out missing semicolons, though. 
That's annoying.)  Sometimes you think about what tasks to work on next and sometimes you think about how your work best fits into the overall design.This arrangement leaves the driver free to work on the tactical challenges of creating rigorous, syntactically correct code without worrying about the big picture, and it gives the navigator the opportunity to consider strategic issues without being distracted by the details of coding. 
Together, the driver and navigator create higher-quality work more quickly than either could produce on their own.11One study found that pairing takes about 15% more effort than one individual working alone, but produces results more quickly and with 15% fewer defects [Cockburn & Williams]. 
Every team is different, so take these results with a grain of salt.Pairing also reinforces good programming habits. 
XP's reliance on continuous testing and design refinement takes a lot of self-discipline. 
When pairing, you'll have positive peer pressure to perform these difficult but crucial tasks. 
You'll spread coding knowledge and tips throughout the team.You'll also spend more time in flow—that highly-productive state in which you're totally focused on the code. 
It's a different kind of flow than normal because you're working with a partner, but it's far more resilient to interruptions. 
To start with, you'll discover that your office mates are far less likely to interrupt you when you're working with someone. 
When they do, one person will handle the interruption while the other continues his train of thought. 
Further, you'll find yourself paying more attention to the conversation with your programming partner than surrounding noise; it fades into the background.If that isn't enough, pairing really is a lot of fun. 
The added brainpower will help you get past roadblocks more easily. 
For the most part, you'll be collaborating with smart, like-minded people. 
Plus, if your wrists get sore from typing, you can hand off the keyboard to your partner and continue to be productive.I recommend pair programming on all production code. 
Many teams who pair frequently, but not exclusively, discover that they find more defects in solo code. 
A good rule of thumb is to pair on anything that you need to maintain, which includes tests and the build script.When you start working on a task, ask another programmer to work with you. 
If another programmer asks for help, make yourself available. 
Never assign partners: pairs are fluid, forming naturally and shifting throughout the day. 
Over time, pair with everyone on the team. 
This will help improve team cohesion and it will spread design skills and knowledge throughout the team.When you need a fresh perspective, switch partners. 
I usually switch when I'm feeling frustrated or stuck. 
Have one person stay on the task to help bring the new partner up to speed. 
Often, even explaining the problem to someone new will help you resolve it.It's a good idea to switch partners several times per day even if you don't feel stuck. 
This will help keep everyone informed and moving quickly. 
I switch whenever I finish a task. 
If I'm working on a big task, I switch within four hours.When you sit down to pair together, make sure that you're physically comfortable. 
Position your chairs side by side, allowing for each others' need for personal space, and make sure the monitor is clearly visible. 
When you're driving, place the keyboard directly in front of you. 
Keep an eye out for this one—for some reason, people pairing tend to contort themselves to reach the keyboard and mouse rather than moving them closer.Paired programmers produce code through conversation. 
As you drive or navigate, think out loud. 
Take small, frequent design steps—test-driven development works best—and talk about your assumptions, short-term goals, general direction, and any relevant history of the feature or project. 
If you're confused about something, ask questions. 
The discussion may enlighten your partner as much as it does you.Expect to feel tired at the end of the day. 
Pairs typically feel that they have worked harder and accomplished more than when working alone. Practice energized work to maintain your ability to pair every day.When you start pairing, expect to feel clumsy and fumble-fingered as you drive. You may feel that your navigator sees ideas and problems much more quickly than you do. 
She does—navigators have more time to think than drivers do. 
The situation will reverse when you navigate. 
Pairing will feel natural in time.When navigating, expect to feel like you want to step in and take the keyboard away from your partner. 
Relax; your driver will often communicate an idea with both words and code. 
He'll make typos and little mistakes—give him time to correct them himself. 
Use your extra brainpower to think about the greater picture. 
What other tests do you need to write?  How does this code fit into the rest of the system?  Is there duplication you need to remove?  Can the code be more clear?  Can the overall design be better?As navigator, help your driver be more productive. 
Think about what's going to happen next and be prepared with suggestions. 
When I'm navigating, I like to keep an index card in front of me. 
Rather than interrupting the driver when I think of an issue, I write my ideas on the index card and wait for a break in the action to bring them up. 
At the end of the pairing session, I tear up the card and throw it away.Similarly, when a question arises, take a moment to look up the answer while the driver continues to work. 
Some teams keep spare laptops on hand for this purpose. 
If you need more than a few minutes, research the solution together. 
Sometimes the best way to do so is to split up, pursue parallel lines of inquiry, and frequently come back together to share what you have learned. 
Spike solutions are a particularly powerful approach.As you pair, switch roles frequently—at least every half hour, and possibly every few minutes. 
If you're navigating and find yourself telling the driver which keys to press, ask for the keyboard. 
If you're driving and need a break, pass the keyboard off to your navigator.To enjoy pair programming, good pairing stations are essential. 
You need plenty of room for both people to sit side by side. 
Typical cubicles, with a workstation located in a corner, won't work. 
They're uncomfortable and require one person to sit behind another, adding psychological as well as physical barriers to peer collaboration.You don't need fancy furniture to make a good pairing station; the best ones I've seen are just simple folding tables found at any good office supply store. 
They should be six feet long, so that two people can sit comfortably side-by-side, and at least four feet deep. 
Each table needs a high-powered development workstation. 
I like to plug in two keyboards and mice so each person can have a set.Splurge on large monitors so that both people can see clearly. 
Some teams mirror the display onto two monitors, which makes things a little easier to see, but you may find yourself pointing to the wrong monitor. 
Others prefer to spread one desktop across two monitors.Pairing can be uncomfortable at first, as it may require you to collaborate more than you're used to. 
These feelings are natural and typically go away after a month or two, but you have to face some challenges.It bears repeating: pairing is no fun if you're uncomfortable. 
When you sit down to pair, adjust your position and equipment so you can sit comfortably. 
Clear debris off the desk and make sure there's room for your legs, feet, and knees.Some people (like me) need a lot of personal space. 
Others like to get up close and personal. 
When you start to pair, discuss your personal space needs and ask about your partner's.Similarly, while it goes without saying that personal hygiene is critical, remember that strong flavors such as coffee, garlic, onions, and spicy foods can lead to foul breath. 
Decide as a team, before any issues come up, how to notify people of challenging personal habits respectfully.Pairing is a collaboration between peers, but sometimes a senior developer will pair with a junior developer. 
Rather than treating these occasions as student/teacher situations, restore the peer balance by creating opportunities for both participants to learn. 
For example, if you know you'll be pairing with a junior developer, you could ask him to research a topic that no one else knows, such as the inner workings of a library that the team depends on. 
Give everyone a chance to be an expert.New drivers sometimes have difficulty involving their partners; they can take over the keyboard and shut down communication. 
To practice communicating and switching roles while pairing, consider ping-pong pairing. 
In this exercise, one person writes a test. 
The other person makes it pass and writes a new test. 
Then the first person makes it pass and repeats the process by writing another test.The flip side of too little communication is too much communication—or rather, too much blunt communication. 
Frank criticism of code and design is valuable, but it may be difficult to appreciate at first. 
Different people have different thresholds, so pay attention to how your partner receives your comments. 
Try transforming declarations (such as "This method is too long") into questions or suggestions ("Could we make this method shorter?" or "Should we extract this code block into a new method?"). 
Adopt an attitude of collaborative problem solving.Even if you don't fall victim to the endless vi versus emacs editor war, you may find your coworkers' tool preferences annoying. 
Try to standardize on a particular toolset. 
Some teams even create a standard image and check it into version control. 
When you discuss coding standards, discuss these issues as well.In pair programming, two people aren't really doing the work of one. 
Although only one keyboard is in use, there's more to programming than that. 
As Ward Cunningham said, "If you don't think carefully, you might think that programming is just typing statements in a programming language."2  In pair programming, one person is programming and the other is thinking ahead, anticipating problems, and strategizing.2http://en.wikiquote.org/wiki/Ward_Cunningham, accessed 12 December 2006.If you're looking for hard data, [Williams] has a chapter on pairing research. 
Keep in mind that the number of variables in software development make it notoriously difficult to conduct large-scale controlled studies. 
Sometimes the best way to know whether something will work for your team is just to try it.Ask permission to try it as an experiment. 
Set aside a month in which everyone pairs on all production code. 
Be sure to keep going for the entire month, as pair programming may be difficult and uncomfortable for the first few weeks.Don't just ask permission of management; be sure your fellow team members are interested in trying pairing as well. 
The only programmers I know who tried pairing for a month and didn't like it are the ones who were forced to do it against their will.This is a decision that your whole team should make together. 
Before you decide, try pairing on all production code (everything you need to maintain) for a month. 
You may enjoy it more than you expect.Regardless of your rule, you will still produce code that you don't need to maintain. 
(Spike solutions are one example.)  These may benefit from individual study.Some production tasks are so repetitive that they don't require the extra brainpower a pair provides. 
Before abandoning pairing, however, consider why your design requires so much repetition. 
It could be an indication of a design flaw. 
Use the navigator's extra time to think about design improvements and consider discussing it with your whole team.When you navigate, you shouldn't have too much trouble staying several steps ahead of your driver. 
If you do, ask your driver to think out loud so you can understand her thought process.As driver, though, you may sometimes find that you're having trouble concentrating. 
Let your navigator know—she may have a suggestion that will help you get through the roadblock. 
At other times, you may just need a few moments of silence to think through the problem.If you find yourself in this situation a lot, you may be taking steps that are too large. 
Use test-driven development and take very small steps. 
Rely on your navigator to keep track of what you still need to do (tell her if you have an idea; she'll write it down) and focus only on the few lines of code needed to make the next test pass.If you are working with a technology you don't completely understand, consider taking a few minutes to work on a spike solution. 
You and your partner can work on this together or separately.A programmer flying solo can do productive tasks that don't involve production code. 
She can research new technologies, or learn more about a technology the team is using. 
She can pair with a customer or tester to review recent changes, polish the application, or do exploratory testing. 
She could be the team's batman (see "Iteration Planning" in Chapter 8).Alternatively, a solo programmer may wish to spend some time reviewing the overall design—either to improve his own understanding, or to come up with ideas for improving problem areas. 
If a large refactoring is partially complete, the team may wish to authorize a conscientious programmer to finish those refactorings.If your team is on the smaller side, you may run out of useful solo tasks. 
In this case, consider relaxing the "no production code" rule or bringing in another programmer.Even a saint will get on your nerves if you have to pair with him day-in, day-out. 
Use your own judgment about when to pair and when you need time to yourself. 
If you feel fine but your pair is getting cranky, don't escalate; just say you're tired and need a break.I pair-programmed with the same person for three months straight during a two-person project. 
I think it helped that we had a large office and a big desk: it gave us room to move around. 
We also kept a mini-fridge stocked with goodies.Even with these comforts, I had my cranky moments. Perhaps the most important factor was that my partner was a very laid-back, easy-going person who put up with my occasional bad mood.One approach is to remember that you can switch when you feel stuck or frustrated. 
In fact, this is a perfect time to switch partners and get a fresh perspective.Some teams use kitchen timers to switch partners at strictly defined intervals. 
[Belshee] reports interesting results from switching every ninety minutes. 
While this could be a great way to get in the habit of switching pairs, make sure everybody is willing to try it.You can use a phone headset and a desktop sharing tool such as VNC or NetMeeting to pair remotely. 
I have heard of teams who use individual workstations with shared screen sessions and VoIP applications.When I tried this, I found it to be a poor substitute for pairing in person. 
XP teams usually sit together, so remote pairing isn't often necessary.When you pair program well, you find yourself focusing intently on the code and your work with your partner. 
You experience fewer interruptions and distractions. 
When interrupted, one person deals with the problem while the other continues thinking. 
Afterward, you slide back into the flow of work immediately. 
At the end of the day, you feel tired yet satisfied. 
You enjoy the intense focus and the camraderie of working with your teammates.The team as a whole enjoys higher quality code. 
Technical debt decreases. 
Knowledge travels quickly through the team, raising everyone's level of competence and helping to integrate new team members quickly.Pairing requires a comfortable work environment (see "Sit Together" in Chapter 6 for design options). 
Most offices and cubicles just aren't set up that way. 
If your workspace doesn't allow programmers to sit side-by-side comfortably, either change the workspace or don't pair program.Similarly, if your team doesn't sit together, pairing may not work for you. 
Although you can pair remotely, it's not as good as in-person.Programmer resistance may be another reason to avoid pairing. 
Pairing is a big change to programmers' work styles and you may encounter resistance. 
I usually work around this by asking people to try it for a month or two before making a final decision. 
If they still resist, you're probably better off avoiding pairing rather than forcing anyone to pair against their will.Pairing is a very powerful tool. 
It reduces defects, improves design quality, shares knowledge amongst team members, supports self-discipline, and reduces distractions; all without sacrificing productivity. 
If you cannot pair program, you need alternatives.Formal code inspections can reduce defects, improve quality, and support self-discipline. 
However, my experience is that programmers have trouble including inspections in their schedules, even when they're in favor of them. 
Pairing is easier to do consistently, and it provides feedback much more quickly than scheduled inspections. 
If you're going to use inspections in place of pairing, add some sort of support mechanism to help them take place.Inspections alone are unlikely to share knowledge as thoroughly as collective code ownership requires. 
If you cannot pair program, consider avoiding collective ownership, at least at first.If you'd still like to have collective code ownership, you need an alternative mechanism for sharing knowledge about the state of the codebase. 
I've formed regular study groups  in which programmers meet daily for a timeboxed half-hour to review and discuss the design.I'm not aware of any other tool that helps reduce distractions as well as pair programming does. 
However, I find that I succumb to more frequent distractions when I'm tired. 
In the absence of pairing, put more emphasis on energized work.Pair Programming Illuminated [Williams] discusses pair programming in depth."The Costs and Benefits of Pair Programming" [Cockburn & Williams] reports on Laurie Williams' initial study of pair programming."Promiscuous Pairing and Beginner's Mind: Embrace Inexperience" [Belshee] is an intriguing look at the benefits of switching pairs at strict intervals."Adventures in Promiscuous Pairing: Seeking Beginner's Mind" [Lacey] explores the costs and challenges of promiscuous pairing. 
It's a must-read if you plan to try Belshee's approach.Peer Reviews in Software: A Practical Guide [Wiegers] discusses formal inspections and peer reviews.We work at a pace that allows us to do our best, most productive work indefinitely.I enjoy programming. 
I enjoy solving problems, writing good code, watching tests pass, and especially removing code while refactoring. 
I program in my spare time and sometimes even think about work in the shower.In other words, I love my work. 
Yet put me on a team with unclear goals, little collective responsibility, arguing, and infighting, and I'll wake up dreading going into work. 
I'll put in my hours at the office, but I'll be tempted to spend my mornings reading email and my afternoons picking at code while surfing through marginally related technical websites.We've all been in this situation. 
Because we're professionals, we strive to produce quality work even when we feel demoralized. 
Still, consider the times of greatest productivity in your career. 
Do you notice a big difference when you wake up and feel blessed to go into work? Isn't it much more satisfying to leave on time at the end of the day, knowing that you accomplished something solid and useful?XP's practice of energized work recognizes that, although professionals can do good work under difficult circumstances, they do their best, most productive work when they're energized and motivated.One of the simplest ways to be energized is to take care of yourself. 
Go home on time every day. 
Spend time with family and friends and engage in activities that take your mind off of work. 
Eat healthy foods, exercise, and get plenty of sleep. 
While you're busy with these other things, your brain will turn over the events of the day. 
You'll often have new insights in the morning.If quality time off is the yin of energized work, focused work is the yang. 
While at work, give it your full attention. 
Turn off interruptions such as email and instant messaging. 
Silence your phones. 
Ask your project manager to shield you from unnecessary meetings and organizational politics.When the yin and yang mesh perfectly, you'll wake up in the morning well-rested and eager to start your day. 
At the end of the day, you'll be tired—though not exhausted—and satisfied with the work you've done.This isn't easy. 
Energized work requires a supportive workplace and home life. 
It's also a personal choice; there's no way to force someone to be energized. 
However, you can remove roadblocks.One of my favorite techniques as a coach is to remind people to go home on time. 
Tired people make mistakes and take shortcuts. 
The resulting errors can end up costing more than the work is worth. 
This is particularly true when someone is sick; in addition to doing poor work, she could infect other people.Pair programming is another way to encourage energized work. 
It encourages focus like no other practice I know. 
After a full day of pairing, you'll be tired but satisfied. 
It's particularly useful when you're not at your best: pairing with someone who's alert can help you stay focused.It may sound silly, but having healthy food available in the workplace is another good way to support energized work. 
Breakfast really is the most important meal of the day. 
Mid-afternoon lows are also common. 
Cereal, milk, vegetables, and energy snacks are a good choice. 
Donuts and junk food, while popular, contribute to the mid-afternoon crash.The nature of the work also makes a difference. 
[McConnell 1996] reports that software developers are motivated to do good, intellectually challenging work. 
Not every project can feed the poor or solve NP-complete problems, but a clear, compelling statement of why the product is important can go a long way. 
Creating and communicating this vision is the product manager's responsibility.An achievable goal goes hand-in-hand with a compelling vision. 
Nothing destroys morale faster than being held accountable for an unachievable goal. 
The planning game addresses this issue by combining customer value with developer estimates to create achievable plans.Speaking of plans, every organization has some amount of politics. 
Sometimes, politics lead to healthy negotiation and compromising. 
Other times, they lead to unreasonable demands and blaming. 
The project manager should deal with these politics, letting the team know what's important and shielding them from what isn't.The project manager can also help team members do fulfilling work by pushing back unnecessary meetings and conference calls. 
Providing an informative workspace and appropriate reporting can eliminate the need for status meetings. 
In an environment with a lot of external distractions, consider setting aside core hours each day—maybe just an hour or two to start—during which everyone agrees not to interrupt the team.Finally, jelled teams have a lot of energy. 
They're a lot of fun, too. 
You can recognize a jelled team by how much its members enjoy spending time together. 
They go to lunch together, share in-jokes, and may even socialize outside of work. 
Like energized work, you can't force jelling, but you can encourage it; many of XP's practices do so. 
The classic work on this subject, [DeMarco & Lister 1999]'s Peopleware, is well worth reading.When you're making more mistakes than progress, it's time to take a break. 
If you're like me, though, that's the hardest time to stop. 
I feel like the solution is just around the corner—even if it's been just around the corner for the last 45 minutes—and I don't want to stop until I find it. 
That's why it's helpful for someone else to remind me to stop. 
After a break or a good night's sleep, I usually see my mistake right away.Sometimes a snack or walk around the building is good enough. 
For programmers, switching pairs can help. 
If it's already the end of the day, though, going home is a good idea.You can usually tell when somebody needs a break. 
Angry concentration, cursing at the computer, and abrupt movements are all signs. 
In a highly collaborative environment, going dark—not talking—can also be a sign that someone needs a break. 
When I notice a pair of programmers whispering to each other, I ask how long it's been since their last passing test. 
I often get a sheepish reply, and that's when I remind them to take a break.Suggesting a break requires a certain amount of delicacy. 
If someone respects you as a leader, then you might be able to just tell him to stop working. 
Otherwise, get him away from the problem for a minute so he can clear his head. 
Try asking him to help you for a moment, or to take a short walk with you to discuss some issue you're facing.If you're practicing test-driven development and continuous integration, your code should be ready to check in every few minutes. 
If you're struggling with a problem and can't check in, go home anyway. 
Often the answer will be obvious in the morning.Some teams revert (delete) code that doesn't pass all of its tests at the end of the day. 
This sounds harsh, but it's a good idea: if you can't easily check in, you've gone far off track. 
You'll do better work in the morning. 
If you're practicing continuous integration well, the loss of code will be minimal and you'll still have learned from the experience.A startup environment often has a lot of excitement and camaraderie. 
This leads to more energy and might mean that you can work long hours and still focus. 
On the other hand, startups sometimes confuse long work hours with dedication to the cause. 
Be careful not to let dedication override your good judgment about when you're too tired to make useful contributions.A sprint to the finish line might boost your energy. 
There's nothing quite like a late-night codefest when the team brings in pizza, everybody works hard, all cylinders fire, and the work comes together at the last moment. 
A great sprint can help the team jell, giving it a sense of accomplishing something in important in the face of adversity. 
However...Sprinting to the finish line is one thing; sprinting for miles is another. 
Extended overtime will not solve your schedule problems. 
In fact, it has serious negative consequences. 
DeMarco calls extended overtime "an important productivity-reduction technique," leading to reduced quality, personnel burnout, increased turnover of staff, and ineffective use of time during normal hours [DeMarco 2002] (p. 64).If you work overtime one week (whatever "overtime" means in your situation), don't work overtime again the next week. 
If I see a team sprinting more than once or twice per quarter, I look for deeper problems.When your team is energized, there's a sense of excitement and camaraderie. 
As a group, you pay attention to detail and look for opportunities to improve your work habits. 
You make consistent progress every week and feel able to maintain that progress indefinitely. 
You value health over short-term progress and feel productive and successful.Energized work is not an excuse to goof off. 
Generate trust by putting in a fair day's work.Some organizations may make energized work difficult. 
If your organization uses the number of hours worked as a yardstick to judge dedication, you may be better off sacrificing energized work and working long hours. 
The choice between quality of life and career advancement is a personal one that only you and your family can make.If your organization makes energized work difficult, mistakes are more likely. 
Pair programming could help tired programmers stay focused and catch each other's errors. 
Additional testing may be necessary to find the extra defects. If you can, add additional contingency time to your release plan for fixing them.The extreme form of this sort of organization is the death march organization, which requires (or "strongly encourages") employees to work extensive overtime week after week. 
Sadly, "Death march projects are the norm, not the exception." [Yourdon] (p. ix).To add insult to injury, [DeMarco & Lister 2003] (p. 161) weighs in: "In our experience, the one common characteristic among death-march projects is low expected value. 
They are projects aimed at putting out products of monumental insignificance. 
The only real justification for the death march is that with value so minuscule, doing the project at normal cost would clearly result in costs that are greater than benefits... if the project is so essential, why can't the company spend the time and money to do it properly?"Peopleware [DeMarco & Lister 1999] is a classic work on programmer motivation and productivity. 
It should be at the top of every software development manager's reading list.Rapid Development [McConnell 1996] has a chapter on "Motivation" with a nice chart comparing programmer motivations to the motivations of managers and the general population.Slack [DeMarco 2002] looks at the effects of extended overtime and overscheduling.Death March [Yourdon] describes how to survive a "death march" project.We are tuned in to the status of our project.Your workspace is the cockpit of your development effort. 
Just as a pilot surrounds himself with information necessary to fly a plane, arrange your workspace with information necessary to steer your project: create an informative workspace.An informative workspace broadcasts information into the room. 
When people take a break, they will sometimes wander over and stare at the information surrounding them. 
Sometimes, that brief zone-out will result in an aha moment of discovery.An informative workspace also allows people to sense the state of the project just by walking into the room. 
It conveys status information without interrupting team members and helps to improve stakeholder trust.The essence of an informative workspace is information. 
One simple source of information is the feel of the room. 
A healthy project is energized. 
There's a buzz in the air—not tension, but activity. 
People converse, work together, and make the occasional joke. 
It's not rushed or hurried, but it's clearly productive. 
When a pair needs help, other pairs notice, lend their assistance, then return to their tasks. 
When a pair completes something well, everyone celebrates for a moment.An unhealthy project is quiet and tense. 
Team members don't talk much, if at all. 
It feels drab and bleak. 
People live by the clock, punching in and punching out—or worse, watching to see who is the first one to dare to leave.Besides the feel of the room, other cues communicate useful information quickly and subconsciously. 
If the build token is away from the integration machine, it's not safe to check out the code right now. 
By mid-iteration, unless about half of the cards on the iteration plan are done, the team is going faster or slower than anticipated.An informative workspace also provides ways for people to communicate. 
This usually means plenty of whiteboards around the walls and stacks of index cards. 
A collaborative design sketch on a whiteboard can often communicate far more quickly and effectively than a half-hour PowerPoint presentation. 
Index cards are great for Class-Responsibility-Collaborator (CRC) design sessions, retrospectives, and planning with user stories.An essential aspect of an informative workspace is the big visible chart. 
The goal of a big visible chart is to display information so simply and unambiguously that it communicates even from across the room.The iteration and release planning boards are ubiquitous examples of such a chart. 
You'll see variants of these planning boards in every XP project. 
For more information about these boards, see the release planning board shown in Figure 8-4 and the iteration planning board shown in Figure 8-9.Another useful status chart is a team calendar, which shows important dates, iteration numbers, and when team members will be out of the office (along with contact information, if appropriate). 
A large plastic perpetual calendar, available at most office supply stores, works well here.Avoid the reflexive temptation to computerize your charts. 
The benefits of the informative workspace stem from the information being constantly visible from everywhere in the room. 
It's difficult and expensive for computerized charts to meet that criterion; you'd have to install plasma screens or projectors everywhere.Even if you can afford big screens everywhere, you will constantly change the types of charts you display. 
This is easier with flip charts and whiteboards than with computers, as creating or modifying a chart is as simple as drawing with pen and paper. Don't let a spreadsheet or project management software constrain what you can track.One type of big visible chart measures specific issues that the team wants to improve. 
Often, the issues come up during a retrospective. 
Unlike the planning boards or team calendar, post these charts only as long as necessary.Create process improvement charts as a team decision and maintain them as a team responsibility. 
When you agree to create a chart, agree to keep it up to date. 
For some charts, this means taking 30 seconds to mark the board when the status changes. 
Each team member should update his own status. 
Some charts involve collecting some information at the end of the day. 
For these, collectively choose someone to update the chart.There are many possible types of process improvement charts; they take forms as diverse as the types of problems that teams experience. 
The principle behind all of them is the same: they appeal to our innate desire for improvement. 
If you show progress towards a mutual goal, people will usually try to improve their status.Consider the problems you're facing and what kind of chart, if any, would help. 
As an example, XP teams have successfully used charts to help improve:Try not to go overboard with your process improvement charts. 
If you post too many, they'll lose their effectiveness. 
My maximum is three to five charts. 
That's not to say that your only decorations should be a handful of charts. Team memorabilia, toys, and works in progress, are also welcome. 
Just make sure the important charts stand out.While having too many process improvement charts can reduce their impact, a bigger problem occurs when the team has too much interest in a chart, that is, in improving a number on a chart. 
They often start gaming the process. 
Gaming occurs when people try to improve a number at the expense of overall progress.For example, if programmers focus on too much on improving the number of tests in the system, they might be reluctant to remove out-of-date tests, making maintenance more difficult, or they might add unnecessary or redundant tests. 
They may not even realize they're doing so.To alleviate this problem, use process improvement charts with discretion. 
Discuss new charts as a team. 
Carefully tie charts to the results you want to see. 
Review their use often and take them down after an iteration or two. 
By that time, a chart has either done its job or it isn't aren't going to help.Above all, never use workspace charts in performance evaluations. 
Don't report them outside the team. 
People who feel judged according to their performance on a chart are much more likely to engage in gaming. 
See Reporting in Chapter 6 for ideas about what to report instead.A digital camera can effectively capture a whiteboard or other chart. 
You could even point a webcam at a chart and webcast it. 
Get creative.Remember, though, that most information in the team workspace is for the team's use only. 
Reporting team progress outside of the team is a separate issue.The first question to ask is, "Did the team really agree to this chart?"  An informative workspace is for the team's benefit, so if team members aren't keeping a chart up-to-date, they may not think that it's beneficial. 
It's possible that the team is passively-aggressively ignoring the chart rather than telling you that they don't want it.I find that when no one updates the charts, it's because I'm being too controlling about them. 
Dialing back the amount of involvement I have with the charts is often enough to get the team to step in. 
Sometimes that means putting up with not-quite-perfect charts or sloppy handwriting, but it pays off.If all else fails, discuss the issue during the retrospective or a stand-up meeting. 
Share your frustration and ask for the team's help in resolving the issue. Prepare to abandon some favorite charts if the team doesn't need them.When you have an informative workspace, you feel like you have up-to-the-minute information about all of the important issues your team is facing. 
You know exactly how far you've come and how far you have to go in your current plan. 
You know whether the team is progressing well or having difficulty, and you know how well you're solving problems.If your team doesn't sit together in a shared workspace, you probably won't be able to create an effective informative workspace.If your team doesn't sit together, but has adjacent cubicles or offices, you might be able to achieve some of the benefits of an informative workspace by posting information in the halls or a common area. 
Teams that are more widely distributed may use electronic tools supplemented with daily stand-up meetings.A traditional alternative is the weekly status meeting, but I find these dreary wastes of time that delay and confuse important information.Agile Software Development [Cockburn] has an interesting section on "Convection Currents of Information" that describes information as heat and big visible charts as "information radiators."We prevent mistakes by fixing our process.When I hear about a serious mistake on my project, my natural reaction is to get angry or frustrated. 
I want to blame someone for screwing up.Unfortunately, this response ignores the reality of Murphy's Law. 
If something can go wrong, it will. 
People are, well, people. Everybody makes mistakes. 
I certainly do. 
Aggressively laying blame might cause people to hide their mistakes, or to try to pin them on others, but this dysfunctional behavior won't actually prevent mistakes.Instead of getting angry, I try to remember Norm Kerth's Prime Directive: everybody is doing the best job they can given their abilities and knowledge (see Retrospectives later in this chapter for the full text of the Prime Directive). 
Rather than blaming people, I blame the process. 
What is it about the way we work that allowed this mistake to happen?  How can we change the way we work so that it's harder for something to go wrong?This is root-cause analysis.A classic approach to root-cause analysis to ask "why" five times. 
Here's a real-world example.Problem: When we start working on a new task, we spend a lot of time getting the code into a working state.Why?  Because the build is often broken in source control.Why?  Because people check in code without running their tests.It's easy to stop here and say, "Aha!  We found the problem. 
People need to run their tests before checking in."  That is a correct answer, as running tests before check-in is part of continuous integration. 
But it's also already part of the process. 
People know know they should run the tests, but they aren't doing it. 
Dig deeper.Why don't they run tests before checking in?  Because sometimes the tests take longer to run than people have available.Why do the tests take so long?  Because they spend a lot of time in database setup and teardown.Why?  Because our design makes it difficult to test business logic without touching the database.Asking "why" five times revealed a much more interesting answer than "people aren't running tests."  It helped to move away from blaming team members and toward an underlying, fixable problem. 
The solution is clear, if not easy: the design needs improvement.Root cause analysis is a technique that you can use for every problem you encounter, from the trivial to the significant. 
You can ask yourself "why" at any time. 
You can even fix some problems just by improving your own work habits.More often, however, fixing root causes requires other people to cooperate. 
If your team has control over the root cause, gather the team members, share your thoughts, and ask for their help in solving the problem. 
A retrospective might be a good time for this.If the root cause is outside the team's control entirely, then solving the problem may be difficult or impossible. 
For example, if your problem is "not enough pairing" and you identify the root cause as "we need more comfortable desks," your team may need the help of Facilities to fix it.In this case, solving the problem is a matter of coordinating with the larger organization. 
Your project manager should be able to help. 
In the meantime, consider alternate solutions that are within your control.When you first start applying root-cause analysis, you'll find many more problems than you can address simultaneously. 
Work on a few at a time. 
I like to chip away at the biggest problem while simultaneously picking off low-hanging fruit.Over time, work will go more smoothly. 
Mistakes will become less severe and less frequent. 
Eventually—it can take months or years—mistakes will be notably rare.At this point, you may face the temptation to over-apply root-cause analysis. 
Beware of thinking that you can prevent all possible mistakes. 
Fixing a root cause may add overhead to the process. 
Before changing the process, ask yourself whether the problem is common enough to warrant the overhead.I usually conduct root-cause analysis in the privacy of my own thoughts, then share my conclusions and reasoning with others. 
Involve whoever is necessary to fix the root-cause.You can use root-cause any time you notice a problem—when you notice a mistake, as you're navigating, and in retrospectives. 
It need only take a few seconds. 
Keep your brain turned on and use root-cause analysis all of the time.If you already understand the underlying causes of your problems, and you're making progress on fixing them, then you have already conducted root-cause analysis. 
However, it's easy to get stuck on a particular solution. 
Asking "why" five times may give you new insight.If your root cause points to an individual, ask "why" again. 
Why did that person do what she did?  How was it possible for her to make that mistake?  Keep digging until you learn how to prevent that mistake in the future.Keep in mind that lectures and punitive approaches are usually ineffective. 
It's better to make it difficult for people to make mistakes than to expect them always to do the right thing.When root-cause analysis is an instinctive reaction, your team values fixing problems rather than placing blame. 
Your first reaction to a problem is to ask how it could have possibly happened. 
Rather than feeling threatened by problems and trying to hide them, you raise them publicly and work to solve them.The primary danger of root-cause analysis is that, ultimately, every problem has a cause outside of your control.Don't use this as an excuse not to take action. 
If a root cause is beyond your control, work with someone (such as your project manager) who has experience coordinating with other groups. 
In the meantime, solve the intermediate problems. 
Focus on what is in your control.Although few organizations actively discourage root-cause analysis, you may find that it is socially unacceptable. 
If your efforts are "disruptive" or a "waste of time," you may be better off avoiding root-cause analysis.You can always perform root-cause analysis in the privacy of your thoughts. 
You'll probably find that a lot of causes are beyond your control. 
Try to channel your frustration and energy to fixing processes that you can influence.The following text is excerpted from The Art of Agile Development by James Shore and Shane Warden, published by O'Reilly. Copyright &copy 2008 the authors. All rights reserved.We continually improve our work habits.No process is perfect. 
Your team is unique, as are the situations you encounter, and they change all the time. 
You must continually update your process to match your changing situations. 
Retrospectives are a great tool for doing so.The most common retrospective, the iteration retrospective, occurs at the end of every iteration.In addition to iteration retrospectives, you can also conduct longer, more intensive retrospectives at crucial milestones. 
These release retrospectives, project retrospectives, and surprise retrospectives (conducted when an unexpected event changes your situation) give you a chance to reflect more deeply on your experiences and condense key lessons to share with the rest of the organization.These other retrospectives are out of the scope of this book. 
They work best when conducted by neutral third parties, so consider bringing in an experienced retrospective facilitator. 
Larger organizations may have such facilitators on staff (start by asking the HR department), or you can bring in an outside consultant. 
If you'd like to conduct them yourself, [Derby & Larsen] and [Kerth] are great resources.Anybody can facilitate an iteration retrospective if the team gets along well. 
An experienced, neutral facilitator is best to start with. 
When the retrospectives run smoothly, give other people a chance to try.Everybody on the team should participate in each retrospective. 
In order to give participants a chance to speak their minds openly, non-team members should not attend.I timebox my retrospectives to exactly one hour. 
Your first few retrospectives will probably run long. 
Give it an extra half-hour, but don't be shy about politely wrapping up and moving to the next step. 
The whole team will get better with practice, and the next retrospective is only a week away.I keep the following schedule in mind as I conduct a retrospective. 
Don't try to match the schedule exactly; let events follow their natural pace:After you've acclimated to this format, change it. 
The retrospective is a great venue for trying new ideas. 
[Derby & Larsen] is full of ideas for iteration retrospectives.Retrospectives are a powerful tool that can be damaging when conducted poorly. 
The process I describe here skips some important safety exercises for the sake of brevity. 
Pay particular attention to the contraindications before trying this practice.In his essay, "The Effective Post-Fire Critique," New York City Fire Department Chief Frank Montagna writes:Everyone makes mistakes, even when lives are on the line. 
The retrospective is an opportunity to learn and improve. 
If an attendee feels threatened, they won't participate. 
The team should never use the retrospective to place blame or attack individuals.As facilitator, it's your job to nip destructive behavior in the bud. 
To this end, I start each retrospective by repeating Norm Kerth's Prime Directive. 
I write it at the top of the whiteboard.I ask each attendee in turn if he agrees to the Prime Directive and wait for a verbal "yes". 
If not, I ask if he can set aside his skepticism just for this one meeting. 
If an attendee still won't agree, I won't conduct the retrospective.If everyone agrees to the Prime Directive, hand out index cards and pencils, then write the following headings on the whiteboard:Ask the group to reflect on the events of the iteration and brainstorm ideas that fall into these categories. 
Think of events that were enjoyable, frustrating, and puzzling, and consider what you'd like to see increase, decrease, and remain the same. 
Write each idea on a separate index card. 
As facilitator, you can write down your ideas too—just be careful not to dominate the discussion.People can come up with as many ideas as they like. 
Five to ten each is typical. 
There's no need to have an idea in each category, or to limit the ideas in a category. 
Any topic is fair game, from the banal ("more cookies") to the taboo ("frustrating: impossible deadline"). 
If people are reluctant to say what they really think, try reading the cards anonymously.Ask people to read out their cards as they finish each one, then hand them in. 
Stick the cards up on the board under their headings. 
If you don't have a ferrous whiteboard, use sticky notes instead of index cards.If people have trouble getting started, describe what happened during the iteration. 
("Wednesday, we had our planning session....")  This approach takes longer, but it might be a good way to jump-start things when you first start doing retrospectives.As people read their cards, others will come up with new ideas. 
The conversation will feed on itself. 
Don't worry if two people suggest the same idea—just put them all up on the board. 
Expect several dozen cards, at least.As the conversation winds down, check the time. 
If you have plenty of extra time, let the silences stretch out. 
Someone will often say something that he has held back, and this may start a new round of ideas. 
If you're running out of time, however, take advantage of the pause to move on to the next stage.Mute mapping is a variant of affinity mapping in which no one speaks. 
It's a great way to categorize a lot of ideas quickly.You need plenty of space for this. 
Invite everyone to stand up, go over to the whiteboard, and slide cards around. 
There are three rules:If two people disagree about where to place a card, they have to work out a compromise without talking.This exercise should take about ten minutes, depending on the size of the team. 
As before, when activity dies down, check the time and either wait for more ideas or move on.Once mute mapping is complete, there should be clear groups of cards on the whiteboard. 
Ask everyone to sit down, then take a marker and draw a circle around each group. 
Don't try to identify the groups yet; just draw the circles. 
If you have a couple of outlier cards, draw circles around those, too. 
Each circle represents a category. 
You can have as many as you need.Once you have circled the categories, read a sampling of cards from each circle and ask the team to name the category. 
Don't try to come up with a perfect name, and don't move cards between categories. 
(There's always next time.)  Help the group move quickly through this step. 
The names aren't that important and trying for perfection can easily drag this step out.Finally, after you have circled and named all the categories, vote on which categories to improve during the next iteration. 
I like to hand out little magnetic dots to represent votes; stickers also work well. 
Give each person five votes. 
Participants can put all their votes on one category if they wish, or spread their votes amongst several categories.After the voting ends, one category should be the clear winner. 
If not, don't spend too much time on it; flip a coin or something.Discard the cards from the other categories. 
If someone wants to take a card to work on individually, that's fine, but not necessary. 
Remember, you'll do another retrospective next week. 
Important issues will recur.Now that the team has picked a category to focus on, it's time to come up with options for improving it. 
This is a good time to apply your root-cause analysis skills. 
Read the cards in the category again, then brainstorm some ideas. 
Half a dozen should suffice.Don't be too detailed when coming up with ideas for improvement. 
A general direction is good enough. 
For example, if "pairing" is the issue, then "switching pairs more often" could be one suggestion, "ping-pong pairing" could be another, and "switching at specific times" could be a third.When you have several ideas, ask the group which one they think is best. 
If there isn't a clear consensus, vote.This final vote is your retrospective objective. 
Pick just one—it will help you focus. 
The retrospective objective is the goal that the whole team will work towards during the next iteration. 
Figure out how to keep track of the objective and who should work out the details.The retrospective serves two purposes: sharing ideas gives the team a chance to grow closer, and coming up with a specific solution gives the team a chance to improve.The thing I dislike about iteration retrospectives is that they often don't lead to specific changes. 
It's easy to leave the retrospective and think, "Well, that's done until next week."  If you're like me, the ensuing lack of action can be a little frustrating.To avoid this frustration, make sure someone is responsible for following through on the retrospective objective. 
It's not that person's job to push or own the objective—that's for the whole team—but it is his job to remind people when appropriate.To encourage follow-through, make the retrospective objective part of the iteration. 
For general behavior changes, such as "switch pairs more often," consider adding a big visible chart to your  informative workspace. 
For specific actions, such as "improve database abstraction layer," create task cards and put them in your iteration plan.Although some ideas may require the assistance of others, if those people can't or won't help, refocus your ideas to what you can do. 
The retrospective is an opportunity for you to decide, as a team, how to improve your own process, not the processes of others.Your project manager may be able to help convey your needs to management and other groups.This is a tough situation, and there may not be anything you can do. 
If there are just one or two people who like to place blame, try talking to them alone beforehand. 
Describe what you see happening and your concern that it's disrupting the retrospective. 
Rather than adopting a parental attitude, ask for their help in solving the problem and be open to their concerns.If a few people constantly argue with each other, talk to them together. 
Explain that you're concerned their arguing is making other people uncomfortable. 
Again, ask for their help.If the problem is widespread across the group, the same approach—talking about it—applies. 
This time, hold the discussion as part of the retrospective, or even in place of it. 
Share what you've observed, and ask the group for their observations and ideas about how to solve the problem. 
Be careful: this discussion is only helpful if the group can hold it without further arguing.If all else fails, you may need to stop holding retrospectives for a while. 
Consider bringing an organizational development (OD) expert to facilitate your next retrospective.Your ideas may be too big. 
Remember, you only have one week, and you have to do your other work, too. 
Try making plans that are smaller scale—perhaps a few hours of work—and follow up every day.Another possibility is that you don't have enough slack in your schedule. 
When you have a completely full workload, nonessential tasks such as improving your work habits go undone. 
(The sad irony is that improving your work habits will give you more time.)Finally, it's possible that the team doesn't feel like they truly have a voice in the retrospective. 
Take an honest look at the way you conduct it. 
Are you leading the team by the nose rather than facilitating?  Consider having someone else facilitate the next retrospective.It's possible they're just shy. 
It's not necessary for everyone to participate all the time. 
Waiting for a verbal response to the Prime Directive can help break the ice.On the other hand, they may have something that they want to say but don't feel safe doing it. 
[Derby & Larsen] have some good suggestions about how to incorporate safety exercises into the retrospective. 
You can also try talking with them individually outside of the retrospective.Over time, every major issue will get its fair share of attention. 
Give the retrospective a few months before deciding that a particular group is disenfranchised. 
One team in my experience had a few testers that felt that their issue was being ignored. 
A month later, after the team had addressed another issue, the testers' concern was on the top of everyone's list.If time doesn't solve the problem—and be patient to start—you can use weighted dot voting, in which some people get more dot votes than others. 
If you can do this without recrimination, it may be a good way to level the playing field.Another option is for one group to pick a different retrospective objective to focus on in addition to the general retrospective objective.It's okay to be decisive about wrapping things up and moving on. 
There's always next week. 
If the group is taking a long time brainstorming ideas or mute mapping, you might say something like, "Okay, we're running out of time. 
Take two minutes to write down your final thoughts (or make final changes) and then we'll move on."That said, I prefer to let a retrospective go long and take its natural course during the first month or so. 
This will allow people to get used to the flow of the retrospective without stressing too much about timelines.It depends on how much your process needs improvement. 
An established team may not need as many iteration retrospectives as a new team. 
I would continue to conduct retrospectives at least every other week.If you feel that your retrospective isn't accomplishing much, perhaps the real problem is that you need a change of pace. 
Try a different approach. 
[Derby & Larsen] has many ideas to try.When your team conducts retrospectives well, your ability to develop and deliver software steadily improves. 
The whole team grows closer and more cohesive, and each group has more respect for the issues other groups face. 
You are honest and open about your successes and failures and are more comfortable with change.The biggest danger in a retrospective is that it will become a venue for acrimony rather than for constructive problem solving. 
A skilled facilitator can help prevent this, but you probably don't have such a facilitator on hand. 
Be very cautious about conducting retrospectives if some team members tend to lash out, attack, or blame others.The retrospective recipe described here assumes that your team gets along fairly well. 
If your team doesn't get along well enough to use this recipe, refer to [Derby & Larsen] for more options and consider bringing in an outside facilitator.If only one or two team members are disruptive, and attempts to work the problem through with them are ineffective, you may be better off removing them from the team. 
Their antisocial influence probably extends beyond the retrospective, hurting teamwork and productivity.There are many ways to conduct retrospectives. 
See [Derby & Larsen] for ideas.I'm not aware of any tools that allow you to improve your process and improve team cohesiveness as well as retrospectives do. 
Some organizations define organization-wide processes. 
Others assign responsibility for the process to a project manager, technical lead, or architect. 
Although these approaches might lead to a good initial process, they don't usually lead to continuous process improvement, and neither approach fosters team cohesiveness.Project Retrospectives [Kerth] is the definitive resource for project retrospectives.Agile Retrospectives [Derby & Larsen] picks up where [Kerth] leaves off, discussing techniques for conducting all sorts of agile retrospectives."The Effective Post-Fire Critique" [Montagna] is a fascinating look at how a life-and-death profession approaches retrospectives.I'm a little weird that way.There's truth to the idea, though. 
Software development is all about information. 
The more effectively your programmers can access and understand the information they need, the more effective they will be at creating software. 
The better information customers and managers have, the better they can manage the schedule and the flow of feedback to the programmers.Communication in the real world is a lot more messy than it is in my image. 
There are no glowing lines to sterilely transport information from one brain to another. 
Instead, people have to work together. 
They have to ask questions, discuss ideas, and even disagree.This chapter contains eight practices to help your team and its stakeholders collaborate efficiently and effectively:We work together effectively and without fear.When a group of people comes together to work as a team, they go through a series of group dynamics known as "Forming, Storming, Norming, and Performing" [Tuckman]. 
It takes the team some time to get through each of these stages. 
They make progress, fall back, bicker, and get along. 
Over time—often months—and with adequate support and a bit of luck, they get to know each other and work well together. 
The team jells. 
Productivity shoots up. 
They do really amazing work.What does it take to achieve this level of productivity?  The team must take joint responsibility for their work. 
Team members need to think of the rest of the team as "us", not "them". 
If a team member notices something that needs doing, she takes responsibility for it, even if it's not her specialty: she does it, finds someone to help her do it, or recruits someone else to take care of it.Conversely, team members must rely on each other for help. 
When one member of a team encounters a question that she cannot answer, she doesn't hesitate to ask someone who does know the answer. 
Sometimes these quick questions turn into longer pairing sessions.Trust is essential for the team to perform this well. 
You need to trust that taking time to help others won't make you look unproductive. 
You need to trust that you'll be treated with respect when you ask for help or disagree with someone.The organization needs to trust the team, too. 
XP is strange and different at first. 
It doesn't provide the normal indicators of progress that managers are accustomed to seeing. 
It takes trust to believe that the team will deliver a success.Trust doesn't magically appear—you have to work at it. 
Here are some strategies for generating trust in your XP team.Many organizations I've worked with have struggled with an "us versus them" attitude between customers and programmers. 
Customers often feel that programmers don't care enough about their needs and deadlines, some of which, if missed, could cost them their jobs. 
Programmers often feel forced into commitments that they can't meet, hurting their health and relationships.Sometimes the acrimony is so intense that the groups actually start doing what the others fear: programmers react by inflating estimates and focusing on technical toys at the expense of necessary features; customers react by ignoring programmer estimates and applying schedule pressure. 
This sometimes happens even when there's no overt face-to-face hostility.This is a difficult situation with no easy answer. 
Such badly broken trust takes a long time to heal. 
Because neither group can force the other to bridge the gap, you have to focus on changing your own attitude.I find that the biggest missing component in this situation is empathy for the other group's position. 
Programmers, remember that customers have corporate masters that demand results. 
Bonuses, career advancement, and even job security depend on successful delivery, and the demands aren't always reasonable. 
Customers must deliver results anyway.Customers, remember that ignoring or overriding programmers' professional recommendations about timelines often leads to serious personal consequences for programmers. 
"Death march teams are the norm, not the exception... [These teams] are often found working 13- to 14-hour days, six days a week... In ["ugly" death march projects], it's not uncommon to see one or two of the project team members collapse from exhaustion, suffer ulcers or a nervous breakdown, or experience a divorce" [Yourdon] (p.ix, p.4, p.61). 
The commonality of this experience among programmers leads to apathy and cynicism about schedules and commitments.Sitting together is the most effective way I know to build empathy. 
Each group gets to see that the others are working just as hard as they are. 
Retrospectives also help, if your team can avoid placing blame. 
Programmers can help by being respectful of customer goals and customers can help by being respectful of programmer estimates and technical recommendations. 
All of this is easier with energized work.I've also seen "us versus them" attitudes between programmers and testers, although it isn't quite as prevalent as customer-programmer discord. 
When it occurs, programmers tend not to show respect for the testers' abilities, and testers see their mission as shooting down the programmers' work.As with customer-programmer discord, empathy and respect are the key to better relations. 
Programmers, remember that testing takes skill and careful work, just as programming does. 
Take advantage of testers' abilities to find mistakes you would never consider, and thank them for helping prevent embarrassing problems from reaching stakeholders and users. 
Testers, focus on the team's joint goal: releasing a great product. 
When you find mistakes, it's not an occasion for celebration or gloating. 
Remember, too, that everybody makes mistakes, and mistakes aren't a sign of incompetence or laziness.Another good way to improve team cohesiveness is to eat together. 
Something about sharing meals breaks down barriers and fosters team cohesiveness. 
Try providing a free meal once per week. 
If you have the meal brought into the office, set a table and serve the food family-style to prevent people from taking food back to their desks. 
If you go to a restaurant, ask for a single long table rather than separate tables.After a project comes to end, the team typically breaks up. 
All of the wonderful trust and cohesiveness that the team has formed is lost. 
The next project starts with a brand-new team and they have to struggle through the four phases of team formation all over again.You can avoid this waste by keeping productive teams together. 
Most organizations think of people as the basic "resource" in the company. 
Instead, think of the team as the resource. 
Rather than assigning people to projects, assign a team to a project. 
Have people join teams and stick together for multiple projects.Some teams will be more effective than others. 
Take advantage of this by using the most effective teams as a training ground for other teams. 
Rotate junior members into those teams so they can learn from the best, and rotate experienced team members out to lead teams of their own. 
If you do this gradually, the team culture and trust will remain intact.I know somebody who worked in a company with two project teams. 
One used XP, met its commitments, and delivered regularly. 
The team next door struggled: it fell behind schedule and didn't have any working software to show. Yet when the company downsized, it let the XP team members go rather than the other team!  Why?  When management looked in on the struggling team, they saw programmers working hard, long hours with their heads down and UML diagrams papering the walls. When they looked in on the XP team, they saw people talking, laughing, and going home at five with nothing but rough sketches and charts on the whiteboards.Like it or not, our projects don't exist in a vacuum. 
XP can seem weird and different to an organization that hasn't seen it before. 
"Are they really working?" outsiders wonder. 
"It's noisy and confusing. 
I don't want to work that way. 
If it succeeds, will they force me to do it too?"Ironically, the more successful XP is, the more these worries grow. 
Alistair Cockburn calls them organizational antibodies.1  If left unchecked, organizational antibodies will overcome and dismantle an otherwise successful XP team.1Via personal communication.No matter how effective you are at meeting your technical commitments, you're in trouble without the good will of your stakeholders. 
Yes, meeting schedules and technical expectations helps, but the non-technical, interpersonal skills—soft skills—your team practices may be just as important to building trust in your team.Does this sound unfair or illogical?  Surely your ability to deliver high-quality software is all that really matters.It is unfair and illogical. 
It's also the way people think—even programmers. 
If your stakeholders don't trust you, they won't participate in your team, hurting your ability to deliver valuable software. 
They might even campaign against you.Don't wait for them to realize how your work can help them. 
Show them.2Thanks to George Dinwiddie for this quote.Several years ago, I hired a small local moving company to move my belongings from one apartment to another. When the movers arrived, I was surprised to see them hustle—they moved quickly from the van to the apartment and back. This was particularly unexpected because I was paying them by the hour. There was no need for them to move so quickly.Those movers impressed me. I felt that they were dedicated to meeting my needs and respecting my pocketbook. If I still lived in that city and I needed to move again, I would hire them in an instant. 
They earned my goodwill—and my trust.In the case of a software team, hustle is energized, productive work. 
It's a sense that the team is putting in a fair day's work for a fair day's pay. 
Energized work, an informative workspace, appropriate reporting, and iteration demos all help convey this feeling of productivity.If your stakeholders have worked with software teams before, they probably have plenty of war wounds from slipped schedules, unfixed defects, and wasted money. 
In addition, they probably don't know much about software development. 
That puts them in the uncomfortable position of relying on your work, having had poor results before, and being unable to tell if your work is any better.Meanwhile, your team consumes thousands of dollars every week in salary and support. 
How do stakeholders know that you're spending their money wisely?  How do they know that the team is even competent?Stakeholders don't know how to evaluate your process, but they can evaluate results. 
Two kinds of results speak particularly clearly to them: working software and delivering on commitments.Fortunately, XP teams demonstrate both of these results every week. 
You make a commitment to deliver working software when you build your iteration and release plans. 
You demonstrate that you've met the iteration commitment in the iteration demo, exactly one week later, and you demonstrate that you've met the release commitment on your predefined release date.This week-in, week-out delivery builds trust in stakeholders like nothing I've ever seen. 
It's extremely powerful. 
All you have to do is create a plan that you can achieve... and then achieve it.Did I say "all you have to do?"  Silly me. 
It's not that easy.First, you need to plan well (see Chapter 8). 
Second, as the poet said, "The best laid schemes o' mice an' men / Gang aft a-gley."33"To a Mouse," by renowned Scottish poet Robert Burns. 
The poem starts, "Wee, sleekit, cow'rin, tim'rous beastie, O, what a panic's in thy breastie!"  This reminds me of how I felt when asked to integrate a project after a year of unintegrated development.In other words, some iterations don't sail smoothly into port on the last day. 
What do you do when your best laid plans gang a-gley?Actually, that's your chance to shine. 
Anyone can look good when life goes according to plan. 
Your true character shows when you deal with unexpected problems.The first thing to do is to limit your exposure to problems. 
Work on the hardest, most uncertain tasks early in the iteration. 
You'll find problems sooner, and you'll have more time to fix them.When you encounter a problem, start by letting the whole team know about it. 
Bring it up by the next stand-up meeting at the very latest. 
This gives the entire team a chance to help solve the problem.If the setback is relatively small, you might be able to absorb it into the iteration by using some of your iteration slack. 
Options for absorbing the problem include reducing non-critical refactorings, postponing a non-essential meeting, or even (as a team decision) cancelling research time. 
I personally volunteer to work an hour or so longer each day until we have resolved the problem—or the iteration ends, whichever comes first—when my family commitments permit.Some problems are too big to absorb no matter how much slack you have. 
In this case, get together as a whole team as soon as possible and replan. 
You may need to remove an entire story or you might be able to reduce the scope of some stories. 
(For more about what to do when things go wrong, see Iteration Planning in Chapter 8.)When you've identified a problem, let the stakeholders know about it. 
They'll appreciate your professionalism even if they don't like the problem. 
I usually wait until the iteration demo to explain problems that we solved on our own, but bring bigger problems to stakeholders' attention right away. 
The product manager is probably the best person to decide who to talk to and when.The sooner your stakeholders know about a problem (and believe me, they'll find out eventually), the more time they have to work around it. 
I include an analysis of the possible solutions as well as their technical costs. 
It can take a lot of courage to have this discussion—but addressing a problem successfully can build trust like nothing else.Be careful, though. 
It's easy to focus too much on meeting your commitments in such a way that actually damages trust. 
Suppose that you need a few more hours in an iteration to finish a particularly valuable story. 
A little bit of overtime is fine. 
Sprinting with a burst of more overtime can even be good for team morale and cohesiveness under the right conditions (see Energized Work in Chapter 5). 
Relying on overtime to meet overcommitment saps the team's energy and reduces your ability to absorb problems. 
Ironically, it leads to more missed commitments; you implicitly promise your stakeholders more than you can deliver for the price they expect to pay in time and resources.If you find yourself using a lot of overtime, something is wrong. 
Most iterations shouldn't have problems. 
When a problem does occur, you should usually be able to solve it by using slack, not overtime. 
I look for systemic problems if I see a team exceeding 10% overtime in more than one iteration per quarter.When XP teams first form, it usually takes individual members a while to think of themselves as part of a single team. 
In the beginning, programmers, testers, and customers often see themselves as separate groups.New on-site customers are often particularly skittish. 
Being part of a development team feels awkward. 
They'd rather work in their normal offices with their normal colleagues. 
Those colleagues are often influential members of the company. 
If the customers are unhappy, those feelings transmit directly back to stakeholders.When starting a new XP project, programmers should make an extra effort to welcome the customers. 
One particularly effective way to do so is to treat customers' goals with respect. 
This may even mean suppressing, for a time, cynical programmer jokes about schedules and suits.Being respectful goes both ways, and customers should also suppress their natural tendencies to complain about schedules and argue with estimates. 
I'm emphasizing the programmers' role here because it plays such a big role in stakeholder perceptions.Another way for programmers to take customers' goals seriously is to come up with creative alternatives for meeting those goals. 
If customers want something that may take a long time or involve tremendous technical risks, suggest alternate approaches to reach the same underlying goal for less cost. 
If there's a more impressive way of meeting a goal that customers haven't considered, by all means mention it—especially if it's not too hard.As programmers and customers have these conversations, barriers will be broken and trust will develop. 
As stakeholders see that, their trust in the team will blossom as well.You can also build trust directly with stakeholders. 
Consider this: the next time a stakeholder stops you in the hallway with a request, what would happen if you immediately and cheerfully took him to a stack of index cards, helped him write the story, and then brought them both to the attention of the product manager for scheduling?It might be a ten-minute interruption for you, but imagine how the stakeholder would feel. 
You responded to his concern, helped him express it, and took immediate steps to get it into the plan.That's worth infinitely more to him than firing an e-mail into a black hole of your request tracking system.You can also promote the team more directly. 
One team posted pictures and charts on the outer wall of the workspace that showed what they were working on and how it was progressing. 
Another invited anyone and everyone in the company to attend its iteration demos.Being open about what you're doing will also help people appreciate your team. 
Other people in the company are likely to be curious, and a little wary, about your strange new approach to software development. 
That curiosity can easily turn to resentment if the team seems insular or stuck-up.You can be open in many ways. 
Consider holding brown-bag lunches describing the process, public code-fests in which you demonstrate your code and XP technical practices, or an "XP open house day" in which you invite people to come see what you're doing and even participate for a little while. 
If you like flair, you can even wear buttons or hats around the office that say "Ask me about XP."In your enthusiasm to demonstrate progress, be careful not to step over the line. 
Borderline behavior includes glossing over known defects in an iteration demo, taking credit for stories that are not 100 percent complete, and extending the iteration for a few days in order to finish everything in the plan.These are minor frauds, yes. 
You may even think that "fraud" is too strong a word—but all of these behaviors give stakeholders the impression that you've done more than you actually have.There's a practical reason not to do this. 
Stakeholders will expect you to complete the remaining features just as quickly, when in fact you haven't even finished the first set. 
You'll build up a backlog of work that looks done but isn't. 
At some point, you'll have to finish that backlog, and the resulting schedule slip will produce confusion, disappointment, and even anger.Even scrupulously honest teams can run into this problem. 
In a desire to look good, teams sometimes sign up for more stories than they can implement well. 
They get the work done, but they take shortcuts to finish it all and don't do enough design and refactoring. 
The design suffers, defects creep in, and the team finds itself suddenly going much slower while struggling to improve code quality.Similarly, don't yield to the temptation to count stories that aren't "done done". 
If a story isn't completely finished, it doesn't count towards your velocity. 
Don't even take partial credit for the story. 
There's an old programming joke: the first 90% takes 90% of the time... and the last 10% takes 90% of the time. 
Until the story is totally done, it's impossible to say for certain how much is left.4"[A]dding manpower to a late software project makes it later" [Brooks], (p. 25)Give it time. 
Teams don't progress through the stages of development in an orderly manner. 
One day, everything may seem rosy; the next, calamitous. 
Storming is necessary for the team to progress: members need to learn how to disagree with each other. 
You can help by letting people know that disagreement is normal and acceptable. 
Find ways to make it safe to disagree.If the team seems like it hasn't made any progress in a month or two, ask for help. 
Talk to your mentor (see "Find a Mentor" in Chapter 2) or an organizational development expert. 
(Your HR department might have somebody on staff. 
You can also hire an OD consultant.)  If there's somebody on the team who is particularly disruptive, consider whether the team would be better off if he moved to a different project.Both are important. 
Do great work and make sure your organization knows it.Overtime won't solve your schedule problems, especially systemic ones, but it might help if you have a small hiccup to resolve. 
The key point to remember is that, while overtime can help solve small problems, it mustn't be the first or only tool in your kit.Problems tend to grow over time. 
The sooner you disclose a problem, the more time you have to solve it. 
It reduces panic, too: early in the project, people are less stressed about deadlines and have more mental energy for problems.Certainly not. 
Everybody on the team should speak up and tell the truth when they see a problem. 
However, there's a big difference between discussing a real problem and simply being cynical.Many programmers have cynical personalities. 
That's okay, but be aware that customers' careers are often on the line. 
They may not be able to tell the difference between a real joke and a complaint disguised as a joke. 
An inappropriate joke can set their adrenaline pumping just as easily as a real problem can.Venting is counterproductive when there's a better way to address the root of the problem.Mistakes happen; it's inevitable. 
Perhaps programmers underestimated the technical challenge of a story. 
Perhaps customers neglected to mention an important aspect of the story. 
Either way, finding the problem early and reworking the plan is your best solution. 
If you can't, admit your mistake and make a better plan for the next iteration. 
(See Iteration Planning in Chapter 8 for more about changing your plans when something goes wrong.)Like overtime, reworking the plan shouldn't happen too often. 
I look for underlying systemic problems if it happens more than once per quarter.When you have a team that works well together, you cooperate to meet your goals and solve your problems. 
You collectively decide priorities and collaboratively allocate tasks. 
The atmosphere in the team room is busy but relaxed, and you genuinely enjoy working with your teammates.When you establish trust within your organization and with your stakeholders, you demonstrate the value of the project and your team. 
You acknowledge mistakes, challenges, and problems, and you find solutions instead of trying to hide them until they blow up. 
You seek solutions instead of blame.Compensation practices can make teamwork difficult. 
An XP team produces results through group effort. 
If your organization relies on individual task assignment for personnel evaluation, teamwork may suffer. 
Similarly, ranking on a curve—in which at least one team member must be marked unsatisfactory, regardless of performance—has a destructive effect on team cohesion. 
These practices can transform your team into a group of competing individuals, which will hurt your ability to practice XP.Even without harmful compensation practices, team members may not trust each other. 
This is a problem, but it isn't necessarily debilitating. 
A team that must deliver software weekly in pursuit of a common goal will learn to trust each other... or implode from the pressure. 
Unfortunately, I can't tell you which outcome will happen to your team.If the team doesn't sit together, it's much harder for good teamwork to occur, and if team members also don't trust each other, it's unlikely that trust will ever develop. 
Be careful of using XP if the team doesn't sit together.Trust is vital for agile projects—perhaps for any project. 
I'm not sure it's possible to work on an agile project without it.The Wisdom of Teams [Katzenbach & Smith], which organizational development consultant Diana Larsen describes as "the best book about teams extant"."Developmental Sequences in Small Groups" [Tuckman] introduces the team development sequence of "Forming, Storming, Norming, and Performing".The Trusted Advisor [Maister et. al.] is a good resource for generating organizational trust.The Power of a Positive No [Ury] describes how to say no respectfully when it's necessary while preserving important relationships. 
Diana Larsen describes this ability as "probably more important than any amount of negotiating skill in building trust."The following text is excerpted from The Art of Agile Development by James Shore and Shane Warden, published by O'Reilly. Copyright &copy 2008 the authors. All rights reserved.We communicate rapidly and accurately.If you've tried to conduct a team meeting via speakerphone, you know how much of a difference face-to-face conversations make. 
Compared to an in-person discussion, teleconferences are slow and stuttered, with uncomfortable gaps in the conversation and people talking over each other.What you may not have realized is how much this affects your work.Imagine you're a programmer on a nonagile team and you need to clarify something in your requirements document in order to finish an algorithm. 
You fire off an email to your domain expert, Mary, then take a break to stretch your legs and get some coffee.When you get back, Mary still hasn't responded, so you check out a few technical blogs that you've been meaning to read. 
Half an hour later, your inbox chimes. 
Mary has responded.Uh-oh... it looks like Mary misunderstood your message and answered the wrong question. 
You send another query, but you really can't afford to wait any longer. 
You take your best guess at the answer—after all, you've been working at this company for a long time, and you know most of the answers—and get back to work.A day later, after exchanging a few more emails, you've hashed out the correct answer with Mary. 
It wasn't exactly what you thought, but you were pretty close. 
You go back and fix your code. 
While in there, you realize that there's an edge case nobody's handled yet.You could bug Mary for the answer, but this is a very obscure case. 
It's probably never going to happen in the field. 
Besides, Mary's very busy and you promised that you'd have this feature done yesterday. 
(In fact, you were done yesterday, except for all these nitpicky little details.)  You put in the most likely answer and move on.As the distance between people grows, the effectiveness of their communication decreases. 
Misunderstandings occur and delays creep in. 
People start guessing to avoid the hassle of waiting for answers. 
Mistakes appear.To combat this problem, most development methods attempt to reduce the need for direct communication. 
It's a sensible response. 
If questions lead to delays and errors, reduce the need to ask questions!The primary tools teams use to reduce reliance on direct communication are development phases and work-in-progress documents. 
For example, in the requirements phase, business analysts talk to customers and then produce a requirements document. 
Later, if a programmer has a question, he doesn't need to talk to an expert. 
He can simply look the answer up in the document.It's a sensible idea, but it has flaws. 
The authors of the documents need to anticipate which questions will come up and write clearly enough to avoid misinterpretations. 
This is hard to do well. 
In practice, it's impossible to anticipate all possible questions. 
Also, adding up-front documentation phases stretches out the development process.In XP, the whole team—including experts in business, design, programming and testing—sits together in an open workspace. 
When you have a question, you need only turn your head and ask. 
You get an instant response, and if something isn't clear, you can discuss it at the whiteboard.Consider the previous story from this new perspective. 
You're a programmer and you need some information from your domain expert, Mary, in order to code an algorithm.This time, rather than sending an email, you turn your head. 
"Mary, can you clarify something for me?" you ask.Mary says, "Sure. 
What do you need?"You explain the problem and Mary gives her answer. 
"No, no," you reply. 
"That's a different problem. 
Here, let me show you on the whiteboard."A few minutes later, you've hashed out the issue and you're back to coding again. 
Whoops!  There's an edge case you hadn't considered. 
"Wait a second, Mary," you say. 
"There's something we didn't consider. 
What about...."After some more discussion, the answer is clear. 
You're a little surprised: Mary's answer was completely different than you expected. 
It's good that you talked it over. 
Now, back to work!  The code is due today, and it took 20 whole minutes to figure out this nitpicky little issue.Sitting together eliminates the waste caused by waiting for an answer, which dramatically improves productivity. 
In a field study of six colocated teams, [Teasley et al.] found that sitting together doubled productivity and cut time to market to almost one third of the company baseline.Those results are worth repeating: the teams delivered software in one-third their normal time. 
After the pilot study, 11 more teams achieved the same result. 
This success led the company to invest heavily in open workspaces, by building a new facility in the US that supports 112 such teams and making plans for similar sites in Europe.How can sitting together yield such impressive results?  Communication.Although programming is the emblematic activity of software development, communication is the real key to software success. 
As [Teasley et al.] report, "Past studies have indicated that less than 30 percent of a programmer's time is spent on traditional programming tasks and less than 20 percent of the time is spent on coding. 
The rest of the time is spent on meetings, problem resolution with the team, resolving issues with customers, product testing, etc."My experience is that programmers on XP teams spend a far greater percentage of their time programming. 
I attribute that to the increased communication effectiveness of sitting together. 
Rather than sitting in hour-long meetings, conversations last only as long as needed and involve only the people necessary.Teams that sit together not only get rapid answers to their questions, they experience what [Cockburn] calls osmotic communication. 
Have you ever been talking with someone in a crowded room and then heard your name out of the blue?  Even though you were focusing on your conversation, your brain was paying attention to all of the other conversations in the room. 
When it heard your name, it replayed the sounds into your conscious mind. 
You not only hear your name, you hear a bit of the conversation around it, too, in a phenomenon known as the cocktail party effect.11The best layman's description of the cocktail party effect I've seen is on Wikipedia: http://en.wikipedia.org/wiki/Cocktail_party_effect, accessed 17 July 2006.Imagine a team that sits together. 
Team members are concentrating on their work and talking quietly with their partners. 
Then somebody mentions something about managing database connections, and another programmer perks up. 
"Oh, Tom and I refactored the database connection pool last week. 
You don't need to manage the connections manually anymore."  When team members are comfortable speaking up like this, it happens often (at least once per day) and saves time and money every time.There's another hidden benefit to sitting together: it helps teams jell and breaks down us-versus-them attitudes between groups. 
Distance seems to encourage adversarial relationships and blaming "those people."  Whenever I see this (for example, between programmers and testers), I suggest that they sit together. 
This helps the groups interact socially and gain respect for each other professionally.To get the most out of sitting together, be sure you have a complete team (see The XP Team in Chapter 3). 
It's important that people be physically present to answer questions. 
If someone must be absent often—product managers tend to fall into this category—make sure that someone else on the team can answer the same questions. 
A domain expert is often a good backup for a traveling product manager.Similarly, sit close enough to each other that you can have a quick discussion without getting up from your desk or shouting. 
This will also help encourage osmotic communication, which relies on team members overhearing conversations.Available instant help doesn't do any good if you don't ask for it. 
Many organizations discourage interruptions, I encourage them on my XP teams. 
There's no sense in banging your head against a wall when the person with the answer is right across the room. 
To support this attitude, many teams have a rule: "We must always help when asked."Interruptions disrupt flow and ruin productivity, so this rule may sound foolish. 
It takes a programmer 15 minutes or more to get back into flow after an interruption [DeMarco & Lister 1999].Fortunately, with pair programming, flow works differently. 
The delay doesn't seem to occur. 
One programmer answers the question and the other keeps thinking about the problem at hand. 
When the interruption is over, a quick "Where were we?" gets work moving again.Pairing helps in a few other ways, too. 
Osmotic communication depends on a buzz of conversation in the room. 
If people aren't pairing, there's less talking. 
Pairing also makes it easier for programmers to ignore irrelevant background conversations.Sitting together is one of those things that's easy to say and hard to do. 
It's not that the act itself is difficult—the real problem is finding space.A team that sits in adjacent cubicles can convert them into an adequate shared workspace, but even with cubicles, it takes time and money to hire people to rearrange the walls.When I say "time", I mean weeks or even months.In a smaller company, you might be able to take matters (and screwdrivers) into your own hands. 
In a larger company, you could run afoul of Facilities if you do that. 
That may be a worthwhile cost, but talk to your project manager first. 
She should have some insight on the best way to get a shared workspace.While you're waiting for construction on your dream workspace to finish, a big conference room is a good alternative. 
One team I worked with set up shop in the company boardroom for six weeks while they waited for their workspace to be ready.Your team will produce a buzz of conversation in its workspace. 
Because they'll be working together, this buzz won't be too distracting for team members. 
For people outside the team, however, it can be very distracting. 
Make sure there's good sound insulation between your team and the rest of the organization.Within the workspace, group people according to the conversations they most need to overhear. 
Programmers should all sit next to each other because they collaborate moment-to-moment. 
Testers should be nearby so programmers can overhear them talk about issues. 
Domain experts and interaction designers don't need to be quite so close, but should be close enough to answer questions without shouting.The product manager and project manager are most likely to have conversations that would distract the team. 
They should sit close enough to be part of the buzz but not so close that their conversations are distracting.An open workspace doesn't leave much room for privacy, and pair programming stations aren't very personal. 
This loss of individuality can make people uncomfortable. 
Be sure that everyone has a space they can call their own. 
You also need an additional enclosed room with a door, or cubes away from the open workspace, so people can have privacy for personal phone calls and individual meetings.As you design your workspace, be sure to include plenty of whiteboards and wall space for an informative workspace. 
Try to have at least 24 linear feet of whiteboard space, magnetic if possible. 
You can never have too many whiteboards.Some teams include a projector in their workspace. 
This is a great idea, as it allows the team to collaborate on a problem without moving to a conference room.Finally, the center of an XP workspace is typically a set of pairing stations. 
I like to have the stations facing each other so people can see each other easily. 
A hollow triangle, square, or oval setup works well. 
Provide a few more pairing stations than there are programming pairs. 
This allows testers and customers to pair as well (either with each other or with programmers) and it provides programmers with space to work solo when they need to.The sample workspace in Figure was designed for a team of 13. 
They had six programmers, six pairing stations, and a series of cubbies for personal effects. 
Nonprogrammers worked in cubbies close to the pairing stations so they could be part of the conversation even when they weren't pairing. 
Programmers' cubbies were at the far end: they typically sat at the pairing stations. 
For privacy, people adjourned to the far end of the workspace or went to one of the small conference rooms down the hall.In addition to the pairing stations, everybody had a laptop for personal work and email. 
The pairing stations all used a group login so that any team member could work at them.Before creating this workspace, the team sat in cubicles in the same part of the office. 
To create the workspace, they reconfigured the inner walls.This workspace was good, but not perfect. 
It didn't have nearly enough wall space for charts and whiteboards and non-programmers didn't have enough desk space. 
On the plus side, there was plenty of room to accommodate people at the pairing stations, which meant that customers paired with programmers frequently, and there were also extra cubbies for bringing people into the team temporarily.The small workspace in Figure was created by an up-and-coming startup when they moved into new offices. 
They were still pretty small so they couldn't create a fancy workspace. 
They had a team of seven: six programmers and a product manager.This team arranged its pairing stations along a long wall. 
They had a table on the side for meetings, and charts and whiteboards on dividers surrounding them. 
The programmers had a pod of half-cubicles to the other side for personal effects, and there were small conference rooms close by for privacy.This was a great workspace with a serious problem: the product manager wasn't in earshot and didn't participate in team discussions. 
The team couldn't get ready answers to its questions and struggled with requirements.Some team members may resist moving to an open workspace. 
Common concerns include loss of individuality and privacy, implied reduction in status from losing a private office, and managers not recognizing individual contributions. 
Team members may also mention potential distractions and noise, but I find that this is usually a cover for one of the other concerns.As with pair programming, people come to enjoy the benefits that sitting together provides, but it can take a few months. 
In [Teasley et al.]'s study, team members initially preferred cubicles to the open workspace, but by the end of the study, they preferred the open workspace.However, forcing people to sit together in hopes that they'll come to like it is a bad idea. 
When I've forced team members to do so, they've invariably found a way to leave the team, even if it meant quitting the company. 
Instead, talk with the team about their concerns and the tradeoffs of moving to an open workspace. 
Discuss how team members will be evaluated in the new system and what provisions for privacy you can make. 
You may be able to address some concerns by providing a shared workspace in addition to existing offices and cubicles.If a large portion of the team is against the open workspace, sitting together is probably not a good choice. 
If you only have one or two adamant objectors and the rest of the team wants to sit together, you may wish to sit together anyway and allow the objectors to move to a different team.A team that's working together in a shared workspace produces a busy hum of activity. 
This can be distracting at first, but most people get used to it in time.For programmers, pair programming is an excellent way to focus your attention away from the background noise. 
You won't notice it if you're pairing. 
Nonprogrammers can work in pairs too.If you work alone and find the background noise distracting, put on headphones, wear earplugs, or sit further away from the team for a time. 
You'll miss out on osmotic communication, but at least you'll be able to concentrate.Sometimes, the team gets a little noisy and rambunctious. 
It's okay to ask for quiet—the sound in the team room should be a hum, not a full-throated chorus. 
Some teams have a bell for team members to ring when they want the team to be more quiet.Especially in the beginning of the project, it's possible that the whole team really does need to hear these conversations. 
As time goes on, team members will learn which conversations they can comfortably ignore.If this is a continuing problem, try stepping a little further away from the pairing stations when a conversation lasts more than a few minutes. 
Interested team members can join the conversation and the rest of the team can continue working.Some people, particularly customers and project managers, need to take a lot of calls as they work. 
Either situate them further away from the rest of the team or arrange for an enclosed office with a door. 
Keep the door open as often as possible to allow information to flow smoothly.When your team sits together, communication is much more effective. 
You stop guessing at answers and ask more questions. 
You overhear other people's conversations and contribute answers they may not expect. 
Team members spontaneously form cross-functional groups to solve problems. 
There's a sense of camaraderie and mutual respect.The hardest part about sitting together is finding room for the open workspace. 
Cubicles, even adjacent cubicles, won't provide the benefits that an open workspace does. 
Start working on this problem now as it can take months to resolve.Don't force the team to sit together against their will. 
Adamant objectors will find a way to leave the team, and possibly the company.Be careful about sitting together if programmers don't pair program. 
Programming alone requires a quiet workspace. 
Pair programming, on the other hand, enables programmers to ignore the noise.Sitting together is one of the most powerful practices in XP. 
It's important for communication and team dynamics. 
Sitting apart tends to fray fragile relationships, particularly between different functional groups, and puts your team at a disadvantage. 
If your team is in a single location, you're better off figuring out how to sit together.If you have a multisite team, consider turning each site into its own team. 
For example, if programmers are in one site and customers are in another, the programmers may engage some business analysts to act as proxy customers. 
In this scenario, the customers and development team should still work together, face-to-face, for several weeks at the beginning of each release.If you have multiple teams of programmers, consider separating their responsibilities so that each works on entirely different codebases. 
[Evans] has an excellent discussion of the options for doing so.You can practice XP with a single, multi-site team, but it requires a lot of travel. 
[Yap] has a good experience report describing how her team made this work 24 hours a day across three time zones. 
She focused on maximizing communication by regularly flying team members to a single location for several weeks at a time. 
They also conducted daily phone calls between locations.If your whole team cannot sit together and you still wish to practice XP, talk to your mentor (see "Find a Mentor" in Chapter 2) about your options and hire experienced XP coaches for each site.Agile Software Development [Cockburn] has an excellent chapter on communication. 
Chapter 3, "Communicating, Cooperating Teams," discusses information radiators, communication quality, and many other concepts related to sitting together.If you can't sit together, "Follow the Sun: Distributed Extreme Programming Development" [Yap] is an interesting experience report describing a single XP team divided into three locations, each eight hours apart.Similarly, Domain-Driven Design [Evans] has an excellent discussion of coordinating multiple development teams in chapter 14, "Maintaining Model Integrity."  While the book's focus is object-oriented domain models, this chapter is applicable to many design paradigms.The following text is excerpted from The Art of Agile Development by James Shore and Shane Warden, published by O'Reilly. Copyright &copy 2008 the authors. All rights reserved.We understand the goals and frustrations of our customers and end-users.An XP team I worked with included a chemist whose previous job involved the software that the team was replacing. 
She was an invaluable resource, full of insight about what did and didn't work with the old product. 
We were lucky to have her as one of our on-site customers—thanks to her, we created a more valuable product.In an XP team, on-site customers are responsible for choosing and prioritizing features. 
The value of the product is in their hands. 
This is a big responsibility—as an on-site customer, how do you know which features to choose?Some of that knowledge comes from your expertise in the problem domain and with previous versions of the software. 
You can't think of everything, though. 
Your daily involvement with the project, although crucial, includes the risk of tunnel vision—you could get so caught up in the daily details of the project that you lose track of your real customers' interests.To widen your perspective, you need to involve real customers. 
The best approach to doing so depends on who you're building your software for.In personal development, the development team is its own customer. 
They're developing the software for their own use. 
As a result, there's no need to involve external customers—the team is the real customer.In-house custom development occurs when your organization asks your team to build something for the organization's own use. 
This is classic IT development. 
It may include writing software to streamline operations, automation for the company's factories, or producing reports for accounting.In this environment, the team has multiple customers to serve: the executive sponsor who pays for the software and the end-users who use the software. 
Their goals may not be in alignment. 
In the worst case, you may have a committee of sponsors and multiple user groups to satisfy.Despite this challenge, in-house custom development makes it easy to involve real customers because they're easily accessible. 
The best approach is to bring your customers onto the team—to turn your real customers into on-site customers.To do so, recruit your executive sponsor or one of his trusted lieutenants to be your product manager. 
He will make decisions about priorities, reflecting the desire of the executive sponsor to create software that provides value to the organization.Also recruit some of the end-users of the software to act as domain experts. 
As with the chemist in the introduction, they will provide valuable information about how real people use the software. 
They will reflect the end-users' desire to use software that makes their jobs better.To avoid tunnel vision, the product manager and other on-site customers should solicit feedback from their colleagues by demonstrating some of the builds created for the iteration demo and discussing their plans for the future.Outsourced custom development is similar to in-house development, but you may not have the connections that an in-house team does. 
As a result, you may not be able to recruit real customers to act as the team's on-site customers.Still, you should try. 
One way to recruit real customers is to move your team to your customer's offices rather than asking them to join you at yours.If you can't bring real customers on to the team, make an extra effort to involve them. 
Meet in person with your real customers for the first week or two of the project so you can discuss the project vision and initial release plan. 
If you're located near each other, meet again for each iteration demo, retrospective, and planning session.If you're far enough apart that regular visits aren't feasible, stay in touch with instant messaging and phone conferences. 
Try to meet face-to-face at least once per month to discuss plans. 
If you are so far apart that monthly meetings aren't feasible, meet at least once per release.Unlike custom development, vertical-market software is developed for many organizations. 
Like custom development, however, it's built for a particular industry and it's often customized for each customer.Because vertical market software has multiple customers, each with customized needs, you have to be careful about giving real customers too much control over the direction of the product. 
You could end up making a product that, while fitting your on-site customer's needs perfectly, alienates your remaining customers.Instead, your organization should appoint a product manager who understands the needs of your real customers impeccably. 
His job—and it's a tough one—is to take into account all of your real customers' needs and combine them into a single, compelling vision.Rather than involving real customers as members of the team, create opportunities to solicit their feedback. 
Some companies create a customer review board filled with their most important customers. 
They share their release plans with these customers and—on a rotating basis—provide installable iteration demo releases for customers to try.Depending on your relationship with your customers, you may be able to ask your customers to donate real end-users to join the team as on-site domain experts. 
Alternatively, as with the chemist in the introduction, you may wish to hire previous end-users to be your domain experts.In addition to the close relationship with your customer review board, you may also solicit feedback through trade shows and other traditional sources.Horizontal-market software is the visible tip of the software development iceberg: software that's intended to be used across a wide range of industries. 
The rows of shrinkwrapped software boxes at your local electronics store are a good example of horizontal-market software. 
So are many web sites.As with vertical-market software, it's probably better to set limits on the control that real customers have over the direction of horizontal-market software. 
Horizontal-market software needs to appeal to a wide audience and real customers aren't likely to have that perspective. 
Again, an in-house product manager who creates a compelling vision based on all customers' needs is a better choice.As a horizontal-market developer, your organization may not have the close ties with customers that vertical-market developers do. 
Thus a customer review board may not be a good option for you. 
Instead, find other ways to involve customers: focus groups, user experience testing, community previews, beta releases, and so forth.You organization should supply a product manager and domain experts. 
See The XP Team in Chapter 3.At first glance, this may seem like custom development, but because the actual audience for the web site is the outside world, it's closer to vertical-market or horizontal-market development. 
The product manager should come from the marketing department, if possible, but you should also solicit the input of people who will be visiting the site.When you include real customers, you improve your knowledge about how they use the software in practice. 
You have a better understanding of their goals and frustrations and you use that knowledge to revise what your produce. 
You increase your changes of delivering a truly useful and thus successful product.One danger of involving real customers is that they won't necessarily reflect the needs of all your customers. 
Be careful that they don't steer you towards creating software that's only useful for them. 
Your project should remain based on a compelling vision. 
Customer desires inform the vision and may even change it, but ultimately the product manager holds final responsibility for product direction.End-users often think in terms of making their existing work better, rather than in terms of finding completely new ways of working. 
For this reason, end-users should be involved but not in control. 
If innovation is important to your project, give innovative thinkers—such as a visionary product manager or interaction designer—a prominent role on your team.Real customer involvement is helpful but not crucial. 
Sometimes the best software comes from people who have a strong vision and pursue it vigorously. 
This software tends to be either completely new or a strong rethinking of existing products.In the absence of real customer involvement, be sure to have a visionary product manager. 
It's best if this person understands the domain well, but you can also hire domain experts to join the team.Still, feedback from real customers is always informative, even if you choose to ignore it. 
It's especially useful when you've deployed software to them; their reaction to working software gives you valuable information about how likely you are to reach the greatest levels of success.The following text is excerpted from The Art of Agile Development by James Shore and Shane Warden, published by O'Reilly. Copyright &copy 2008 the authors. All rights reserved.We understand each other.Try describing the business logic in your current system to a nonprogrammer domain expert. 
Are you able to explain how the system works in terms the domain expert understands?  Can you avoid programmer jargon, such as the names of design patterns or coding styles?  Is your domain expert able to identify potential problems in your business logic?If not, you need a ubiquituous language.One of the challenges of professional software development is that programmers aren't necessarily experts in the areas for which they write software. 
For example, I've helped write software that controls factory robots, directs complex financial transactions, and analyzes data from scientific instruments. 
When I started on these projects, I knew nothing about those things.It's a conundrum. 
The people who are experts in the problem domain—the domain experts—are rarely qualified to write software. 
The people who are qualified to write software—the programmers—don't always understand the problem domain.Overcoming this problem is, fundamentally, an issue of communication. 
Domain experts communicate their expertise to programmers, who in turn encode that knowledge in software. 
The challenge is communicating that information clearly and accurately.Imagine for a moment that you're driving to a job interview. 
You forgot your map, so you're getting directions from a friend on your cell phone (hands free, of course!):The problem in this scenario is that you and your friend are speaking two different languages. 
You're talking about what you see on the road and your friend is talking about what he sees on his map. 
You need to translate between the two, and that adds delay and error. You'll get to your job interview eventually, but you'll probably miss a few turns along the way and you might not get there on time.A similar problem occurs between programmers and domain experts. 
Programmers program in the language of technology: classes, methods, algorithms, and databases. 
Domain experts talk in the language of their domain: financial models, chip fabrication plants, and the like.You could try to translate between the two languages, but it will add delays and errors. 
You'd produce some software eventually, but you'd probably introduce some bugs along the way. 
Instead, pick just one language for the whole team to use... a ubiquitous language.Programmers should speak the language of their domain experts, not the other way around.Imagine that you're creating a piece of software for typesetting musical scores. 
The publishing house you're working for provides an XML description of the music and you need to render it properly. 
This is a difficult task, filled with seemingly minor stylistic choices that are vitally important to your customers.In this situation, you could focus on XML elements, parents, children, and attributes. 
You could talk about device contexts, bitmaps, and glyphs. 
If you did, your conversation might sound something like this:Instead, focus on domain terms rather than technical terms.The domain expert's answer is different the second time because he unsderstands the question. 
The conversation in the first example would have led to a bug.As a programmer, you might have trouble speaking the language of your domain experts. 
When you're working on a tough problem, it's difficult to make the mental translation from the language of code to the language of the domain.A better approach is to design your code to use the language of the domain. 
You can name your classes, methods, and variables anything. 
Why not use the terms that your domain experts use?This is more than learning the domain to write the software; this is reflecting in code how the users of the software think and speak about their work. 
By encoding your understanding of the domain, you refine your knowledge and—due to code's uncompromising need for precision—expose gaps in your knowledge that would otherwise result in bugs.To continue the example, a program to typeset a musical score based on XML input could be designed around XML concepts. 
A better approach, though, would be to design it around domain concepts, as shown in Figure.One powerful way to design your application to speak the language of the domain is to create a domain model. 
This process deserves its own book; [Evans] and [Wirfs-Brock & McKean] are two worthy examples.The ubiquitous language informs programmers, but the programmers' need for rigorous formalization also informs the rest of the team. 
I often see situations in which programmers ask a question—inspired by a coding problem—that in turn causes domain experts to question some of their assumptions.Your ubiquitous language, therefore, is a living language. 
It's only as good as its ability to reflect reality. 
As you learn new things, improve the language as well. 
There are three caveats about doing this, however.First, ensure that the whole team—especially the domain experts—understands and agrees with the changes you're proposing. 
This will probably require a conversation to resolve any conflicts. 
Embrace that!Second, check that the changes clarify your understanding of the business requirements. 
It may seem clearer to make a change, but the language must still reflect what the users need to accomplish with the software. 
Keep out programmer jargon—you can help domain experts refine their understanding of complicated corner cases, but don't replace their language with your own.Third, update the design of the software with the change. 
The model and the ubiquitous language must always stay in sync. 
A change to the language implies a change to the model. 
Yes, this does mean that you should refactor the code when your understanding of the domain changes. 
Delaying these changes introduces an insidious type of technical debt: a mismatch between your design and reality, which will lead to ugly kludges and bugs.It's okay to use technical language in areas that are unrelated to the domain. 
For example, it's probably best to call a database connection a "connection" and a button a "button."  However, you should typically encapsulate these technical details behind a domain-centric face.When you share a common language between customers and programmers, you reduce the risk of miscommunication. 
When you use this common language within the design and implementation of the software, you produce code that's easier to understand and modify.When the whole team uses the ubiquitous language while sitting together, everyone can overhear domain discussions, especially during pairing sessions. 
Team members overhear domain and implementation discussions and join in the conversation to resolve questions and expose hidden assumptions.If you don't have any domain experts sitting with your team, you may have trouble understanding the domain experts' thought process deeply enough to have a ubiquitous language. 
Attempting a ubiquitous language is even more important in this situation, though, as it will allow you to communicate more effectively with domain experts when you do have the opportunity to speak with them.On the other hand, some problems are so technical they don't involve non-programmer domain knowledge at all. 
Compilers and web servers are examples of this category. 
If you're building this sort of software, the language of technology is the language of the domain. 
You'll still have a ubiquitous language, but that language will be technical.Some teams have no experience creating domain-centric designs. 
If this is true of your team, proceed with caution. 
Domain-centric designs require a shift in thinking that can be difficult. 
See "Further Reading" at the end of this section to get started and consider hiring a programmer with experience in this area to help you learn.It's always a good idea to speak the language of your domain experts. 
However, avoiding a domain-centric design can lead to simpler designs in small, technology-centric projects involving trivial business rules. 
Be careful, though: this design approach leads to defects and complex, unmaintainable designs in larger projects. 
See [Fowler 2002a] for further discussion of this trade-off.Domain-Driven Design [Evans] is an excellent and thorough discussion of how to create a domain-centric design.Object Design [Wirfs-Brock & McKean] discusses roles, responsibilities, and behaviors in the context of modelling applications.Patterns of Enterprise Application Architecture [Fowler 2002a] has a good discussion of the trade-offs between domain models and other architectural approaches.We know what our teammates are doing.I have a special antipathy for status meetings. 
You know—a manager reads a list of tasks and asks about each one in turn. 
They seem to go on forever, although my part in them is typically only five minutes. 
I learn something new in perhaps ten of the other minutes. 
The remaining 45 minutes are pure waste.There's a good reason that organizations hold status meetings: people need to know what's going on. 
XP projects have a more effective mechanism: informative workspaces and the daily stand-up meeting.A stand-up meeting is very simple. 
At a pre-set time every day, the whole team stands in a circle. 
One at a time, each person briefly describes new information that the team should know.Some teams use a formal variant of the stand-up called the Daily Scrum [Schwaber & Beedle]. 
It comes from an agile process also called Scrum. 
In the Daily Scrum, participants specifically answer three questions:I prefer an informal approach, but both styles are valid. 
Try both and use whichever approach works best for you.One problem with stand-up meetings is that they interrupt the day. 
This is a particular problem for morning stand-ups: because team members know the meeting will interrupt their work, they sometimes wait for the stand-up to end before starting to work. 
If people arrive at different times, early arrivals sometimes just waste time until the stand-up starts. 
You can reduce this problem by moving the stand-up to later in the day, such as just before lunch.The purpose of a stand-up meeting is to give everybody a rough idea of where the team is. 
It's not to give a complete inventory of everything happening in the project. 
The primary virtue of the stand-up meeting is brevity. 
That's why we stand: our tired feet remind us to keep the meeting short.Each person usually only needs to say a few sentences about her status. 
Thirty seconds per person is usually enough. 
More detailed discussions should take place in smaller meetings with only the people involved. 
Here are some examples:A programmer:The product manager:A domain expert:A programmer responds:If the stand-up lasts longer than ten minutes—15 at the very most—it's taking too long. 
If people typically speak for thirty seconds each, then a team of ten should be able to have a five-minute stand-up meeting on most days.Brevity is a tough art to master. 
To practice, try writing your statement on an index card in advance, then read from the card during the stand-up.Another approach is to timebox the stand-up. 
Set a timer for five or ten minutes, depending on the size of the team. 
When the timer goes off, the meeting is over, even if there are some people who haven't spoken yet. 
At first, you'll find that the meeting is cut off prematurely, but the feedback should help people learn to speak more briefly after a week or two.If you're tactful, you can also interrupt extended reports or conversations and ask that people hold the discussion in a smaller group after the stand-up.Yes; I ask that outsiders stand outside the circle and not speak unless they have something brief and relevant to add.Some people, due to their position or personality, disrupt the smooth flow of the stand-up. 
If they're not members of the team, I prefer to use another mechanism to keep them up to date, such as the informative workspace, reports, and iteration demos. 
The product manager or project manager are probably the best people to manage this relationship.If they rush to finish quickly, participants might devolve into no-content statements like "same as yesterday" or "nothing new."  If this happens a lot, gently remind participants to go into a bit more detail.I've been tempted to introduce Mr. Laggard to Mr. Baseball Bat myself. 
Keep in mind that this is illegal in most countries and tough on team cohesiveness.Instead, the most effective way I know of combatting this problem is to start and to end meetings on time even if people are absent. 
Yes; you can either convene in a common location, or you can use speakerphones and a teleconference. 
If you can possibly stand together, do—stand-up meetings by teleconference are a lot less effective. 
I find that people tend to ramble.You can improve a teleconference stand-up by investing in good phone equipment, reminding people to stand up even when they're off-site, and being diligent about taking conversations offline.When you conduct daily stand-up meetings, the whole team is aware of issues and challenges that other team members face, and it takes action to remove them. 
Everyone knows the project's current status and what the other team members are working on.Don't let the daily stand-up stifle communication. 
Some teams find themselves waiting for the stand-up rather than going over and talking to someone when they need to. 
If you find this happening, eliminating the stand-up for a little while may actually improve communication.Beware of leaders who dominate the stand-up. 
As reviewer Jonathan Clarke so aptly put it, the ideal leader is "a charismatic but impatient colleague who will hurry and curtail speakers."  The stand-up is a meeting of equals—no one person should dominate.If you can't conduct a daily stand-up meeting, you need to stay in touch in some other way. 
If your team sits together, the resulting natural communication may be sufficient. 
Watch for unpleasant surprises that more communication can prevent.Another alternative is the traditional weekly status meeting. 
I find these more effective when team members submit their statuses to a single moderator who can present collated information in ten or fifteen minutes. 
However, I've also seen this approach fall apart quickly.It's Not Just Standing Up: Patterns for Daily Stand-up Meetings [Yip], at http://www.martinfowler.com/articles/itsNotJustStandingUp.html, is a nice collection of patterns for stand-up meetings.Stand-Up Meeting AntiPatterns [Miller], at http://fishbowl.pastiche.org/2003/11/19/standup_meeting_antipatterns, takes the opposite approach and describes common antipatterns and their solutions.Back in the days of the telegraph, as the story goes, telegraph operators could recognize each other on the basis of how they keyed their dots and dashes. 
Each operator had a unique style, or fist, that experts could recognize easily. 
Programmers have style, too. 
We each have our own way of producing code. 
We refine our style over years until we think it's the most readable, the most compact, or the most informative it can be.Individual style is great when you're working alone. 
In team software development, however, the goal is to create a collective work that is greater than any individual could create on his own. 
Arguing about whose style is best gets in the way; it's easier if we work together in a single style.XP suggests creating a coding standard: guidelines to which all developers agree to adhere when programming.I once led a team of four programmers who had widely differing approaches to formatting. 
When we discussed coding standards, I catalogued three different approaches to braces and tabs. 
Each approach had its own vigorous defender. 
I didn't want us to get bogged down in arguments, so I said that people could use whatever brace style they wanted.The result was predictable: we had three different approaches to formatting in our code. 
I even saw two different ways of indenting within a single, short method.You know what surprised me?  It wasn't that bad. 
Sure, the layout was ugly, and I would have preferred consistency, but the code was still readable. 
In the end, the rest of our coding standard mattered much more than formatting.We all agreed that clearly named variables and short methods were important. 
We agreed to use assertions to make our code fail fast, not to optimize without measurements, and never to pass null references between objects. 
We agreed on how we should and shouldn't handle exceptions, what to do about debugging code, and when and where to log events. 
These standards helped us far more than a consistent formatting style would have because each one had a concrete benefit. 
Perhaps that's why we were able to agree on them when we couldn't agree on formatting styles.Don't get me wrong: a consistent formatting standard is good. 
If you can agree on one, do!  However, when you're putting together your coding standard, don't fall into the trap of arguing about formatting. 
There are more important issues.Creating a coding standard is an exercise in building consensus. 
It may be one of the first things that programmers do as a team. 
Over time, you'll amend and improve the standards. 
The most important thing you may learn from creating the coding standard is how to disagree constructively.To that end, I recommend applying two guidelines:Hold your first discussion of coding standards during the first iteration. 
The project will typically start out with some discussion of stories and vision, then some release planning and iteration planning (see Go! in Chapter 4). 
After iteration planning, customers and testers will continue working on the release plan. 
That's a great time for programmers to talk about coding standards.The best way to start your coding standard is often to select a industry-standard style guide for your language. 
This will take care of formatting questions and allow you to focus on design-related questions. 
If you're not sure what it should encompass, starting points include:Limit your initial discussion to just one hour. 
Write down what you agree on. 
If you disagree about something, move on. 
You can come back to it later.If you have trouble, take a step back and talk about your goals for the software and the results you would like to see. 
Agree about these issues first, even if you disagree about specific approaches. 
You will have many opportunities to improve your standard. 
Make the most important decisions now, and move on.Depending on your team, this may be a contentious discussion. 
In that case, consider bringing in a professional facilitator to redirect the discussion to your team goals when things gets heated. 
Your HR department might be able to provide someone, or you can use an outside consultant.Plan to hold another one-hour coding standard meeting a few days later, and another one a few weeks after that. 
The long break will allow you to learn to work together and to try out your ideas in practice. 
If there's still disagreement, experiment with one approach or the other, then revisit the issue.Hold these initial meetings as often as they're useful. 
After that, change the standard at any time. 
Just stand up, announce your intention to the team, and, if everybody agrees, change the flip chart. 
Retrospectives are another good time to discuss changes to the coding standard.Over time, some of the items in the standard will become second nature. 
Cross them off to make room for more important issues. 
As you work together, you will recognize ways in which new standards can help. 
Add these new standards to the list in the same way as before, as long as everybody agrees to try them.No matter what standards you choose, someone will be probably unhappy with some guideline even with a consensus-based approach. 
You'll probably find some practices jarring and grating at first. 
Over time, you'll get used to it. 
Coding standards are, in many ways, an aesthetic choice: it doesn't really matter what the standard is, as long as it's consistent and thoughtful. 
One of the marks of a professional is the willingness to put aside personal aesthetics for a team aesthetic.It's possible to pressure a dissenter into accepting a coding standard she doesn't agree with, but it's probably not a good idea. 
Doing so is a good way to create resentment and discord.Instead, remember that few decisions are irrevocable in agile development; mistakes are opportunities to learn and improve. 
Ward Cunningham put it well:11http://en.wikiquote.org/wiki/Ward_CunninghamGo ahead and leave the contested item out of the standard. 
Maybe lack of standardization in that area will lead to a mess. 
If it does, the team will learn from the experience and you can change the standard.People make mistakes. 
Pair programming helps developers catch mistakes and maintain self-discipline. 
It provides a way to discuss formatting and coding questions not addressed by the guidelines. 
It's also an excellent way to improve the standard; it's much easier to suggest an improvement when you can talk it over with someone first.Collective code ownership also helps people adhere to the standard, because many different people will edit the same piece of code. 
Code tends to settle on the standard as a result.There are less effective approaches. 
Some teams use automated tools to check their source code for adherence to the coding standard. 
Others program their version control system to reformat files upon check-in. 
I don't like either approach: to me, the latter says that you don't trust people to make good decisions on their own, and the former tends to raise false warnings.I've also heard of teams who elevate their coding standards to requirements and punish infractions. 
The idea of enforcing a coding standard leaves a bad taste in my mouth. 
Your teammates are presumably professionals who pride themselves on doing good work. 
No coding standard can substitute for professional judgment. 
Try not to get too upset when you see people deviating from the standard.Assume your colleagues are professional and well-meaning. 
If someone is not following the standard, assume that there's a good reason—even if all the evidence is to the contrary. 
Your challenge is to find that reason and address it. 
This approach shows respect for others and this will improve others' respect for you.Start by talking with your colleague alone to see if there's a disagreement. 
Take an attitude of collaborative problem solving: instead of saying "Why aren't you propagating exceptions like we agreed?" ask, "What do you think about the 'propagate exceptions' standard we agreed on?  Should we keep it?"  Give objections full consideration, raise them with the rest of the team, and consider changing the standard.If the objector agrees with the standard but isn't applying it, it's possible that the standard isn't appropriate in every situation. 
Ask about specific cases you've noticed. 
Again, be collaborative, not confrontational. 
Say something like, "I think we're on the same page regarding the importance of propagating exceptions. 
In that case, can you explain what's happening in this method?  I don't understand why this code doesn't propagate the exception here."During this discussion, you may learn that the objector doesn't understand the standard. 
By this time, you should be in a good situation to discuss the standard and what it means. 
If he's a junior programmer and needs more help, coordinate with the rest of the team to make sure he gets plenty of pairing time with experienced developers.There is another possibility for teams new to XP. 
Switching to XP is a big change and can make people feel like they've lost control. 
Sometimes they react by picking small things that they refuse to change. 
An obstinate desire to stick with a particular coding standard, regardless of the wishes of the rest of the team, might be a symptom of this reaction.In this case, your best solution may be to let the infractions slide for several months. 
Over time, as team members become more comfortable with the changes in their environment, they'll relax and be more willing to compromise.Leave old code alone if it works and you don't need to read or touch it otherwise. 
It's expensive and risky to spend a lot of time fixing legacy code upfront. 
Instead, as you modify and refactor those sections of code, bring them up to the new coding standards. 
When you fix a bug, add a feature, or improve abstraction and factoring, use the new standards on everything you modify.You can also use an automated tool to perform large-scale formatting changes. 
Don't spend too much time on this, but if you can do it easily, you might as well. 
I prefer to integrate immediately before and after such an operation, because reformatting changes tend to disguise other changes. 
Be aware that making such large-scale changes can render your version control system's change history much more difficult to read.When you agree on coding standards and conventions, you improve the maintainability and readability of your code. 
You can take up different tasks in different subsystems with greater ease. 
Pair programming moves much more smoothly, and you look for ways to improve the expressability and robustness of your code as you write it.Don't allow coding standards to become a divisive issue for your team.Some teams work so well together they don't need an written coding standard; their coding standard is implicit.If you have a new team, create a written coding standard even if everybody gets along well. 
New teams often go through an initial honeymoon period in which team members are reluctant to disagree with each other. 
Eventually, disagreements will come out. 
It's much better to create a standard before problems escalate.1The Agile Manifesto, http://www.agilemanifesto.org/Like a rock at the top of a hill, code has potential—potential energy for the rock and potential value for the code. 
It takes a push to realize that potential. 
The rock has to be pushed onto a slope in order to gain kinetic energy; the software has to be pushed into production in order to gain value.It's easy to tell how much you need to push a rock. 
Big rock?  Big push. 
Little rock?  Little push. 
Software isn't so simple—it often looks ready to ship long before it's actually done. 
It's my experience that teams underestimate how hard it will be to push their software into production.To make things worse, software's potential value changes. 
If nothing ever pushes that rock, it will sit on top of its hill forever; its potential energy won't change. 
Software, alas, sits on a hill that undulates. 
You can usually tell when your hill is becoming a valley, but if you need weeks or months to get your software rolling, it might be sitting in a ditch by the time you're ready to push.In order to meet commitments and take advantage of opportunities, you must be able to push your software into production within minutes. 
This chapter contains seven practices that give you leverage to turn your big release push into a ten-minute tap:2The value stream map was inspired by [Poppendieck & Poppendieck].We're done when we're production-ready."Hey, Liz!" Rebecca sticks her head into Liz's office. 
"Did you finish that new feature yet?"Liz nods. 
"Hold on a sec," she says, without pausing in her typing. 
A flurry of keystrokes crescendos and then ends with a flourish. 
"Done!"  She swivels around to look at Rebecca. 
"It only took me half a day, too.""Wow, that's impressive," says Rebecca. 
"We figured it would take at least a day, probably two. 
Can I look at it now?""Well, not quite," says Liz. 
"I haven't integrated the new code yet.""Okay," Rebecca says. 
"But once you do that, I can look at it, right?  I'm eager to show it to our new clients. 
They picked us precisely because of this feature. 
I'm going to install the new build on their test bed so they can play with it."Liz frowns. 
"Well, I wouldn't show it to anybody. 
I haven't tested it yet. 
And you can't install it anywhere—I haven't updated the installer or the database schema generator.""I don't understand," Rebecca says irritably. 
"I thought you said you were done!""I am," insists Liz. 
"I finished coding just as you walked in. 
Here, I'll show you.""No, no, I don't need to see the code," Rebecca says. 
"I need to be able to show this to our customers. 
I need it to be finished. 
Really finished.""Well, why didn't you say so?" says Liz. 
"This feature is done—all coded up. 
It's just not done done. 
Give me a few more days."Wouldn't it be nice if, once you finished a story, you never had to come back to it?  That's the idea behind "done done."  A completed story isn't a lump of unintegrated, untested code. 
It's ready to deploy.Partially-finished stories result in hidden costs to your project. 
When it's time to release, you have to complete an unpredictable amount of work. 
This destabilizes your release planning efforts and prevents you from meeting your commitments.To avoid this problem, make sure all of your planned stories are "done done" at the end of each iteration. 
You should be able to deploy the software at the end of any iteration, although normally you'll wait until more features have been developed.What does it take for software to be "done done"?  That depends on your organization. 
I often explain that a story is only complete when the customers can use it as they intended. 
Create a checklist that shows the story completion criteria. 
I write mine on the iteration planning board:Some teams add "Documented" to this list, meaning that the story has documentation and help text. 
This is most appropriate when you have a technical writer as part of your team.Other teams include "Performance" and "Scalability" in their "done done" list, but these can lead to premature optimization. 
I prefer to schedule performance, scalability, and similar issues with dedicated stories (see Performance Optimization in Chapter 9).XP works best when you make a little progress on every aspect of your work every day, rather than reserving the last few days of your iteration for getting stories "done done."  This is an easier way to work, once you get used to it, and it reduces the risk of finding unfinished work at the end of the iteration.Use test-driven development to combine testing, coding, and designing. 
When working on an engineering task, make sure it integrates with the existing code. 
Use continuous integration and keep the ten-minute build up to date. 
Create an engineering task (see Iteration Planning in Chapter 9) for updating the installer and have one pair work on it in parallel with the other tasks for the story.Just as importantly, include your on-site customers in your work. 
As you work on a UI task, show a customer what the screen will look like, even if it doesn't work yet (see Incremental Requirements in Chapter 9). 
Customers often want to tweak a UI when they see it for the first time. 
This can lead to a surprising amount of last-minute work if you delay any demos to the end of the iteration.Similarly, as you integrate various pieces, run the software to make sure they all work together. 
While this shouldn't take the place of testing, it's a good check to help prevent you from missing anything. 
Enlist the help of the testers on occasion and ask them to show you exploratory testing techniques. 
(Again, this review doesn't replace real exploratory testing.)Throughout this process, you may find mistakes, errors, or outright bugs. 
When you do, fix them right away—then improve your work habits to prevent that kind of error from occurring again.When you believe the story is "done done," show it to your customers for final acceptance review. 
Because you reviewed your progress with customers throughout the iteration, this should only take a few minutes.This may seem like an impossibly large amount of work to do in just one week. 
It's easier to do if you work on it throughout the iteration rather than saving it up for the last day or two. 
The real secret, though, is to make your stories small enough that you can completely finish them all in a single week.Many teams new to XP create stories that are too large to get "done done."  They finish all of the coding, but they don't have enough time to completely finish the story—perhaps the UI is a little off, or a bug snuck through the cracks.Remember, you are in control of your schedule. 
You decide how many stories to sign up for and how big they are. 
Make any story smaller by splitting it into multiple parts (see Stories in Chapter 8) and only working on one of the pieces this iteration.Creating large stories is a natural mistake, but some teams compound the problem by thinking, "well, we really did finish the story, except for that one little bug."  They give themselves credit for the story, which inflates their velocity and perpetuates the problem.If you have trouble getting your stories "done done," don't count those stories towards your velocity (see Estimating in Chapter 8). 
Even if the story only has a few minor UI bugs, count it as a zero when calculating your velocity. 
This will lower your velocity, which will help you choose a more manageable amount of work in your next iteration.You may find that this lowers your velocity so much that you can only schedule one or two stories in an iteration. 
That means that your stories were too large to begin with. 
Split the stories you have and work on making future stories smaller.In addition to helping customers and programmers, testers are responsible for nonfunctional testing and exploratory testing. 
Typically, they do these only after stories are "done done". 
They may perform some non-functional tests, however, in the context of a specific performance story.Regardless, the testers are part of the team, and everyone on the team is responsible for helping the team meet its commitment to deliver "done done" stories at the end of the iteration. 
Practically speaking, this usually means that testers help customers with customer testing, and they may help programmers and customers review the work in progress.If you can absorb the change with your slack, go ahead and make the change. 
Turn larger changes into new stories.This sort of feedback is normal—expect it. 
The product manager should decide whether the changes are appropriate, and if they are, he should modify the release plan. 
If you are constantly surprised by stakeholder changes, consider whether your on-site customers truly reflect your stakeholder community.When your stories are "done done," you avoid unexpected batches of work and spread wrap-up and polish work throughout the iteration. 
Customers and testers have a steady workload through the entire iteration. 
The final customer acceptance demonstration takes only a few minutes. 
At the end of each iteration, your software is ready to demonstrate to stakeholders with the scheduled stories working to their satisfaction.This practice may seem advanced. 
It's not, but it does require self-discipline. 
To be "done done" every week, you must also work in iterations and use small, customer-centric stories.In addition, you need a whole team—one that includes customers and testers (and perhaps a technical writer as well) in addition to programmers. 
The whole team must sit together. 
If they don't, the programmers won't be able to get the feedback they need in order to finish the stories in time.Finally, you need incremental design and architecture and test-driven development in order to test, code, and design each story in such a short timeframe.This practice is the cornerstone of XP planning. 
If you aren't "done done" at every iteration, your velocity will be unreliable. 
You won't be able to ship at any time. 
This will disrupt your release planning and prevent you from meeting your commitments, which will in turn damage stakeholder trust. 
It will probably lead to increased stress and pressure on the team, hurt team morale, and damage the team's capacity for energized work.The alternative to being "done done" is to fill the end of your schedule with make-up work. 
You will end up with an indeterminate amount of work to fix bugs, polish the UI, create an installer, and so forth. 
Although many teams operate this way, it will damage your credibility and your ability to deliver. 
I don't recommend it.Let's cook up a bug pie. 
First, start with a nice, challenging language. 
How about C?  We'll season it with a dash of assembly.Next, add extra bugs by mixing in concurrent programming. 
Our old friends Safety and Liveness are happy to fight each other over who provides the most bugs. 
They supplied the Java multithreading library with bugs for years!Now we need a really difficult problem domain. 
How about real-time embedded systems?To top off this disaster recipe, we need the right kind of programmers. 
Let's see... experts... no... senior developers... no... aha!  Novices!  Just what we need.Take your ingredients—C, real-time embedded systems, multitasking—and don't forget the novices—add a little assembly for seasoning, shake well, and bake for three years. 
(I do love me a bug pie.)Here's how it turns out:1"Embedded Agile Project By the Numbers with Newbies" [Van Schooenderwoert]These folks had everything stacked against them—except their coach and her approach to software development. 
If they can do it, so can you.If you're on a team with a bug-count in the hundreds, the idea of "no bugs" probably sounds ridiculous. 
I'll admit: "no bugs" is an ideal to strive for, not something your team will necessarily achieve.However, XP teams can achieve dramatically lower bug rates. 
[Van Schooenderwoert]'s team averaged one and a half bugs per month in a very difficult domain. 
In an independent analysis of a company practicing a variant of XP, QSM Associates reported an average reduction from 2,270 defects to 381 defects [Mah].You might think that improvements like this are terribly expensive. 
They're not. 
In fact, agile teams tend to have above-average productivity.22See, for example, [Van Schooenderwoert], [Mah], and [Anderson 2006].Evidence for these results is as yet anecdotal. 
Rigorous scientific studies of complete software development methodologies are rare due to the large number of variables involved. 
While there are organizations that draw performance conclusions by aggregating hundreds of projects, none that I am aware of have enough data to draw conclusions on agile projects in general, let alone XP specifically. 
(QSM Associates is a well-regarded example of such an organization; as of June 2006, they only had data from a few agile projects.3)3Personal communication with Michael Mah of QSM Associates.In the absence of conclusive proof, how can you know if your team will achieve these results?  There's only one way to know for sure: try it and see. 
It doesn't take superpowers. 
Teams of novices coached by an experienced developer have done this. 
All you need is commitment to follow the XP practices rigorously and support from your organization to do so.Many approaches to improving software quality revolve around finding and removing more defects4 through traditional testing, inspection, and automated analysis.4I use "defect" synonymously with "bug".The agile approach is to generate fewer defects. 
This isn't a matter of finding defects earlier; it's a question of not generating them at all.For example, [Van Schooenderwoert] delivered 21 bugs to customers. 
Working from Capers Jones' data, Van Schooenderwoert says that a "best in class" team building their software would have generated 460 defects, found 95 percent of them, and delivered 23 to their customer.5  In contrast, Van Schooenderwoert's team generated 51 defects, found 59 percent of them, and delivered 21 to their customer. 
At 0.22 defects per function point, this result far exceeds Capers Jones' "best in class" expectation of two defects per function point.5An "average" team would have generated 1,035, found 80%, and delivered 207.To achieve these results, XP uses a potent cocktail of techniques:This may seem like a lot to do, but most of it comes naturally as part of the XP process. 
Most of these activities improve productivity by increasing code quality or removing obstacles. 
If you do them as part of XP, you don't have to do many of the other more expensive activities that other teams perform, such as an exhaustive upfront requirements gathering phase, disruptive code inspections, or a separate bug-fixing phase.Don't worry—I'm not going to wave my hands and say, "Too many bugs?  No problem!  Just write fewer bugs!"  To stop writing bugs, you have to take a rigorous, thoughtful approach to software development.Start with test-driven development (TDD), which is a proven technique for reducing the number of defects you generate [Janzen & Saiedian]. 
It leads to a comprehensive suite of unit and integration tests, and perhaps more importantly, structures your work into small, easily-verifiable steps. 
Teams using TDD report that they rarely need to use a debugger.To enhance the benefits of test-driven development, work sensible hours and program all production code in pairs. 
This improves your brainpower, which helps you make fewer mistakes and allows you to see mistakes more quickly. 
Pair programming also provides positive peer pressure, which helps you maintain the self-discipline you need to follow defect-reduction practices.Test-driven development helps you eliminate coding defects, but code isn't your only source of defects. 
You can also produce good code that does the wrong thing. 
To prevent these requirements-oriented defects, work closely with your stakeholders. 
Enlist on-site customers to sit with your team. 
Use customer tests to help communicate complicated domain rules. 
Have testers work with customers to find and fix gaps in their approach to requirements. 
Demonstrate your software to stakeholders every week, and act on their feedback.Supplement these practices with good coding standards and a "done done" checklist. 
These will help you remember and avoid common mistakes.Writing fewer bugs is an important first step to reducing the number of defects your team generates. 
If you accomplish that much, you're well ahead of most teams. 
Don't stop now, though. 
You can generate even fewer defects.Even with test-driven development, your software will accumulate technical debt over time. 
Most of it will be in your design, making your code defect-prone and difficult to change, and it will tend to congregate in specific parts of the system. 
According to [Boehm], about 20% of the modules in a program are typically responsible for about 80% of the errors.These design flaws are unavoidable. 
Sometimes a design that looks good when you first create it won't hold up over time. 
Sometimes a shortcut that seemed like an acceptable compromise will come back and bite you. 
Sometimes your requirements will change and your design will need to change as well.Whatever its cause, technical debt leads to complicated, confusing code that's hard to get right. 
It breeds bugs. 
To generate fewer defects, pay down your debt.Although you could dedicate a week or two to fixing these problems, the best way to pay off your debt is to make small improvements every week. 
Keep new code clean by creating simple designs and refactoring as you go. 
Use the slack in each iteration to pay down debt in old code.Programmers have long known that the longer you wait to fix a bug, the more it costs to fix [McConnell] (p.75). 
In addition, unfixed bugs probably indicate further problems. 
Each bug is the result of a flaw in your system that's likely to breed more mistakes. 
Fix it now and you'll improve both quality and productivity.To fix the bug, start by writing an automated test that demonstrates the bug. 
It could be a unit test, integration test, or customer test, depending on what kind of defect you've found. 
Once you have a failing test, fix the bug. 
Get a green bar.Don't congratulate yourself yet—you've fixed the problem, but you haven't solved the underlying cause. 
Why did that bug occur?  Discuss the code with your pairing partner. 
Is there a design flaw that made this bug possible?  Can you change an API to make such bugs more obvious?  Is there some way to refactor the code that would make this kind of bug less likely?  Improve your design. 
If you've identified a systemic problem, discuss it with the rest of your team in your next stand-up meeting or iteration retrospective. 
Tell people what went wrong so they can avoid that mistake in the future.Fixing bugs quickly requires the whole team to participate. 
Programmers, use collective code ownership so any pair can fix a buggy module. 
Customers and testers, personally bring new bugs to the attention of a programmer and help him reproduce it. 
These actions are easiest when the whole team sits together.In practice, it's not possible to fix every bug right away. 
You may be in the middle of working on something else when you find a bug. 
When this happens to me, I ask my navigator to write the problem on our to-do list. 
We come back to it 10 or 20 minutes later, when we come to a good stopping point.Some bugs are too big to fix while you're in the middle of another task. 
For these, I write the bug on a story card and announce it to the whole team. 
(Some teams use red story cards for this purpose.)  We collectively decide if there's enough slack in the iteration to fix the bug and still meet our other commitments. 
If there is, we create tasks for the newly-created story and pairs volunteer for them as normal. 
Sometimes your only task will be "fix the bug."  I use the story card as its own task card when this happens.If there isn't enough slack to fix the bug, estimate the cost to fix it and ask your product manager to decide whether to fix it in this release. 
If it's important enough to fix, schedule it into the very next iteration.6Thanks to Ron Jeffries for this insight.These practices will dramatically cut down on the number of bugs in your system. 
However, they only prevent bugs you expect. 
Before you can write a test to prevent a problem, you have to realize the problem can occur.Exploratory testing, in the hands of a good—some might say diabolical—tester, is an invaluable way to broaden your understanding of the types of problems that can occur. 
An exploratory tester uses her intuition and experience to tell her what kinds of problems programmers and customers have the most trouble considering. 
For example, she might unplug her network cable in the middle of an operation or perform a SQL injection attack on your database.If your team had typical adversarial relationships between programmers, customers, and testers, these sorts of unfair tests might elicit bitter griping from programmers. 
Remember, though—you're all in this together. 
The testers have exposed a hole in your thought process and, by doing so, saved you from uncomfortable apologies to stakeholders or from dramatic failures in production. 
Congratulate your testers on their ingenuity—and as you write tests, remember the problems they have exposed.Exploratory testing is a very effective way of finding unexpected bugs. 
It's so effective that the rest of the team might start to get a little lazy.Don't rely on exploratory testing to find bugs in your software. 
(Really!)  Your primary defense against bugs is test-driven development and all of the other good practices I've mentioned. 
Instead, use exploratory testing to test your process. 
When an exploratory test finds a bug, it's a sign that your work habits—your process—have a hole in them. 
Fix the bug, then fix your process.Testers, only conduct exploratory testing on stories that the team agrees are "done done."  Programmers and customers, if your testers find any problems, think of them as bugs. 
Take steps to prevent them from occurring, just as you would for any other bug. 
Aim for a defect rate of under one or two bugs per month including ones found in exploratory testing.A good exploratory tester will find more bugs than you expect. 
To make the bug rate go down, fix your process.Some bugs occur because we're human. 
(D'oh!)  More often, bugs indicate an unknown flaw in our approach. 
When the number of bugs you generate gets low enough, you can do something usually associated with NASA's Space Shuttle software: root-cause analysis and process improvement on every bug.When you fix a bug, start by writing an automated test and improving your design to make the bug less likely. 
This is the beginning of root-cause analysis, but you can go even further.As you write the test and fix the design, ask questions. 
Why was there no test preventing this bug?  Why does the design need fixing?  Use the "five whys" technique to consider the root cause. 
Then, as a team, discuss possible root causes and decide how best to change your work habits to make that kind of bug more difficult.Next, ask yourselves if the root cause of this bug could also have led to other bugs that you haven't yet found. 
Testers, ask yourselves if this type of mistake reveals a blind spot in the team's thinking. 
Perform additional testing to explore these questions.If you follow these practices, bugs should be a rarity. 
Your next step is to treat them that way. 
Rather than shrugging your shoulders when a bug occurs—"Oh yeah, another bug, that's what happens in software"—be shocked and dismayed. 
"That's our fourth bug this month!  Why do we have so many bugs?"I'm not so naïve as to suggest that the power of belief will eliminate bugs, but if you're already close, the shift in attitude will help you make the leap from reducing bugs to nearly eliminating bugs. 
You'll spend more energy on discovering and fixing root causes.For this reason, I recommend that new XP teams not install a bug database. 
If you're only generating a bug or two per month, you don't need a database to keep track of your bugs; you can just process them as they come in. 
Explicitly not providing a database helps create the attitude that bugs are abnormal. 
It also removes the temptation to use the database as a substitute for direct, personal communication.Depending on your industry, you may need a formal way to track defects, so a database may be appropriate. 
However, I never assume that a database will be necessary until the requirements of the situation prove it. 
Even then, I look for ways to use our existing process to meet the regulatory or organizational requirements. 
Although some shudder at the informality, archiving red bug cards in a drawer may suffice.Even if you are able to eliminate your bug database, you still need to be able to reproduce bugs. 
You may need screenshots, core dumps, and so forth. 
If you fix bugs as soon as they come in, you may be able to work directly out of your email inbox. 
Otherwise, you can keep this information in the same place that you keep other requirements details. 
(See Incremental Requirements in Chapter 9.)All else remaining equal, experienced developers will always produce better results and more quickly than novices. 
If you have the option to bring high-quality people into your team, do it.The point is that these practices are within the grasp of average teams—even teams of novices, as long as they have an experienced coach. 
They won't achieve zero bugs, but they are likely to achieve better results than they would otherwise.This approach only prevents bugs that you think to prevent. 
Security, concurrency, and other difficult problem domains may introduce defects that you never considered.In this situation, using exploratory testing to test and fix your process is even more important. 
You may need to hire a specialist tester, such as a security expert, to probe for problems and teach the team how to prevent such problems in the future.It depends on how much slack you have left in the iteration. 
Early in the iteration, when there's still a lot of slack, I might spend as much as four pair-hours on a defect. 
Later, when there's less slack, I might only spend ten minutes on it.Bugs are usually harder to find than to fix, so enlist the help of the testers. 
The fix often takes mere minutes once you've isolated the problem.Perhaps. 
This is one reason we pair: pairing helps us maintain our team discipline. 
If you find yourself succumbing to the temptation to ignore a bug, write it on a story card rather than letting it slide by. 
Let the rest of the team know about the bug and ask somebody to volunteer to fix it.It's a bit of conundrum: the team is supposed to prevent bugs from occurring in "done done" stories, so exploratory testing shouldn't find anything. 
Yet if exploratory testing doesn't find anything, you could be testing the wrong things.If bugs are particularly devastating for your project, ask an independent testing group to test a few of your iteration releases. 
If they don't find anything surprising, then you can have confidence in your exploratory testing approach.This is probably overkill for most teams. 
In practice, if you're following the practices and your testers haven't found anything, you can comfortably release your software. 
Re-evaluate your approach if your stakeholders or customers find a significant bug.Most legacy code doesn't have any tests and is chock-full of bugs. 
You can dig your way out of this hole, but it will take a lot of time and effort. 
See "Applying XP to an Existing Project" in Chapter 4 for details.When you produce nearly zero bugs, you are confident in the quality of your software. 
You're comfortable releasing your software to production without further testing at the end of any iteration. 
Stakeholders, customers, and users rarely encounter unpleasant surprises, and you spend your time producing great software instead of fighting fires."No Bugs" depends on the support and structure of all of XP. 
To achieve these results, you need to practice nearly all of the XP practices rigorously:If you aren't using all these practices, don't expect dramatic reductions in defects. 
Conversely, if you have a project that's in XP's sweet spot (see Is XP Right For Us? in Chapter 4) and you're using all of the XP practices, more than a few bugs per month may indicate a problem with your approach to XP. 
You need time to learn the practices, of course, but if you don't see improvements in your bug rates within a few months, consider asking for help (see "Find a Mentor" in Chapter 2).You can also reduce bugs by using more and higher quality testing (including inspection or automated analysis) to find and fix a higher percentage of bugs. 
However, testers will need some time to review and test your code, which will prevent you from being "done done" and ready to ship at the end of each iteration."Embedded Agile Project By the Numbers with Newbies" [Van Schooenderwoert] expands on the embedded C project described in the introduction.We keep all of our project artifacts in a single, authoritative place.To work as a team, you need some way to coordinate your source code, tests, and other important project artifacts. 
A version control system provides a central repository which helps coordinate changes to files and also provides a history of changes.A project without version control may have snippets of code scattered among developer machines, networked drives, and even removable media. 
The build process may involve one or more people scrambling to find the latest versions of several files, trying to put them in the right places, and only succeeding through the application of copious caffeine, pizza, and stress.A project with version control uses the version control system to mediate changes. 
It's an orderly process in which developers get the latest code from the server, do their work, run all of the tests to confirm that their code works, then check in their changes. 
This process, called continuous integration, occurs several times a day for each pair.If multiple developers modify the same file without using version control, they're likely to accidentally overwrite each other's changes. 
To avoid this pain, some developers turn to a locking model of version control: when they work on a file, they lock it to prevent anyone else from making changes. 
The files in their sandboxes are read-only until locked. 
If you have to check out a file in order to work on it, then you're using a locking model.While this approach solves the problem of accidentally overwriting changes, it can cause other, more serious problems. 
A locking model makes it difficult to make changes. 
Team members have to carefully coordinate who is working on which file, and that stifles their ability to refactor and make other beneficial changes. 
To get around this, teams often turn to strong code ownership, the worst of the code ownership models because only one person has the authority to modify a particular file. 
Collective code ownership is a better approach, but it's very hard to do if you use file locking.Instead, use a concurrent model of version control. 
This  model allows two people to edit the same file simultaneously. 
The version control system automatically merges their changes—nothing gets overwritten accidentally. 
If two people edit the exact same lines of code, the version control system prompts them to merge the two lines manually.Automatic merges may seem risky. 
They would be risky if it weren't for continuous integration and the automated build. 
Continuous integration reduces the scope of merges to a manageable level, and the build, with its comprehensive test suite, confirms that merges work properly.One of the most powerful uses of a version control system is the ability to go back in time. 
You can update your sandbox with all the files from a particular point in the past.This allows you to use diff debugging. 
When you find a challenging bug that you can't debug normally, go back in time to an old version of the code when the bug didn't exist. 
Then go forward and backwards until you isolate the exact check-in that introduced the bug. 
You can review the changes in that check-in alone to get insight into the cause of the bug. 
With continuous integration, the number of changes will be small.1Thanks to Andreas Kö for demonstrating this.Time travel is also useful for reproducing bugs. 
If somebody reports a bug and you can't reproduce it, try using the same version of the code that the reporter is using. 
If you can reproduce the behavior in the old version but not in the current version, especially with a unit test, you can be confident that the bug is and will remain fixed.It should be obvious that you should store your source code in version control. 
It's less obvious that you should store everything else in there, too. 
Although most version control systems allow you to go back in time, it doesn't do you any good unless you can build the exact version you had at that time. 
Storing the whole project in version control—including the build system—gives you the ability to re-create old versions of the project in full.As much as possible, keep all of your tools, libraries, documentation, and everything else related to the project in version control. 
Tools and libraries are particularly important. 
If you leave them out, at some point you'll update one of them, and then you'll no longer be able to go back to a time before the update. 
Or, if you do, you'll have to painstakingly remember which version of the tool you used to use and manually replace it.For similar reasons, store the whole project in a single repository. 
Although it may seem natural to split the project into multiple repositories—perhaps one for each deliverable, or one for source code and one for documentation—this approach increases the opportunities for things to get out of sync.Perform your update and commit actions on the whole tree as well. 
Typically, this means updating or committing from the top-level directory. 
It may be tempting to commit only the directory you've been working in, but that leaves you vulnerable to the possibility of having your sandbox split across two separate versions.The only project-related artifact I don't keep in version control is generated code. 
Your automated build should should re-create generated code automatically.There is one remaining exception to what belongs in version control: code you plan to throw away. 
Spike solutions (see Spike Solutions in Chapter 9), experiments, and research projects may remain unintegrated, unless they produce concrete documentation or other artifacts that will be useful for the project. 
Check in only the useful pieces of the experiment. 
Discard the rest.Customer data should go in the repository, too. 
That includes documentation, notes on requirements (see Incremental Requirements in Chapter 9), technical writing such as manuals, and customer tests (see Customer Tests in Chapter 9).When I mention this to programmers, they worry that the version control system will be too complex for customers to use. 
Don't underestimate your customers. 
While it's true that some version control systems are very complex, most have user-friendly interfaces. 
For example, the TortoiseSvn Windows client for the open-source Subversion version control system is particularly nice.Even if your version control system is somewhat arcane, you can always create a pair of simple shell scripts or batch files—one for update and one for commit—and teach your customers how to run them. 
If you sit together, you can always help your customers when they need something more sophisticated, such as time travel or merging.One of the most important ideas in XP is that you keep the code clean and ready to ship. 
It starts with your sandbox. 
Although you have to break the build in your sandbox in order to make progress, confine it to your sandbox. 
Never check in code that breaks the build. 
This allows anybody to update at any time without worrying about breaking their build—and that, in turn, allows everyone to work smoothly and share changes easily.Because your build automatically creates a release, any code that builds is theoretically ready to release. 
In practice, the code may be clean but the software itself won't be ready for the outside world. 
Stories will be half done, user interface elements will be missing, and some things won't entirely work.By the end of each iteration, you will have finished up all of these lose ends. 
Each story will be "done done," and you will deploy the software to stakeholders as part of your iteration demo. 
This software represents a genuine increment of value for your organization. 
Make sure you can return to it at any time by tagging the tip of the repository. 
I usually name mine "Iteration X," where X is the number of iterations we have conducted.Not every end-of-iteration release to stakeholders gets released to customers. 
Although it contains completed stories, it may not have enough to warrant a release. 
When you conduct an actual release, add another tag to the end-of-iteration build to mark the release. 
I usually name mine "Release Y," where Y is the number of releases we have conducted.To summarize, your code goes through four levels of completion:One of the most devastating mistakes a team can make is to duplicate their codebase. 
It's easy to do. 
First, a customer innocently requests a customized version of your software. To deliver this version quickly, it seems simple to duplicate the codebase, make the changes, and ship it. Yet that copy and paste customization doubles the number of lines of code that you need to maintain.I've seen this cripple a team's ability to deliver working software on a timely schedule. 
It's nearly impossible to recombine a duplicated codebase without heroic and immediate action. 
That one click doesn't just lead to technical debt; it leads to indentured servitude.Unfortunately, version control systems actually make this mistake easier to make. 
Most of these systems provide the option to branch your code—that is, to split the repository into two separate lines of development. 
This is essentially the same thing as duplicating your codebase. 
Branches have their uses, but using them to provide multiple customized versions of your software is risky. 
Although version control systems provide mechanisms for keeping multiple branches synchronized, doing so is tedious work that steadily becomes more difficult over time. 
Instead, design your code to support multiple configurations. 
Use a plug-in architecture, a configuration file, or factor out a common library or framework. 
Top it off with a build and delivery process that creates multiple versions.Branches work best when they are short-lived or when you use them for small numbers of changes. 
If you support old versions of your software, a branch for each version is the best place to put bug fixes and minor enhancements for those versions. 
Some teams create a branch in preparation for a release. 
Half of the team continues to perform new work, and the other half attempts to stabilize the old version. 
In XP, your code shouldn't require stabilization, so it's more useful to create such a branch at the point of release, not in preparation for release.Branches can also be useful for continuous integration and other code management tasks. 
These private branches live for less than a day. 
You don't need private branches to successfully practice XP, but if you're familiar with this approach, feel free to use it.There are plenty of options. 
In the open source realm, Subversion is popular and particularly good when combined with the TortoiseSvn front end. 
Of the proprietary options, Perforce gets good reviews, although I haven't tried it myself.Avoid Visual SourceSafe (VSS). 
VSS is a popular choice for Microsoft teams, but it has numerous flaws and problems with repository corruption—an unacceptable defect in a version control system.Your organization may already provide a recommended version control system. 
If it meets your needs, use it. 
Otherwise, maintaining your own version control system isn't much work and requires little of a server besides disk space.Yes, as much as possible. 
If you install tools and libraries manually, two undesirable things will happen. 
First, whenever you make an update, everyone will have to manually update their computer. 
Second, at some point in the future you'll want to build an earlier version and you'll spend several hours struggling to remember which versions of which tools you need to install.Some teams address these concerns by creating a "tools and libraries" document and putting it in source control, but it's a pain to keep such a document up-to-date. 
Keeping your tools and libraries in source control is a simpler, more effective method.Some tools and libraries require special installation, particularly on Windows, which makes this strategy more difficult. 
They don't all need installation, though—some just come with an installer because it's a cultural expectation. 
See if you can use them without installing them, and try to avoid those that you can't easily use without special configuration.For tools that require installation, I put their install packages in version control, but I don't install them automatically in the build script. 
The same is true for tools that are useful but not necessary for the build, such as IDEs and diff tools.Rather than store the database itself in version control, set up your build to initialize your database schema and migrate between versions. 
Store the scripts to do so in version control.In order for time travel to work, you need to be able to exactly reproduce your build environment for any point in the past. 
In theory, everything required to build should be in version control, including your compiler, language framework, and even database management system (DBMS) and operating system (OS). 
Unfortunately, this isn't always practical. 
I include as much as I can, but I don't usually include my DBMS or operating system.Some teams keep an image of their entire OS and installed software in version control. 
This is an intriguing idea, but I haven't tried it.Slow updates may be a sign of a poor-quality version control system. 
The speed of better systems depends on the number of files that have changed, not the total number of files in the system.One way to make your updates faster is to be selective about what parts of your tools and libraries you include. 
Rather than including the entire distribution—documentation, source code, and all—include only the bare minimum needed to build. 
Many tools only need a handful of files to execute. 
Include distribution package files in case someone needs more details in the future.If you don't intend to change their code and you plan on updating infrequently, you can manually copy their source code into your repository.If you have more sophisticated needs, many version control systems will allow you to integrate with other repositories. 
Your system will automatically fetch their latest changes when you update. 
It will even merge your changes to their source code with their updates. 
Check your version control system's documentation for more details.Be cautious of making local changes to third-party source code; this is essentially a branch, and it incurs the same synchronization challenges and maintenance overhead that any long-lived branch does. 
If you find yourself making modifications beyond vendor-supplied configuration files, consider pushing those changes upstream, back to the vendor, as soon as possible.Certainly. 
You may wish to provide read-only access unless you have well-defined ways of coordinating changes from other teams.With good version control practices, you are easily able to coordinate changes with other members of the team. 
You easily reproduce old versions of your software when you need to. 
Long after your project has finished, your organization can recover your code and rebuild it when they need to.You should always use some form of version control, even on small one-person projects. 
Version control will act as a backup and protect you when you make sweeping changes.Concurrent editing, on the other hand, can be dangerous if an automatic merge fails and goes undetected. 
Be sure you have a decent build if you allow concurrent edits. 
Concurrent editing is also safer and easier if you practice continuous integration and have good tests.There is no practical alternative to version control.You may choose to use file locking rather than concurrent editing. 
Unfortunately, this approach makes refactoring and collective code ownership very difficult, if not impossible. 
You can alleviate this somewhat by keeping a list of proposed refactorings and scheduling them, but the added overhead is likely to discourage people from suggesting significant refactorings.Pragmatic Version Control Using Subversion [Mason] is a good introduction to the nuts and bolts of version control that specifically focuses on Subversion.Source Control HOWTO [Sink], at http://www.ericsink.com/scm/source_control.html, is a helpful introduction to version control for programmers with a Microsoft background.Software Configuration Management Patterns: Effective Teamwork, Practical Integration [Berczuk & Appleton] goes into much more detail about the ways in which to use version control.We eliminate build and configuration hassles.Here's an ideal to strive for. 
Imagine that you've just hired a new programmer. 
On the programmer's first day, you walk him over to the shiny new computer you just added to your open workspace."We've found that keeping everything in version control and having a really great automated build makes us a lot faster," you say. 
"Here, I'll show you. 
This computer is new, so it doesn't have any of our stuff on it yet."You sit down together. 
"Okay, go ahead and check out the source tree."  You walk him through the process and the source tree starts downloading. 
"This will take a while because we have all of our build tools and libraries in version control, too," you say. 
"Don't worry—like any good version control system, it brings down changes, so it's only slow the first time. 
We keep tools and libraries in version control because it allows us to update them easily. 
Come on, let me show you around the office while it downloads."After giving him the tour, you come back. 
"Good, it's finished," you say. 
"Now, watch this—this is my favorite part. 
Go to the root of the source tree and type build."The new programmer complies, then watches as build information flies by. 
"It's not just building the source code," you explain. 
"We have a complex application that requires a web server, multiple web services, and several databases. 
In the past, when we hired a new programmer, he would spend his first couple of weeks just configuring his workstation. 
Test environments were even worse. 
We used to idle the whole team for days while we wrestled with problems in the test environment. 
Even when the environment worked, we all had to share one environment and we couldn't run tests at the same time."All that has changed. 
We've automated all of our setup. 
Anybody can build and run all of the tests on their own machine, any time they want. 
I could even disconnect from the network right now and it would keep building. 
The build script is doing everything: it's configuring a local web server, initializing a local database... everything."Ah!  It's almost done. 
It's built the app and configured the services. 
Now it's running the tests. 
This part used to be really slow, but we've made it much faster lately by improving our unit tests so we could get rid of our end-to-end tests."Suddenly, everything stops. 
The cursor blinks quietly. 
At the bottom of the console is a message: BUILD SUCCESSFUL."That's it," you say proudly. 
"Everything works. 
I have so much confidence in our build and tests that I could take this and install it on our production servers today. 
In fact, we could do that right now, if we wanted to, just by giving our build a different command."You're going to enjoy working here."  You give the new programmer a friendly smile. 
"It used to be hell getting even the simplest things done. 
Now, it's like butter. 
Everything just works."What if you could build and test your entire product—or create a deployment package—at any time, just by pushing a button?  How much easier would that make your life?Producing a build is often a frustrating and lengthy experience. 
This frustration can spill over to the rest of your work. 
"Can we release the software?"  "With a few days of work."  "Does the software work?"  "My piece does, but I can't build everything."  "Is the demo ready?"  "We ran into a problem with the build—tell everyone to come back in an hour."Sadly, build automation is easy to overlook in the rush to finish features. 
If you don't have an automated build, start working on one now. 
It's one of the easiest things you can do to make your life better.There are plenty of useful build tools available, depending on your platform and choice of language. 
If you're using Java, take a look at Ant. 
In .NET, NAnt and MSBuild are popular. 
Make is the old standby for C and C++. 
Perl, Python, and Ruby each have their preferred build tools as well.Your build should be comprehensive but not complex. 
In addition to compiling your source code and running tests, it should configure registry settings, initialize database schemas, set up web servers, launch processes—everything you need to build and test your software from scratch without human intervention. 
Once you get the basics working, add the ability to create a production release, either by creating an install file or actually deploying to servers.A key component of a successful automated build is the local build. 
A local build will allow you to build and test at any time without worrying about other people's activities. 
You'll do this every few minutes in XP, so independence is important. 
It will also make your builds run faster.Be cautious of IDEs and other tools that promise to manage your build automatically. 
Their capability often begins and ends with compiling source code. 
Instead, take control of your build script. 
Take the time to understand exactly how and why it works and when to change it. 
Rather than starting with a pre-made template, you might be better off creating a completely new script. 
You'll learn more, and you'll be able to eliminate the complexity a generic script adds.The details are up to you. 
In my build scripts, I prefer to have all auto-generated content go into a single directory called build/. 
The output of each major step (such as compiling source code, running tests, collating files into a release, or building a release package) goes into a separate directory under build/. 
This structure allows me to inspect the results of the build process and—if anything goes wrong—wipe the slate clean and start over.At the start of the project, in the very first iteration, set up a bare-bones build system. 
The goal of this first iteration is to produce the simplest possible product that exercises your entire system. 
That includes delivering a working—if minimal—product to stakeholders.Because the product is so small and simple at this stage, creating a high-quality automated build is easy. 
Don't try to cover all of the possible build scenarios you need in the future. 
Just make sure that you can build, test, and deploy this one simple product—even if it does little more than "Hello, world!"  At this stage, deployment might be as simple as creating a .zip file.Once you have the seed of your build script, it's easy to improve. 
Every iteration, as you add features that require more out of your build, improve your script. 
Use your build script every time you integrate. 
To make sure it stays up-to-date, never configure the integration machine manually. 
If you find that something needs configuration, modify the build script to configure it for you.If you want to add a build script to an existing system, I have good news and bad news. 
The good news is that creating a comprehensive build script is one of the easiest ways to improve your life. 
The bad news is that you probably have a bunch of technical debt to pay off, so it won't happen overnight.As with any agile plan, the best approach is to work in small stages that provide value as soon as possible. 
If you have a particularly complex system with lots of components, work on one component at a time, starting with the one that's most error-prone or frustrating to build manually.Once you've picked the right component to automate, start by getting it to compile. 
That's usually an easy step, and it allows you to make progress right away.Next, add the ability to run unit tests and make sure they pass. 
You probably compile and run unit tests in your IDE already, so this may not seem like a big improvement. 
Stick with it; making your build script able to prove itself is an important step. 
You won't have to check the results manually any more.Your next step depends on what's causing you the most grief. 
What is the most annoying thing about your current build process?  What configuration issue springs up to waste a few hours every week?  Automate that. 
Repeat with the next-biggest annoyance until you have automated everything. 
Once you've finished this, congratulations!  You've eliminated all of your build annoyances. 
You're ahead of most teams: you have a good build script.Now it's time to make a great build script. 
Take a look at how you deploy. 
Do you create a release package such as an installer, or do you deploy directly to the production servers?  Either way, start automating the biggest annoyances in your deployment process, one at a time. 
As before, repeat with the next-biggest annoyance until you run out of nits to pick.This won't happen overnight, but try to make progress every week. 
If you can solve one annoyance every week, no matter how small, you'll see noticeable improvement within a month. 
As you work on other things, try not to add new debt. 
Include all new work in the build script from the beginning.A great build script puts your team way ahead of most software teams. 
After you get over the rush of being able to build the whole system any time you want, you'll probably notice something new: the build is slow.With continuous integration, you integrate every few hours. 
Each integration involves two builds: one on your machine and one on the integration machine. 
You need to wait for both builds to finish before continuing on because you can never let the build break in XP. 
If the build breaks, you have to roll back your changes.A ten-minute build leads to a twenty-minute integration cycle. 
That's a pretty long delay. 
I prefer a ten or fifteen-minute integration cycle. 
That's about the right amount of time to stretch my legs, get some coffee, and talk over our work with my pairing partner.The easiest way to keep the build under five minutes (with a ten-minute maximum) is to keep the build times down from the beginning. 
One team I worked with started to look for ways to speed up the build whenever it exceeded 100 seconds.Many new XP teams make the mistake of letting their build get too long. 
If you're in that boat, don't worry. 
You can fix long build times in the same agile way you fix all technical debt: piece by piece, focusing on making useful progress at each step.For most teams, their tests are the source of a slow build. 
Usually it's because their tests aren't focused enough. 
Look for common problems: are you writing end-to-end tests when you should be writing unit tests and integration tests?  Do your unit tests talk to a database, network, or file system?You should be able to run about 100 unit tests per second. 
Unit tests should comprise the majority of your tests. 
A fraction (less than ten percent) should be integration tests, which check that two components synchronize properly. 
When the rest of your tests provide good coverage, only a handful—if any—need to be end-to-end tests. 
See Test-Driven Development in Chapter 9 for more information.Although tests are the most common cause of slow builds, if compilation speed becomes a problem, consider optimizing code layout or using a compilation cache or incremental compilation. 
You could also use a distributed compilation system or take the best machine available for use as the build master. 
Don't forget to take advantage of the dependency evaluation features of your build tool: you don't need to rebuild things that haven't changed.In the worst-case scenario, you may need to split your build into a "fast" build that you run frequently and a "slow" build that an integration server runs when you check in (see Continuous Integration later in this chapter). 
Be careful—this approach leads to more build failures than a single, fast build. 
Keep working on making your build faster.All of the programmers are responsible for maintaining the script. 
As the codebase evolves, the build script should evolve with it.At first, one person will probably be more knowledgeable about the script than others. 
When you need to update the script, pair with this person and learn all you can.The build script is the center of your project automation universe. 
The more you know about how to automate your builds, the easier your life will become and the faster you'll be able to get work done.You need to be able to update your scripts continuously to meet your specific needs. 
It's unlikely that anybody can be more responsive to your needs than you are. 
If the CM department is a bottleneck, ask your project manager for help. 
He may be able to give you control over the scripts.Alternatively, you might be able to use a two-stage build in which you run your own scripts privately before handing control over to the CM department.Improving your build directly improves your productivity and quality of life. 
It's important enough to include in every iteration as part of your everyday work. 
The best way to do this is to include enough slack in your iteration for taking care of technical debt such as slow builds. 
If a particular story will require changes to the build script, include that time in your story estimate.Yes, as much as possible. 
See Version Control earlier in this chapter for details.Ten minutes is a good rule of thumb. 
Your build is too long when pairs move on to other tasks before the integration cycle completes.Many IDEs use an underlying build script that you can control. 
If not, you may be better off using a different IDE. 
Your other alternative is to have a separate command line-based build system, such as Ant, NAnt, or make. 
You risk duplicating information about dependencies, but sometimes that cost is worthwhile.If possible, use a cross compiler. 
If that doesn't work, consider using a cross-platform build tool. 
The benefits of testing the build on your development platform outweigh the initial work in creating a portable system.Even if your product relies on yet-to-be-built custom hardware or unavailable third-party systems, you still need to build and test your part of the product. 
If you don't, you'll discover a ton of integration and bug-fixing work when the system becomes available.A common solution for this scenario is to build a simulator for the missing system, which allows you to build integration tests. 
When the missing system becomes available, the integration tests help you determine if the assumptions you built into the simulator were correct.Missing components add risk to your project, so try to get your hands on a test system as soon as possible.At least once per iteration. 
Building from scratch is often much slower than an incremental build, so it depends on how fast the build is and how good your build system is. 
If you don't trust your build system, build from scratch more often. 
You can set up a smoke-testing system that builds the project from scratch on every check-in.My preference is to reduce build times so that incremental builds are unnecessary, or to fix the bugs in the build system so I trust the incremental builds. 
Even so, I prefer to build from scratch before delivering to customers.With a good automated build, you can build a release any time you want. 
When somebody new joins the team, or when you need to wipe a workstation and start fresh, it's a simple matter of downloading the latest code from the repository and running the build.When your build is fast and well-automated, you build and test the whole system more frequently. 
You catch bugs earlier and, as a result, spend less time debugging. 
You integrate your software frequently without relying on complex background build systems, which reduces integration problems.Every project should have a good automated build. 
Even if you have a system that's difficult to build, you can start chipping away at the problem today.Some projects are too large for the ten-minute rule to be effective. 
Before you assume that this is true for your project, take a close look at your build procedures. 
You can often reduce the build time much more than you realize.If the project truly is too large to build in ten minutes, it's probably under development by multiple teams or sub-teams. 
Consider splitting the project into independent pieces that you can build and test separately.If you can't build your system in less than ten minutes (yet), establish a maximum acceptable threshhold and stick to it. 
Drawing this line helps identify a point beyond which you will not allow more technical debt to accumulate. 
Like a sink full of dishes two hours before a dinner party, the time limit is a good impetus to do some cleaning.We keep our code ready to ship.Most software development efforts have a hidden delay between when the team says "we're done" and when the software is actually ready to ship. 
Sometimes that delay can stretch on for months. 
It's the little things: merging everyone's pieces together, creating an installer, prepopulating the database, building the manual, and so forth. 
Meanwhile, the team gets stressed out because they forgot how long these things take. 
They rush, leave out helpful build automation, and introduce more bugs and delays.Continuous integration is a better approach. 
It keeps everybody's code integrated and builds release infrastructure along with the rest of the application. 
The ultimate goal of continuous integration is to be able to deploy all but the last few hours work at any time.Practically speaking, you won't actually release software in the middle of an iteration. 
Stories will be half-done and features will be incomplete. 
The point is to be technologically ready to release even if you're not functionally ready to release.If you've ever experienced a painful multi-day (or multi-week) integration, integrating every few hours probably seems foolish. 
Why go through that hell so often?Actually, short cycles make integration less painful. 
Shorter cycles lead to smaller changes, which means there are fewer chances for your changes to overlap with someone else's.That's not to say that collisions don't happen. 
They do. 
They're just not very frequent because everybody's changes are so small.In order to be ready to deploy all but the last few hours of work, your team needs to do two things:To integrate, update your sandbox with the latest code from the repository, make sure everything builds, then commit your code back to the repository. 
You can integrate any time you have a successful build. 
With test-driven development, that should happen every few minutes. 
I integrate whenever I make a significant change to the code or create something I think the rest of the team will want right away.Each integration should get as close to a real release as possible. 
The goal is to make preparing for a release such an ordinary occurance that, when you actually do ship, it's a nonevent1. 
Some teams that use continuous integration automatically burn an installation CD every time they integrate. 
Others create a disk image or, for network-deployed products, automatically deploy to staging servers.1...except for the release party, of course.When was the last time you spent hours chasing down a bug, only to find that it was a problem with your computer's configuration or in somebody else's code?  Conversely, when was the last time you spent hours blaming your computer's configuration (or somebody else's code) only to find that the problem was in code you just wrote?On typical projects, when we integrate, we don't have confidence in the quality of our code or in the quality of the code in the repository. 
The scope of possible errors is wide; if anything goes wrong, we're not sure where to look.Reducing the scope of possible errors is the key to developing quickly. 
If you have total confidence that your software worked five minutes ago, then only the actions you've taken in the last five minutes could cause it to fail now. 
That reduces the scope of the problem so much that you can often figure it out just by looking at the error message—there's no debugging necessary.To achieve this, agree as a team never to break the build. 
This is easier than it sounds: you can actually guarantee that the build will never break (well, almost never) by following a little script.To guarantee an always-working build, you have to solve two problems. 
First, you need to make sure that what works on your computer will work on anybody's computer. 
(How often have you heard the phrase, "It worked on my machine!"?)  Second, you need to make sure that nobody gets code that hasn't been proven to build successfully.To do this, you need a spare development machine to act as a central integration machine. 
You also need some sort of physical object to act as an integration token. 
(I use a rubber chicken. 
Stuffed toys work well, too.)With an integration machine and integration token, you can ensure a working build in several simple steps.Run a full build to make sure everything compiles and passes tests after you get the code. 
If it doesn't, something went wrong. 
The most common problem is a configuration issue on your machine. 
Try running a build on the integration machine. 
If it works, debug the problem on your machine. 
If it doesn't, find the previous integrators and beat them about the head and shoulders, if only figuratively.If the build fails on the integration machine, you have to fix the problem before you give up the integration token. 
The fastest way to do so is to roll back your changes. 
However, if nobody is waiting for the token, you can just fix the problem on your machine and check in again.Avoid fixing problems manually on the integration machine. 
If the build worked on your machine, you probably forgot to add a file or to add a new configuration to the build script. 
In either case, if you correct the problem manually, the next people to get the code won't be able to build.The most important part of adopting continuous integration is getting people to agree to integrate frequently (every few hours) and never to break the build. 
Agreement is the key to adopting continuous integration because there's no way to force people not to break the build.If you're starting with XP on a brand-new project, continuous integration is easy to do. 
In the first iteration, install a version control system. 
Introduce a ten-minute build with the first story, and grow your release infrastructure along with the rest of your application. 
If you are disciplined about continuing these good habits, you'll have no trouble using continuous integration throughout your project.If you're introducing XP to an existing project, your tests and build may not yet be good enough for continuous integration. 
Start by automating your build (see Ten-Minute Build earlier in this chapter), then add tests. 
Slowly improve your release infrastructure until you can deploy at any time.The most common problem facing teams practicing continuous integration is slow builds. 
Whenever possible, keep your build under ten minutes. 
On new projects, you should be able to keep your build under ten minutes all the time. 
On a legacy project, you may not achieve that goal right away. 
You can still practice continuous integration, but it comes at a cost.When you use the integration script discussed earlier, you're using synchronous integration—you're confirming that the build and tests succeed before moving on to your next task. 
If the build is too slow, synchronous integration becomes untenable. 
(For me, 20 or 30 minutes is too slow.)  In this case, you can use asynchronous integration instead. 
Rather than waiting for the build to complete, start your next task immediately after starting the build, without waiting for the build and tests to succeed.The biggest problem with asynchronous integration is that it tends to result in broken builds. 
If you check in code that doesn't work, you have to interrupt what you're doing when the build breaks half an hour or an hour later. 
If anyone else checked out that code in the meantime, their build won't work either. 
If the pair that broke the build has gone home or to lunch, someone else has to clean up the mess. 
In practice, the desire to keep working on the task at hand often overrides the need to fix the build.If you have a very slow build, asynchronous integration may be your only option. 
If you must use this, a continuous integration server is the best way to do so. 
It will keep track of what to build and automatically notify you when the build has finished.Over time, continue to improve your build script and tests. 
Once the build time gets down to a reasonable number (15 or 20 minutes), switch to synchronous integration. 
Continue improving the speed of the build and tests until synchronous integration feels like a pleasant break rather than a waste of time.Some teams have sophisticated tests, measuring such qualities as performance, load, or stability, that simply cannot finish in under ten minutes. 
For these teams, multistage integration is a good idea.A multistage integration consists of two separate builds. 
The normal ten-minute build, or commit build, contains all the normal items necessary to prove that the software works: unit tests, integration tests, and a handful of end-to-end tests (see Test-Driven Development in Chapter 9 for more about these types of tests). 
This build runs synchronously as usual.In addition to the regular build, a slower secondary build runs asynchronously. 
This build contains the additional tests that do not run in a normal build: performance tests, load tests, and stability tests.Although a multistage build is a good idea for a mature project with sophisticated testing, most teams that I encounter use multistage integration as a workaround for a slow test suite. 
I prefer to improve the test suite instead; it's more valuable to get better feedback more often.If this is the case for you, a multistage integration might help you transition from asynchronous to synchronous integration. 
However, although a multistage build is better than completely asynchronous integration, don't let it stop you from continuing to improve your tests. 
Switch to fully synchronous integration when you can: only synchronous integration guarantees a known-good build.You can integrate at any time, even when the task or story you're working on is only partially done. 
The only requirement is that the code builds and passes its tests.Take a break. 
Get a cup of tea. 
Perform ergonomic stretches. 
Talk with your pair about design, refactoring opportunities, or next steps. 
If your build is under ten minutes, you should have time to clear your head and consider the big picture without feeling like you're wasting time.Although asynchronous integration may seem like a more efficient use of time, in practice it tends to disrupt flow and leads to broken builds. 
If the build fails, you have to interrupt your new task to roll back and fix the old one. 
This means you must leave your new task half-done, switch context (and sometimes partners) to fix the problem, then switch back. 
It's wasteful and annoying.Instead of switching gears in the middle of a task, many teams let the build remain broken for a few hours while they finish the new task. 
If other people integrate during this time, the existing failures hide any new failures in their integration. 
Problems compound and cause a vicious cycle of painful integrations leading to longer broken builds, which lead to more integration problems, which lead to more painful integrations. 
I've seen teams that practice asynchronous integration leave the build broken for days at a time.Remember, too, that the build should run in under ten minutes. 
Given a fast build, the supposed inefficiency of synchronous integration is trivial, especially as you can use that time to reflect on your work and talk about the big picture.You can make asynchronous integration work if you're disciplined about keeping the build running fast, checking in frequently, running the build locally before checking in, and fixing problems as soon as they're discovered. 
In other words, do all the good things you're supposed to do with continuous integration.Synchronous integration makes you confront these issues head on, which is why it's so valuable. 
Asynchronous integration, unfortunately, makes it all too easy to ignore slow and broken builds. 
You don't have to ignore them, of course, but my experience is that teams using asynchronous integration have slow and broken builds much more often than teams using synchronous integration.Ron Jeffries said it best:22Via the art-of-agile mailing list, http://tech.groups.yahoo.com/group/art-of-agile/message/365.The overriding rule of the known-good build is that you must know the build works when you put the integration token back. 
Usually, that means checking in, running the build on the integration machine, and seeing it pass. 
Sometimes—we hope not often—it means rolling back your check-in, running the old build, and seeing that pass instead.If your version control system cannot support this, consider getting one that does. 
Not being able to revert easily to a known-good point in history is a big danger sign. 
You need to be able to revert a broken build with as much speed and as little pain as possible so you can get out of the way of other people waiting to integrate. 
If your version control can't do this for you, create an automated script that will.One way to script this is to check out the older version to a temporary sandbox. 
Delete all of the files in the regular sandbox except for the version control system's metadata files, then copy all of the non-metadata files over from the older version. 
This will allow you to check in the old version on top of the new one.Oops—you've almost certainly exposed some sort of configuration bug. 
It's possible that the bug was in your just-integrated build script, but it's equally possible that there was a latent bug in one of the previous scripts and you accidently exposed it. 
(Lucky you.)Either way, the build has to work before you give up the integration token. 
Now you debug the problem. 
Enlist the help of the rest of the team if you need to; a broken integration machine is a problem that affects everybody.In theory, if the build works on your local machine, it should work on any machine. 
In practice, don't count on it. 
The integration machine is a nice, pristine environment that helps prove the build will work anywhere. 
For example, I occasionally forget to check in a file; watching the build fail on the integration machine when it passed on mine makes my mistake obvious.Nothing's perfect, but building on the integration machine does eliminate the majority of cross-machine build problems.One cause of integration problems is infrequent integration. 
The less often you integrate, the more changes you have to merge. 
Try integrating more often.Another possibility is that your code tends to overlap with someone else's. 
Try talking more about what you're working on and coordinating more closely with the pairs that are working on related code.If you're getting a lot of failures on the integration machine, you probably need to do more local builds before checking in. 
Run a full build (with tests) before you integrate to make sure your code is okay, then another full build (with tests) afterwards to make the integrated code is okay. 
If that build succeeds, you shouldn't have any problems on the integration machine.It's possible that your teammates haven't all bought into the idea of continuous integration. 
I often see teams in which only one or two people have any interest in continuous integration. 
Sometimes they try to force continuous integration on their teammates, usually by installing a continuous integration server without their consent. 
It's no surprise that the team reacts to this sort of behavior by ignoring broken builds. 
In fact, it may actually decrease their motivation to keep the build running clean.Talk to the team about continuous integration before trying to adopt it. 
Discuss the trade-offs as a group, collaboratively, and make a group decision about whether to apply it.If your team has agreed to use continuous integration but is constantly breaking the build anyway, perhaps you're using asynchronous integration. 
Try switching to synchronous integration, and follow the integration script exactly.When you integrate continuously, releases are a painless event. 
Your team experiences fewer integration conflicts and confusing integration bugs. 
The on-site customers see progress in the form of working code as the iteration progesses.Don't try to force continuous integration on a group that hasn't agreed to it. 
This practice takes everyone's willful cooperation.Using continuous integration without a version control system and a ten-minute build is painful.Synchronous integration becomes frustrating if the build is longer than ten minutes and too wasteful if the build is very slow. 
My threshhold is twenty minutes. 
The best solution is to speed up the build.A physical integration token only works if all of the developers sit together. 
You can use a continuous integration server or an electronic integration token instead, but be careful to find a token that's as easy to use and as obvious as a  physical token.Integration tokens don't work at all for very large teams: people spend too much time waiting to integrate. 
Use private branches in your version control system instead: Check your code into a private branch, build the branch on an integration machine—you can have several—then promote the branch to the mainline if the build succeeds.If you can't perform synchronous continuous integration, try using a continuous integration server and asynchronous integration. 
It will lead to more problems than synchronous integration, but is the best of the alternatives.If you don't have an automated build, you won't be able to practice asynchronous integration. 
Delaying integration is a very high risk activity. 
Instead, create an automated build as soon as possible, and start practicing one of the forms of continuous integration.Some teams perform a daily build and smoke test. 
Continuous integration is a more advanced version of the same practice; if you have a daily build and smoke test, you can migrate to continuous integration. 
Start with asynchronous integration and steadily improve your build and tests until you can use synchronous integration.We are all responsible for high quality code.There's a metric for the risk imposed by concentrating knowledge in just a few people's heads—it's called the truck number. 
How many people can get hit by a truck before the project suffers irreparable harm?It's a grim thought, but it addresses a real risk. 
What happens when a critical person goes on holiday, stays home with a sick child, takes a new job, or suddenly retires?  How much time will you spend training a replacement?Collective code ownership spreads responsibility for maintaining the code to all the programmers. 
Collective code ownership is exactly what it sounds like: everyone shares reponsibility for the quality of the code. 
No single person claims ownership over any part of the system, and anyone can make any necessary changes anywhere.In fact, improved code quality may be the most important part of collective code ownership. 
Collective ownership allows—no, expects—everyone to fix problems they find. 
If you encounter duplication, unclear names, or even poorly designed code, it doesn't matter who wrote it. 
It's your code. 
Fix it!Collective code ownership requires letting go of a little bit of ego. 
Rather than taking pride in your code, take pride in your team's code. 
Rather than complaining when someone edits your code, enjoy how the code improves when you're not working on it. 
Rather than pushing your personal design vision, discuss design possibilities with the other programmers and agree on a shared solution.Collective ownership requires a joint commitment from team members to produce good code. 
When you see a problem, fix it. 
When writing new code, don't do a half-hearted job and assume somebody else will fix your mistakes. 
Write the best code you can.On the other hand, collective ownership means that you don't have to be perfect. 
If you've produced code that works, is of reasonable quality, and you're not sure how to make it better, don't hesitate to let it go. 
Someone else will improve it later, if and when it needs it.If you're working on a project that has knowledge silos—in other words, little pockets of code that only one or two people understand—then collective code ownership might seem daunting. 
How can you take ownership of code that you don't understand?To begin, take advantage of pair programming. 
When somebody picks a task involving code you don't understand, volunteer to pair with him. 
When you work on a task, ask the local expert to pair with you. 
Similarly, if you need to work on some unfamiliar code, take advantage of your shared workspace to ask a question or two.Rely on your inference skills as well. 
You don't need to know exactly what's happening in every line of code. 
In a well-designed system, all you need to know is what each package (or namespace) is responsible for. 
Then you can infer high-level class responsibilities and method behaviors from their names. 
(See Refactoring in Chapter 9.)Rely on the unit tests for further documentation and as your safety net. 
If you're not sure how something works, change it anyway and see what the tests say. 
An effective test suite will tell you when your assumptions are wrong.As you work, look for opportunities to refactor the code. 
In particular, I often find that refactoring code helps me to understand it. 
It benefits the next person too; well-factored code tends toward simplicity, clarity, and appropriate levels of abstraction.If you're just getting started with XP, you might not yet have a great set of unit tests and the design might be a little flaky. 
In this case, you may not be able to infer the design, rely on unit tests, or refactor, so pairing with somebody who knows the code well becomes more important. 
Be sure to spend extra time introducing unit tests and refactoring so that the next person can take ownership of the code without extra help.It's not easy to let a great piece of code out of your hands. 
It's difficult sometimes to subsume the desire to take credit for a particularly clever or elegant solution, but it's necessary for your team to take advantage of all of the benefits of collaboration.It's also good for you as a programmer. 
Why?  The whole codebase is yours—not just to modify, but to support and improve. 
You get to expand your skills. 
Even if you're an absolute database guru, you don't have to write only database code through out the project. 
If writing a litte UI code sounds interesting, find a programming partner and have at it.You also don't have to carry the maintenance burden for a piece of code someone assigned you to write. 
Generally, the pair that finds a bug fixes the bug. 
They don't need your permission. 
Even better, they don't necessarily need your help; they may know the code now as well as you did when you wrote it.It's a little scary at first to come into work and not know exactly what you'll work on, but it's also freeing. 
You no longer have long subprojects lingering overnight or over the weekend. 
You get variety and challenge and change. 
Try it—you'll like it.Please do!  Collective code ownership shares knowledge and improves skills, but it won't make everyone an expert at everything.Don't let specialization prevent you from learning other things, though. 
If your specialty is databases and the team is working on user interfaces this week, take on a user interface task. 
It can only improve your skills.People naturally gravitate to one part of the system or another. 
They become experts in particular areas. 
Everybody gains a general understanding of the overall codebase, but each person only knows the details of what he's worked with recently.The tests and simple design allow this approach to work. 
Simple design and its focus on code clarity make it easier to understand unfamiliar code. 
The tests act both as a safety net and as documentation.It does, and so it also requires continuous integration. 
Continuous integration decreases the chances of merge conflicts.In the first week or two of the project, when there isn't much code, conflicts are more likely. 
Treat the code gently for the first couple of iterations. 
Talk together frequently and discuss your plans. 
As you progress, the codebase will grow, so there will be more room to make changes without conflict.Rather than turning your junior programmers loose on the code, make sure they pair with experienced members of the team. 
Keep an eye on their work and talk through design decisions and trade-offs. 
How else will they learn your business domain, learn your codebase, or mature as developers?If you have combined programmers working on several projects into a single team (as described in the discussion of team size in Is XP Right For Us? in Chapter 4), then yes, the whole team should take responsibility for all code. If your programmers have formed multiple separate teams, then they usually should not share ownership across teams.When you practice collective code ownership, you constantly make minor improvements to all parts of the codebase, and you find that the code you've written improves without your help. 
When a team member leaves or takes a vacation, the rest of the team continues to be productive.Don't use collective code ownership as an excuse for no code ownership. 
Managers have a saying: "shared responsibility is no responsibility at all."  Don't let that happen to your code. Collective code ownership doesn't mean someone else is responsible for the code; it means you are responsible for the code—all of it. 
(Fortunately, the rest of the team is there to help you.)Collective code ownership requires good communication. 
Without it, the team cannot maintain a shared vision, and code quality will suffer. 
Several XP practices help provide this communication: a team that includes experienced designers, sitting together, and pair programming.Although they are not strictly necessary, good design and tests make collective code ownership is easier. 
Proceed with caution unless you use test-driven development, simple design, and agree on coding standards. 
To take advantage of collective ownership's ability to improve code quality, the team must practice relentless refactoring.To coordinate changes, you must use continuous integration and a concurrent model of version control.A typical alternative to collective code ownership is strong code ownership, in which each module has a specific owner and only that person may make changes. 
A variant is weak code ownership, in which one person owns a module but others can make changes so long as they coordinate with the owner. 
Neither approach, however, shares knowledge or enables refactoring as well as collective ownership does.If you cannot use collective code ownership, you need to adopt other techniques to spread knowledge and encourage refactoring. 
Pair programming may be your best choice. 
Consider holding weekly design workshops as well to review the overall design and brainstorm improvements.I recommend against strong code ownership. 
It encourages rigid silos of knowledge, making you vulnerable to any team member's absence. 
Weak code ownership is a better choice, although it still doesn't provide the benefits of collective ownership.We communicate necessary information effectively.The word documentation is full of meaning. 
It could mean written instructions for end-users, or detailed specifications, or an explanation of APIs and their use. 
Still, these are forms of communication—that's the commonality.Communication happens all the time in a project. 
Some helps you get your work done; you ask a specific question, get a specific answer, and use that to solve a specific problem. 
This is the purpose of work-in-progress documentation, such as requirements documents and design documents.Other communication provides business value, as with product documentation such as user manuals and API documentation. 
A third type—handoff documentation—supports the long-term viability of the project by ensuring that important information is communicated to future workers.XP has the whole team sit together to promote the first type of communication. 
Close contact with domain experts and the use of ubiquitous language create a powerful oral tradition that transmits information when necessary. 
There's no substitute for face-to-face communication. 
Even a phone call loses important nuances in conversation.XP teams also use test-driven development to create a comprehensive test suite. 
When done well, this captures and communicates details about implementation decisions as unambiguous, executable design specifications that are readable, runnable, and modifiable by other developers. 
Similarly, the team uses customer testing to communicate information about hard-to-understand domain details. 
A ubiquitous language helps to further reveal the intent and purpose of the code.The team does document some things, such as the vision statement and story cards, but these act more as reminders than as formal documentation. 
At any time, the team can and should jot down any notes that help them do their work, such as design sketches on a whiteboard, details on a story card, or hard-to-remember requirements in a wiki or spreadsheet.In other words, XP teams don't need traditional written documentation to do their work. 
The XP practices support work-in-progress communication in other ways—ways that are actually more effective than written documentation.Some projects need to produce specific kinds of documentation to provide business value. 
Examples include user manuals, comprehensive API reference documentation, and reports. 
One team I worked with created code coverage metrics—not because they needed them, but because senior management wanted the report to see if XP would increase the amount of unit testing.Because this documentation carries measurable business value but isn't otherwise necessary for the team to do its work, schedule it in the same way as all customer-valued work: with a story. 
Create, estimate, and prioritize stories for product documentation just as you would any other story.If you're setting the code aside or preparing to hand the project off to another team (perhaps as part of final delivery), create a small set of documents recording big decisions and information. 
Your goal is to summarize the most important information you've learned while creating the software—the kind of information necessary to sustain and maintain the project.Besides an overview of the project and how it evolved in design and features, your summary should include nonobvious information. 
Error conditions are important. 
What can go wrong, when might it occur, and what are the possible remedies?  Are there any traps or sections of the code where the most straightforward approach was inappropriate?  Do certain situations reoccur and need special treatment?This is all information you've discovered through development as you've learned from writing the code. 
In clear written form, this information helps to mitigate the risk of handing the code to a fresh group.As an alternative to handoff documentation, you can gradually migrate ownership from one team to another. 
Exploit pair programming and collective code ownership to move new developers and other personnel onto the project and to move the previous set off in phases. 
Rather than a sharp break (or big thud) as one team's involvement ends and the other begins, the same osmotic communication that helps a team grow can help transition, repopulate, or shrink a team.It could be. 
In order to reduce documentation, you have to replace it with some other form of communication. 
That's what XP does.Increasing the amount of written communication also increases your risk. 
What if that information goes out of date?  How much time does someone need to spend updating that documentation, and could that person spend that time updating the tests or refactoring the code to communicate that information more clearly?The real risk is in decreasing the amount and accuracy of appropriate communication for your project, not in favoring one medium of communication. 
Favoring written communication may decrease your agility, but favoring spoken communication may require more work to disseminate information to the people who need it.When you communicate in the appropriate ways, you spread necessary information effectively. 
You reduce the amount of overhead in communication. 
You mitigate risk by presenting only necessary information.Alistair Cockburn describes a variant of Extreme Programming called "Pretty Adventuresome Programming":11http://c2.com/cgi/wiki?PrettyAdventuresomeProgramming, accessed 24 May 20072Most teams now use one or two-week iterations. 
I recommend one-week iterations for new teams; see Iteration Planning in Chapter 8.3Emphasis in original.4Alistair later added, "I am interested in having available a sarcasm-filled derisively delivered phrase to hit people with who use XP as an excuse for sloppy, slap-dash development. 
I, of all people, think it actually is possible to turn dials to different numbers [Alistair means that you don't have to go as far as XP does to be successful], but I have no patience with people who slap the XP logo on frankly sloppy development."In other words, continue to create documentation until you have practices in place to take its place. 
You have to be rigorous in your practice of XP in order to stop writing work-in-progress documentation. 
Particularly important is a whole team (with all of the team roles filled—see The XP Team in Chapter 3) that sits together.Some organizations value written documentation so highly that you can't eliminate work-in-progress documents. 
In my experience, these organizations usually aren't interested in trying XP. 
If yours is like this, but wants to do XP anyway, talk with management about why those documents are important and whether XP can replace them. 
Perhaps handoff documents are an acceptable compromise. 
If not, don't eliminate work-in-progress documents. 
Either schedule the documents with stories or include the cost of creating and updating documents in your estimates.If you think of documents as communication mechanisms rather than printed paper, you'll see that there are a wide variety of alternatives for documentation. 
Different media have different strengths. 
Face-to-face conversations are very high bandwidth but can't be referenced later, whereas written documents are very low bandwidth (and easily misunderstood) but can be referred to again and again.Alistair Cockburn suggests an intruiging alternative to written documents for handoff documentation: rather than creating a design overview, use a video camera to record a whiteboard conversation between an eloquent team member and a programmer who doesn't understand the system. 
Accompany the video with a table of contents that provides timestamps for each portion of the conversation.The larger your project becomes, the harder it is to plan everything in advance. 
The more chaotic your environment, the more likely it is that your plans will be thrown off by some unexpected event. 
Yet in this chaos lies opportunity.Rather than try to plan for every eventuality, embrace the possibilities that change brings you. 
This attitude is very different from facing change with clenched jaws and white knuckles. 
In this state of mind, we welcome surprise. 
We marvel at all of the power we have to identify and take advantage of new opportunities. 
The open horizon stretches before us. 
We know in which direction we need to travel, and we have the flexibility in our plan to choose the best way to get there. 
We'll know it when we find it.This approach may sound like it's out of control. 
It would be, except for eight practices that allow you to control the chaos of endless possibility.We know why our work is important and how we'll be successful.Vision. 
If there's a more derided word in the corporate vocabulary, I don't know what it is. 
It brings to mind bland corporate-speak. 
"Our vision is to serve customers while maximizing stakeholder value and upholding the family values of our employees."  Bleh. 
Content-free baloney.Don't worry—that's not what you're going to do.Before a project begins, someone in the company has an idea. 
Suppose it's someone in the Wizzle-Frobitz company1. "Hey!" he says, sitting bolt upright in bed. 
"We could frobitz the wizzles so much better if we had some software that sorted the wizzles first!"1Not a real company.Maybe it's not quite that dramatic. 
The point is, projects start out as ideas focused on results. 
Sell more hardware by bundling better software. 
Attract bigger customers by scaling more effectively. 
Open up a new market by offering a new service. 
The idea is so compelling that it gets funding, and the project begins.Somewhere in the transition from idea to project, the compelling part—the vision of a better future—often gets lost. 
Details crowd it out. 
You have to hire programmers, domain experts, and interaction designers. 
You must create stories, schedule iterations, and report on progress. 
Hustle, people, hustle!That's a shame, because nothing matters more than delivering the vision. 
The goal of the entire project to frobitz wizzles better. 
If the details are perfect (the wizzles are sorted with elegance and precision) but the vision is forgotten (the wizzle sorter doesn't work with the frobitzer) the software will probably fail. 
Conversely, if you ship something that helps frobitz wizzles better than anything else could, then does it really matter how you did it?Sometimes the vision for a project strikes as a single, compelling idea. 
One person gets a bright idea, evangelizes it, and gets approval to pursue it. 
This person is a visionary.More often, the vision isn't so clear. 
There are multiple visionaries, each with their own unique idea of what the project should deliver.Either way, the project needs a single vision. 
Someone must unify, communicate, and promote the vision to the team and to stakeholders. 
That someone is the product manager.Like the children's game of telephone, every step between the visionaries and the product manager reduces the product manager's ability to accurately maintain and effectively promote the vision.If you only have one visionary, the best approach is to have that visionary act as product manager. 
This reduces the possibility of any telephone-game confusion. 
As long as the vision is both worthwhile and achievable, the visionary's day-to-day involvement as product manager greatly improves the project's chances of delivering an impressive product.If the visionary isn't available to participate fully, as is often the case, someone else must be the product manager. 
Ask the visionary to recommend a trusted lieutenant or protégé: someone who has regular interaction with the visionary and understands how he thinks.Frequently, a project will have multiple visionaries. 
This is particularly common in custom software development. 
If this is the case on your project, you need to help the visionaries combine their ideas into a single, cohesive vision.Before you go too far down that road, however, ask yourself whether you actually have multiple projects. 
Is each vision significantly different?  Can you execute them serially, one vision at a time, as separate projects built by the same team on a single codebase?  If you can, that may be your best solution.If you can't tease apart the visions (do try!), you're in for a tough job. 
In this case, the role of the product manager must change. 
Rather than being a visionary himself, the product manager facilitates discussion between multiple visionaries. 
The best person for the job is one who understands the business well, already knows each of the visionaries, and has political savvy and good facilitation skills.It might be more accurate to call this sort of product manager a product facilitator or customer coach, but I'll stick with product manager for consistency.After you've worked with visionaries to create a cohesive vision, document it in a vision statement. 
It's best to do this collaboratively, as doing so will reveal areas of disagreement and confusion. 
Without a vision statement, it's all too easy to gloss over disagreements and end up with an unsatisfactory product.Once created, the vision statement will help you maintain and promote the vision. 
It will act as a vehicle for discussions about the vision and a touchpoint to remind stakeholders why the project is valuable.Don't forget that the vision statement should be a living document: the product manager should review it on a regular basis and make improvements. 
However, as a fundamental statement of the project's purpose, it may not change much.The vision statement documents three things: what the project should accomplish, why it is valuable, and the project's success criteria.The vision statement can be short. 
I limit mine to a single page. 
Remember, the vision statement is a clear and simple way of describing why the project deserves to exist. 
It's not a roadmap; that's the purpose of release planning.In the first section—what the project should accomplish—describe the problem or opportunity that the project will address, expressed as an end result. 
Be specific, but not prescriptive. 
Leave room for the team to work out the details.Here is a real vision statement describing "Sasquatch," a product developed by two entrepreneurs who started a new company.In the second section, describe why the project is valuable.In the final section, describe the project's success criteria: how you will know that the project has succeeded and when you will decide. 
Use concrete, clear, and unambiguous targets.After creating the vision statement, post it prominently as part of the team's informative workspace. 
Use the vision to evangelize the project with stakeholders and to explain the priority (or deprioritization) of specific stories.Be sure to include the visionaries in product decisions. 
Invite them to release planning sessions. 
Make sure they see iteration demos, even if that means a private showing. 
Involve them in discussions with real customers. 
Solicit their feedback about progress, ask for their help in improving the plan, and give them opportunities to write stories. 
They can even be an invaluable resource in company politics, as successful visionaries are often senior and influential.Including your visionaries may be difficult, but make the effort; distance between the team and its visionaries decreases the team's understanding of the product it's building. 
While the vision statement is necessary and valuable, a visionary's personal passion and excitement for the product communicates far more clearly. 
If the team interacts with the visionary frequently, they'll understand the product's purpose better and they'll come up with more ideas for increasing value and decreasing cost.If the visionaries cannot meet with the team at all, then the product manager will have to go to them to share the plan, get feedback, and conduct private demos. 
This is the least effective way of involving the visionaries, and you must decide if the product manager understands the vision well enough to act in their stead. 
Ask your mentor for help making this decision. 
If you conclude that the product manager doesn't understand the vision well, talk with your executive sponsor about the risks of continuing, and consider that your team may be better off doing something else until the visionaries are available.Even if there are big disagreements about the vision, you should still pursue a unified vision. 
Otherwise, the final product will be just as fragmented and unsatisfactory as the vision is. 
You may benefit from engaging the services of a professional facilitator to help mediate the discussions.Certainly. 
Be sure you cover what, why, and success criteria. 
Find a way to fit those topics into the template. 
Keep your vision statement to a single page if you can.Rapidly shifting goals tend to be common with entrepreneurial visionaries. 
It isn't due to lack of vision or consistency; instead, your visionary sees a variety of opportunities and changes direction to match.If the vision is constantly changing, this may be a sign that what you think of as the vision is just a temporary strategy in a larger, overarching vision. 
Take your concerns to the visionary and stakeholders and try to identify that larger vision.If you succeed in discovering the larger vision, adaptive release planning (see "Adapt Your Plans" later in this chapter) can help you keep up with your visionary. 
Adaptive planning's emphasis on learning and on taking advantage of opportunities will fit in perfectly with your visionary's entrepreneurial spirit.Your visionary may continue to shift direction more quickly than you can implement her ideas. 
Wild and unpredictable shifts make it difficult to develop software effectively. 
The planning game helps; stick with the normal mechanism of scheduling and implementing stories. 
Your product manager should act as a buffer in this case, protecting the team from rapid shifts and explaining to your visionary what the team can reasonably accomplish.Of course!  This works particularly well with release planning—it can be a great way to help customers choose the priority of stories as they plan their next release.I'm less fond of visions for iteration planning, just because iterations are so short and simple that the extra effort usually isn't worth it.When your project has a clear, compelling vision, prioritizing stories is easy. 
You can easily see which stories to keep and which to leave out. 
Programmers contribute to planning discussions by suggesting ways to maximize value while minimizing development cost. 
Your release plan incorporates small releases that deliver value.When the visionary promotes the vision well, everyone understands why the project is important to the company. 
Team members experience higher morale, stakeholders trust the team more, and the organization supports the project.Always pursue a unified vision for your projects, even at the risk of discovering that there isn't one. 
If the project isn't worth doing, it's better to cancel the project now than after spending a lot of money.Some project communities are so small and tightly knit that everyone knows the vision. 
However, even these teams can benefit from creating a one-page vision statement.Some of the ideas in this section were inspired by the "Mastering Projects" workshop presented by True North pgs, Inc. 
If you have the opportunity to attend their workshop, take advantage of it.Imagine you've been freed from the shackles of deadlines. 
"Maximize our return on investment," your boss says. 
"We've already talked about the vision for this project. 
I'm counting on you to work out the details. 
Create your own plans and set your own release dates—just make sure we get a good return on our investment."Now what?First, work on only one project at a time. 
Many teams work on several projects simultaneously, which is a mistake. 
Task switching has a substantial cost: "[T]he minimum penalty is 15 percent... Fragmented knowledge workers may look busy, but a lot of their busyness is just thrashing." [DeMarco 2002]. 
Working on one project at a time allows you to release each project as you complete it, increasing the total value of your work.Consider a team that has two projects. 
In this simplified example, each project has equal value: when complete, each project will yield $$ in value every month. 
Each project takes three months to complete.In Scenario A (see Figure), the team works on both projects simultaneously. 
To avoid task-switching penalties, they switch between projects every month. 
They finish Project 1 after five months and Project 2 after six. 
At the end of the seventh month, the team has earned $$$$$$.In Scenario B, the team works on just one project at a time. 
They release Project 1 at the end of the third month. 
It starts making money while they work on Project 2, which they complete after the sixth month, as before. 
Although the team's productivity didn't change—the projects still took six months—they earned more money from Project 1. 
By the end of the seventh month, they earned $$$$$$$$$$. 
That's nearly twice as much value with no additional effort.Something this easy ought to be criminal. 
What's really astounding is the number of teams that work on simultaneous projects anyway.Releasing early is an even better idea when you're working on a single project. 
If you group your most valuable features together and release them first, you can achieve startling improvements in value.Consider another example team. 
This team has just one project. 
In Scenario A (see Figure), they build and release it after six months. 
The project is worth $$$$$ per month, so at the end of the seventh month, they've earned $$$$$.In Scenario B, the team groups the most valuable features together, works on them first, and releases them after three months. 
The first release starts making $$$ per month. 
They then work on the remaining features and release them at the end of the sixth month. 
As before, their productivity hasn't changed. 
All that's changed is their release plan. 
Yet due to the income from the first release, the team has made $$$$$$$$$$$$$$ by the end of the end of the seventh month—nearly triple that of Scenario A with its single release.These scenarios are necessarily simplified. 
Software by Numbers [Denne & Cleland-Huang] has a more sophisticated example that uses real numbers and calculates value over the entire life of the product (see Table). 
In their example, the authors convert a five-year project with two end-of-project releases (Scenario A) into five yearly releases ordered by value (Scenario B). 
As before, the team's productivity remains the same.Scenario A is a marginal investment somewhat equivalent to obtaining a 12.8% interest rate. 
It requires an investment of $2.76 million and yields profits of $1.288 million. 
Considering the risk of software development, the investors can put that money to better use elsewhere. 
The project should not be funded.Scenario B—the same project released more often—is an excellent investment somewhat equivalent to obtaining a 36.3% interest rate. 
Although Scenario B costs more because it conducts more releases, those releases allow the project to be self-funding. 
As a result, it requires a smaller investment of $1.64 million and yields profits of $3.088 million. 
This project is well worth funding.Look at these results again. 
Each of these examples shows dramatic increases in value. 
Yet nothing changed except the order in which the teams released their features!Releasing frequently doesn't mean setting aggressive deadlines. 
In fact, aggressive deadlines extend schedules rather than reducing them [McConnell 1996] (p.220). 
Instead, release more often by including less in each release. 
Minimum marketable features [Denne & Cleland-Huang 2004] are an excellent tool for doing so.A minimum marketable feature, or MMF, is the smallest set of functionality that provides value to your market, whether that market is internal users (as with custom software) or external customers (as with commercial software). 
MMFs may provide value in many ways, such as competitive differentiation, revenue generation, and cost savings.As you create your release plan, think in terms of stakeholder value. 
Sometimes it's helpful to think of stories and how they make up a single MMF. 
Other times, you may think of MMFs that you can later decompose into specific stories. 
As you brainstorm, don't forget the minimum part of minimum marketable feature—try to make each feature as small as possible.Once you have minimal features, group them into possible releases. 
This is a brainstorming exercise, not your final plan, so try a variety of groupings. 
Think of ways to minimize the number of features needed in each release.The most difficult part of this exercise is figuring out how to make small releases. 
It's one thing for a feature to be marketable, and another for a whole release to be marketable. 
This is particularly difficult when you're launching a new product. 
To succeed, focus on what sets your product apart, not the features it needs to match the competition.Imagine you're the product manager for a team that's creating a new word processor. 
The market for word processors is quite mature, so it might seem impossible to create a small first release. 
There's so much to do just to match the competition, let alone to provide something new and compelling. 
You need basic formatting, spellchecking, grammar checking, tables, images, printing... the list goes on forever.Approaching a word processor in this way is daunting to the point where it may seem like a worthless effort. 
Rather than trying to match the competition, focus on the features that make your word processor unique. 
Release those features first—they are probably the features that have the most value.Suppose that the competitive differentiation for your word processor is its powerful collaboration capabilities and web-based hosting. 
The first release might have four features:  basic formatting, printing, web-based hosting, and collaboration. 
You could post this first release as a technical preview to start generating buzz. 
Later releases could improve on the base features and justify charging a fee: tables, images, and lists in one release, spellchecking and grammar checking in another, and so on.Lest this seem foolish, consider Writely, the online word processing application. 
It doesn't have the breadth of features that Microsoft Word does, and it probably won't for many years. 
Instead, it focuses on what sets it apart: collaboration, remote document editing, secure online storage, and ease of use.11http://www.writely.com/.According to venture capitalist Peter Rip, the developers released the first alpha of Writely two weeks after they decided to create it.2  How much is releasing early worth?  Ask Google. 
Ten months later, they bought Writely,3 even though Writely still didn't come close to Microsoft Word's feature set.4  Writely is now known as Google Docs.2"Writely is the seed of a Big idea", http://earlystagevc.typepad.com/earlystagevc/2005/09/writely_is_the_.html.3"Writely - The Back Story", http://earlystagevc.typepad.com/earlystagevc/2006/03/same_steve_and_j.html.4"Only in a bubble is Google's web WP an Office-killer", http://www.theregister.co.uk/2006/03/10/google_writely_analysis/.If such significant results are possible from frequent releases, imagine what you could accomplish if you could also increase the value of each release. 
This is actually pretty easy: after each release, collect stakeholder feedback, cancel work on features that turned out to be unimportant, and put more effort into those features that stakeholders find most valuable.With XP, you can change your plans more often than once per release. 
XP allows you to adjust your plan every iteration. 
Why?  To react to unexpected challenges quickly. 
More importantly, it allows you to take advantage of opportunities. 
Where do these opportunities come from?  You create them.The beginning of every software project is when you know the least about what will make the software valuable. 
You might know a lot about its value, but you will always know more after you talk with stakeholders, show them demos, and conduct actual releases.As you continue, you will discover that some of your initial opinions about value were incorrect. 
No plan is perfect, but if you change your plan to reflect what you've learned—if you adapt—you create more value.To increase the value of your software, create opportunities to learn. 
Think of your plan as a plan for learning as much as it is a plan for implementation. 
Focus on what you don't know. 
What are you uncertain about?  What might be a good idea?  Which good ideas can you prove in practice?  Don't just speculate—create experiments. 
Include a way of testing each uncertainty.For example, if you were creating a collaborative online word processor, you might not be sure how extensive your support for importing Microsoft Word documents should be. 
Some sort of support is necessary, but how much?  Supporting all possible Word documents would take a long time to implement and prevent you from adding other, possibly more valuable features. 
Too little support could damage your credibility and cause you to lose customers.To test this uncertainty, you could add a rudimentary import feature to your software (clearly marked "experimental"), release it, and have it create a report on the capabilities needed to support the types of documents that real users try to import. 
The information you gather will help you adapt your plan and increase your product's value.To take the most advantage of the opportunities you create, build a plan that allows you to release at any time. 
Don't get me wrong—the point is not to actually release all the time, but to enable you to release at any time.Why do this?  It allows you to keep your options open. 
If an important but completely new opportunity comes along, you can release what you have and immediately change directions to take advantage of the opportunity. 
Similarly, if there's some sort of disaster, such as the project's surprise cancellation, you can release what you have anyway. 
At any time, you should be able to release a product that has value proportional to the investment you've made.To release at any time, build your plan so that each story stands alone. 
Subsequent stories can build on previous stories, but each one should be releasable on its own. 
For example, one item in your plan might be "Provide login screen," and the next might be "Allow login screen to have client-specific branding."  The second item enhances the first, but the first is releasable on its own.Suppose you're creating a system that gets data from a user, validates the data, and writes it to a database. 
You might initially create a story for each step: "Get data," "Validate data," and "Write data to database."  These are sometimes called horizontal stripes. 
This is an easy way to create stories, but it prevents you from releasing, or even effectively reviewing, the software until you finish all three stories. 
It gives you less flexibility in planning, too, because the three stories form an all-or-nothing clump in your schedule.A better approach is to create stories that do all three tasks, but provide narrower individual utility. 
For example, you might create the stories "Process customer data," "Process shipping address," and "Process billing information."  These are vertical stripes (see Figure).Don't worry too much if you have trouble making your stories perfectly releasable. 
It takes practice. 
Releasable stories give you more flexibility in planning, but a few story clumps in your plan won't hurt much. 
With experience, you'll learn to make your plans less lumpy.There are two basic types of plans: scopeboxed plans and timeboxed plans. 
A scopeboxed plan defines the features the team will build in advance, but the release date is uncertain. 
A timeboxed plan defines the release date in advance, but the specific features that release will include are uncertain.Timeboxed plans are almost always better. 
They constrain the amount of work you can do and force people to make difficult but important prioritization decisions. 
This requires the team to identify cheaper, more valuable alternatives to some requests. 
Without a timebox, your plan will include more low-value features.To create your plan, first choose your release dates. 
I like to schedule releases at regular intervals, such as once per month and no more than three months apart.Now flesh out your plan by using your project vision to guide you in brainstorming minimum marketable features. 
Decompose these into specific stories and work with the programmers to get estimates. 
Using the estimates as a guide, prioritize the stories so that the highest-value, lowest-cost stories are done first. 
(For more details, see The Planning Game later in this chapter).The end result will be a single list of prioritized stories. 
Using your velocity, risk factors, and story estimates, you can predict how many stories each release will include (see Risk Management later in this chapter). 
With that information as a guide, discuss options for reducing costs and splitting stories so that each release provides a lot of value.This final list of stories is your release plan. 
Post it prominently (I use a magnetic whiteboard—see Figure) and refer to it during iteration planning. 
Every week, consider what you've learned from stakeholders and discuss how you can use that information to improve your plan.It takes a lot of time and effort to brainstorm stories, estimate them, and prioritize them. 
If you're adapting your plan as you go, some of that effort will be wasted. 
To reduce waste, plan at the latest responsible moment. 
The latest responsible moment is the last moment at which you can responsibly make a decision (see XP Concepts in Chapter 3). 
In practice, this means that the further away a particular event is, the less detail your release plan needs to contain.Another way to look at this is to think in terms of planning horizons. 
Your planning horizon determines how far you look into the future. 
Many projects try to determine every requirement for the project up front, thus using a planning horizon that extends to the end of the project.To plan at the latest responsible moment, use a tiered set of planning horizons instead. 
Use long planning horizons for general plans and short planning horizons for specific, detailed plans, as shown in Figure.Your planning horizons depend on your situation and comfort level. 
The more commitments you need to make to stakeholders, the longer your detailed planning horizons should be. 
The more uncertain your situation is, or the more likely you are to learn new things that will change your plan, the shorter your planning horizons should be. 
If you aren't sure which planning horizons to use, ask your mentor for guidance. 
Here are some good starting points:Does the idea of spending two months travelling in a foreign country without advance hotel reservations seem scary?  In practice, it was easy and relaxing, but when I tell the story of our adaptively-planned trip to Europe (see the "Adaptive Planning in Action" sidebar), audiences get nervous.Organizations often have a similar reaction to adaptive planning. 
An adaptive plan works to achieve a vision. 
However, just as my wife and I achieved our vision—"have fun visiting a lot of European cities"—but didn't know exactly which cities we would visit, an adaptive team will achieve its vision even though it cannot say exactly what it will deliver.No aspect of agile development challenges organizational culture more than the transition to adaptive planning. 
It requires changes not only to the development team, but to reporting, evaluation, and executive oversight as well. 
The choice of adaptive planning extends to surprisingly diverse parts of the project community, and people often have a shocked or emotional reaction to the idea.As a result, you may not be able to influence a change to adaptive planning. 
Unless you have executive support, any change that does occur will probably be slow and gradual. 
Even with executive support, this change is difficult.You can work within your organization's culture to do adaptive planning under the radar. 
Use adaptive planning, but set your planning horizons to match the organization's expectations. 
Generally, estimating and prioritizing stories for the remainder of the current release is enough. 
This works best if you have small, frequent releases.As your stakeholders and executives gain trust in your ability to deliver, you may be able to shorten your detailed planning horizons and migrate further towards an adaptive plan.You may be confusing iterations with releases. 
Although the team should release software to internal stakeholders every week as part of the iteration demo, you may not choose to release to end-users or real customers that often.Weekly releases are a great choice if you have the opportunity. 
Your ability to do so will depend on your business needs.Although you may not plan out all of the details of your project in advance, you should have plenty of detail to share with stakeholders. 
You should always know the overall vision for the project. 
Depending on your planning horizons, you will probably have a list of the features for the next release as well as a planned date for that release. 
You will also have specific, estimated stories for near-term work.If your stakeholders need more information or predictability, you may need longer planning horizons. 
In any event, be sure to let stakeholders know that this is your current plan and that it is subject to change if you find better ways of meeting the project vision.Any development effort requires that the organization trust that the team will do its job. 
If stakeholders require a detailed plan in order to trust you, use longer planning horizons that allow you to provide the plan they desire.If you're not sure you can deliver on the project vision, focus your plan on discovering whether you can. 
You may need to extend your planning horizons or create a small, limited-availability release to test crucial concepts. 
The details depend on your situation, so if you're not sure what to do, ask your mentor for guidance.No matter your decision, clearly convey your concern to stakeholders and let them know how you intend to address the uncertainty.When you create, maintain, and communicate a good release plan, the team and stakeholders all know where the project is heading. 
The plan shows how the team will meet the project vision, and team members are confident that the plan is achievable. 
You complete features and release high-quality software regularly and consistently.If you are adapting your plan well, you consistently seek out opportunities to learn new things about your plan, your product, and your stakeholders. 
As you learn, you modify your plan to take advantage of new insights. 
Stakeholders and the team agree that each release is better than originally planned.Not all of these ideas are appropriate for everyone. 
I've put the easiest ones first, but even the easy ones have limitations.Working on one project at a time is an easy, smart way to increase your return on investment. 
Despite its usefulness, working on one project at a time is anathema to some organizations. 
Proceed with caution.Releasing frequently requires that your customers and users be able to accept more frequent releases. 
This is a no-brainer for most web-based software because users don't have to do anything to get updates. 
Other software might require painful software rollouts, and some even require substantial testing and certification. 
That makes frequent releases more difficult.Adaptive planning requires that your organization define project success in terms of value rather than "delivered on time, on budget, and as specified."  This can be a tough idea for some organizations to swallow. 
You may be able to assuage fears about adaptive plans by committing to a specific release date but leaving the details of the release unspecified.Keeping your options open—that is, being ready to release, and thus change directions,  at any time—requires a sophisticated development approach. 
Practices such as test-driven development, continuous integration, and incremental design and architecture help.Tiered planning horizons require a cohesive vision and regular updates to the plan. 
Use them when you can reliably revisit the plan at least once per iteration. 
Be sure your team includes a product manager and on-site customers who are responsible for maintaining the plan.Finally, be cautious of plans without a predefined release date or a release date more than three months in the future. 
Without the checkpoint and urgency of a near-term release, these plans risk wandering off course.The classic alternative to adaptive release planning is predictive release planning, in which the entire plan is created in advance. 
This can work in stable environments, but tends to have trouble reacting to changes.If you don't use incremental design and architecture, [Denne & Cleland-Huang] provide a sophisticated Incremental Funding Methodology that shows how to prioritize technical infrastructure alongside features. 
However, XP's use of incremental design neatly sidesteps this need.Finally, teams with an established product and a relatively small need for changes and enhancements don't always need a release plan. 
Rather than thinking in terms of features or releases, these teams work from a small story backlog and release small enhancements every iterations. 
In some cases, they conduct daily deployment. 
You could think of this as an adaptive plan with a very short planning horizon.Software by Numbers [Denne & Cleland-Huang] provides a compelling and detailed case for conducting frequent releases.Agile Development Ecosystems [Highsmith] has an excellent discussion of adaptive planning in chapter 15.Lean Software Development [Poppendieck & Poppendieck] discusses postponing decisions and keeping your options open in chapter 3.Our plans take advantage of both business and technology expertise.You may know when and what to release, but how do you actually construct your release plan?  That's where the planning game comes in.In economics, a game is something in which "players select actions and the payoffs depend on the actions of all players."1  The study of these games "deals with strategies for maximizing gains and minimizing losses... [and are] widely applied in the solution of various decision making problems."21Deardorff's Glossary of International Economics, http://www-personal.umich.edu/~alandear/glossary/g.html2Dictionary definition of "game theory", http://dictionary.reference.com/search?q=game theory&x=0&y=0That describes the planning game perfectly. 
It's a structured approach to creating the best possible plan given the information available.The planning game is most notable for the way it maximizes the amount of information contributed to the plan. 
As a result, it is a strikingly effective way to plan. 
Althought it has limitations, if you work within them, I know of no better way to plan.XP assumes that customers have the most information about value: what best serves the organization. 
Programmers have the most information about costs: what it will take to implement and maintain those features. 
To be successful, the team needs to maximize value while minimizing costs. 
A successful plan needs to take into account information from both groups, as every decision to do something is also a decision not to do something else.Accordingly, the planning game requires the participation of both customers and programmers. 
(Testers may assist, but they do not have an explicit role in the planning game.)  It's a cooperative game; the team as a whole wins or loses, not individual players.Because programmers have the most information about costs—they're most qualified to say how long it will take to implement a story—they estimate.Because customers have the most information about value—they're most qualified to say what is important—they prioritize.Neither group creates the plan unilaterally. 
Instead, both groups come together, each with their areas of expertise, and play the planning game:During the planning game, programmers and customers may ask each other questions about estimates and priorities, but each group has final say over its area of expertise.The result of the planning game is a plan: a list of stories in priority order. 
Even if two stories are of equivalent priority, one must come before the other. 
If you're not sure which to put first, pick one at random.Release planning is always a difficult process because there are many more stories to do than there is time available to do them. 
Also, each stakeholder has his own priorities, and balancing these desires is challenging. 
Tempers rise and the discussion gets heated—or worse, some people sit back and tune out, only to complain later. 
This struggle is natural and happens on any project, XP or not, that tries to prioritize conflicting needs.My favorite way to plan is to gather the team, along with important stakeholders, around a large conference table. 
Customers write stories on index cards, programmers estimate them, and customers place them on the table in priority order. 
One end of the table represents the stories to do first and the other end represents stories to do last. 
The plan tends to grow from the ends towards the middle, with the most difficult decisions revolving around stories that are neither critical nor useless.Using index cards and spreading them out on a table allows participants to point to stories and move them around. 
It reduces infighting by demonstrating the amount of work to be done in a visible way. 
The conversation focuses on the cards and their relative priorities rather than vague discussions of principles or on "must have / not important" distinctions.When customers and programmers work directly together throughout this process, something amazing happens. I call it the miracle of collaboration. 
It really is a miracle because time appears out of nowhere.Like all miracles, it's not easy to achieve. 
When programmers give an estimate, customers often ask a question that causes every programmer's teeth to grind:  "Why does it cost so much?"The instinctive reaction to this question is defensive. "It costs so much because software development is hard, damn it! Why are you questioning me!?"Programmers, there's a better way to react. 
Reword the customer's question in your head into a simple request for information: "Why is this expensive?"  Answer by talking about what's easy and what's difficult.For example, imagine that a product manager requests a toaster to automatically pop up the toast when it finishes. 
The programmers reply that the feature is very expensive, and when the product manager asks why, the programmers calmly answer, "Well, popping up the toast is easy; that's just a spring. But detecting when the toast is done—that's new. We'll need an optical sensor and some custom brownness-detecting software."This gives the product manager an opportunity to ask, "What about all those other toasters out there? How do they know when the toast is done?"The programmers respond: "They use a timer, but that doesn't really detect when the toast is done. It's just a kludge."Now the product manager can reply, "That's okay! Our customers don't want a super toaster. They just want a regular toaster. Use a timer just like everyone else.""Oh, okay. Well, that won't be expensive at all."When you have honest and open dialog between customers and programmers, the miracle of collaboration occurs and extra time appears out of nowhere. Without communication, customers tend not to know what's easy and what's not, and they end up planning stories that are difficult to implement. 
Similarly, programmers tend not to know what customers think is important, and they end up implementing stories that aren't very valuable.With collaboration, the conflicting tendencies can be reconciled. 
For example, a customer could ask for something unimportant but difficult, and the programmers could point out the expense and offer easier alternatives. 
The product manager could then change direction and save time. 
Time appears out of nowhere. It's the miracle of collaboration.In my experience, programmers are highly educated professionals with high motivation to meet customer expectations. 
[McConnell 1996] (pp. 255-256) validates this experience: "Software developers like to work. 
The best way to motivate developers is to provide an environment that makes it easy for them to focus on what they like doing most, which is developing software... [Developers] have high achievement motivation: they will work to the objectives you specify, but you have to tell them what those objectives are."Although programmer estimates may be higher than you like, it's most likely because they want to set realistic expectations. 
If the estimates do turn out to be too high, the team will achieve a higher velocity and automatically do more each iteration to compensate.Customers want to ship a solid, usable product. 
They have to balance that desire with the desire to meet crucial market windows. 
As a result, they may sometimes ask for options that compromise important technical capabilities. 
They do so because they aren't aware of the nuances of technical trade-offs in the same way that programmers are.Programmers, you are most qualified to make decisions on technical issues, just as the customers are most qualified to make decisions on business issues. 
When the customers ask for an explanation of an estimate, don't describe the technical options. 
Instead, interpret the technology and describe the options in terms of business impact.That is, rather than describing the options like:... try instead:If a technical option simply isn't appropriate, don't mention it, or mention your decision in passing as part of the cost of doing business:Be firm. 
Yes, everything is important, but something has to come first and something will come last. 
Someone has to make the tough schedule decisions. 
That's the product manager's job.When you play the planning game well, both customers and programmers feel that they have contributed to the plan. 
Any feelings of pressure and stress focus on the constraints of the plan and possible options, rather than on individuals and groups. 
Programmers suggest technical options for reducing scope while maintaining the project vision. 
Customers ruthlessly prioritize the stories that best serve the vision.The planning game is a simple, effective approach that relies on many of XP's simplifying assumptions, such as:If these conditions are not true on your team, you may not be able to take advantage of the planning game.The planning game also relies on the programmers' abilities to implement design and architecture incrementally. 
Without this capability, the team will find itself creating technical stories or strange story dependencies that make planning more difficult.Finally, the planning game assumes that the team has a single dominant constraint. 
(For more information about the Theory of Constraints, see XP Concepts in Chapter 3.)  It's very rare for a system to exhibit two constraints simultaneously, so this shouldn't be a problem. 
Similarly, the planning game assumes that the programmers are the constraint. 
If this isn't true for your team, discuss your options with your mentor.There are a wide variety of project planning approaches. 
The most popular seems to be Gantt charts that assume task-based plans and schedule what each individual person will do.In contrast to that approach, the planning game focuses on what the team produces, not on what individuals do. 
The team has the discretion to figure out how to produce each story and organizes itself to finish the work on time.This focus on results, rather than tasks, combined with the planning game's ability to balance customer and programmer expertise makes it the most effective approach to software planning I've experienced. 
However, if you wish to use another approach to planning, you can do so. 
Talk with your mentor about how to make your preferred approach to planning work with the rest of the XP practices.This section will go online later this year. In the meantime, why not buy the book?We stop at predetermined, unchangeable time intervals and compare reality to plan.Iterations are the heartbeat of an XP project. 
When an iteration starts, stories flow in to the team as they select the most valuable stories from the release plan. 
Over the course of the iteration, the team breathes those stories to life. 
By the end of the iteration, they've pumped out working, tested software for each story and are ready to begin the cycle again.Iterations are an important safety mechanism. 
Every week, the team stops, looks at what it's accomplished, and shares those accomplishments with stakeholders. 
By doing so, the team coordinates its activities and communicates its progress to the rest of the organization. 
Most importantly, iterations counter a common risk in software projects: the tendency for work to take longer than expected.Programming schedules die in inches. 
At first you're on schedule:  "I'll be done once I finish this test."  Then you're limping. "I'll be done as soon as I fix this bug."  Then gasping. 
"I'll be done as soon as I research this API flaw... no, really."  Before you know it, two days have gone by and your task has taken twice as long as you estimated.Death by inches sneaks up on a team. 
Each delay is only a few hours, so it doesn't feel like a delay, but they multiply across the thousands of tasks in a project. 
The cumulative effects are devastating.Iterations allow you to avoid this surprise. 
Iterations are exactly one week long and have a strictly defined completion time. 
This is a timebox: work ends at a particular time regardless of how much you've finished. 
Although the iteration timebox doesn't prevent problems, it reveals them, which gives you the opportunity to correct the situation.In XP, the iteration demo marks the end of the iteration. 
Schedule the demo at the same time every week. 
Most teams schedule the demo first thing in the morning, which gives them a bit of extra slack the evening before for dealing with minor problems.Iterations follow a consistent, unchanging schedule:Many teams start their iterations on Monday morning and end Friday evening, but I prefer iterations that start on Wednesday morning. 
This allows people to leave early on Friday or take Monday off without missing important events. 
It also allows the team to conduct releases before the weekend.After the iteration demo and retrospective are complete, iteration planning begins. 
Start by measuring the velocity of the previous iteration. 
Take all of the stories that are "done done" and add up their original estimates. 
This number is the amount of story points you can reasonably expect to complete in the upcoming iteration.With your velocity in hand, you can select the stories to work on this iteration. 
Ask your customers to select the most important stories from the release plan. 
Select stories that exactly add up to the team's velocity. 
You may need to split stories (see Stories later in this chapter) or include one or two less important stories to make the estimates add up perfectly.Because the iteration planning meeting takes stories from the front of the release plan, you should have already estimated and prioritized those stories. 
As a result, selecting stories for the iteration plan should only take a few minutes, with perhaps ten or 15 minutes more to explain:After you have chosen the stories for the iteration, everybody but the programmers can leave the meeting, although anybody is welcome to stay if she likes. 
At least one customer should stick around to answer programmer questions and to keep an ear out for misunderstandings.At this point, the real work of iteration planning begins. 
Start by breaking the stories down into engineering tasks.Engineering tasks are concrete tasks for the programmers to complete. 
Unlike stories, engineering tasks don't need to be customer-centric. 
Instead, they're programmer-centric. 
Typical engineering tasks include:Brainstorm the tasks that you need in order to finish all the iteration's stories. 
Some tasks will be specific to a single story; others will be useful for multiple stories. 
Focus only on tasks that are necessary for completing the iteration's stories. 
Don't worry about all-hands meetings, vacations, or other interruptions.Brainstorming tasks is a design activity. 
If everybody has the same ideas about how to develop the software, it should go fairly quickly. 
If not, it's a great opportunity for discussion before coding begins. 
Although brainstorming engineering tasks is a design activity, you don't need to go into too much detail. 
Each engineering task should take a pair one to three hours to complete. 
(This translates into about two to six hours of estimated effort.)  Let pairs figure out the details of each task when they get to them.As each team member has an idea for a task, he should write it down on a card, read it out loud, and put it on the table. 
Everybody can work at once. 
You'll be able to discard duplicate or inappropriate tasks later.As you work, take advantage of your on-site customer's presence to ask about the detailed requirements for each story. 
What do the customers expect when the story is done?After you've finished brainstorming tasks, spread them out on the table and look at the whole picture. 
Are these tasks enough to finish all of the stories?  Are there any duplicates or overlaps?  Is anybody uncertain about how the plan works with the way the software is currently designed?  Discuss and fix any problems.Next, estimate the tasks. 
As with brainstorming, this can occur in parallel, with individual programmers picking up cards, writing estimates, and putting them back. 
Call out the estimates as you finish them. 
If you hear somebody call out an estimate that you disagree with, stop to discuss it and come to consensus.Estimate the tasks in ideal hours. 
How long would the task take if you had perfect focus on the task, suffered no interruptions, and could have the help of anybody on the team?  Estimate in person-hours as well: a task that takes a pair two hours is a four-hour estimate. 
If any of the tasks are bigger than six hours of effort, split them into smaller tasks. 
Combine small tasks that are less than an hour or two.Finally, stop and take a look at the plan again. 
Does anybody disagree with any of the estimates?  Does everything still fit together?As a final check, add up the estimates and compare them to the total task estimates from your previous iteration. 
Using this plan, can you commit to delivering all the stories?  Is there enough slack in the plan for dealing with unexpected problems?You may discover that you aren't comfortable committing to the plan you have. 
If so, see if there are any tasks you can remove or simplify. 
Discuss the situation with your on-site customers. 
Can you replace a difficult part of a story with something easier but equally valuable?  If not, split or remove a story.Similarly, if you feel that you can commit to doing more, add a story to the plan.Continue to adjust the plan until the team is ready to commit to delivering its stories. 
With experience, you should be able to make plans that don't need adjustment.Commitment is a bold statement. 
It means that you're making a promise to your team and to stakeholders to deliver all of the stories in the iteration plan. 
It means that you think the plan is achievable and that you take responsibility, as part of the team, for delivering the stories.Hold a little ceremony at the end of the iteration planning meeting. 
Gather the whole team together—customers, testers, and programmers—and ask everyone to look at the stories. 
Remind everybody that the team is about to commit to delivering these stories at the end of the iteration. 
Ask each person, in turn, if he can commit to doing so. 
Wait for a verbal "yes".It's okay to say "no". 
If anybody seems uncomfortable saying "yes" out loud, remind them that "no" is a perfectly fine answer. 
If somebody does say no, discuss the reason as a team and adjust the plan accordingly.Commitment is important because it helps the team consistently deliver iterations as promised, which is the best way to build trust in the team's ability. 
Commitment gives people an opportunity to raise concerns before it's too late. 
As a pleasant side effect, it helps the team feel motivated to work together to solve problems during the iteration.After you finish planning the iteration, work begins. 
Decide how you'll deliver on your commitment. 
In practice, this usually means that programmers volunteer to work on a task and ask for someone to pair with them. 
As pairs finish their tasks, they break apart. 
Individuals pick up new tasks from the board and form new pairs.Other team members each have their duties as well. 
XP assumes that programmers are the constraint in the system, so other team members rarely have a task planning board like the programmers do. 
Instead, the customers and testers keep an eye on the programmers' progress and organize their work so it's ready when the programmers need it. 
This maximizes the productivity of the whole team.As work continues, revise the iteration plan to reflect the changing situation. 
(Keep track of your original task and story estimates, though, so you can use them when you plan the next iteration.)  Remember that your commitment is to deliver stories, not tasks, and ask whether your current plan will succeed in that goal.At the end of the iteration, release your completed software to stakeholders. 
With a good ten-minute build, this shouldn't require any more than a button press and a few minutes' wait. 
The following morning, start a new iteration by demonstrating what you completed the night before.Iteration planning should take anywhere from half an hour to four hours. 
Most of that time should be for discussion of engineering tasks. 
For established XP teams, assuming they start their iteration demo first thing in the morning, planning typically ends by lunchtime.New teams often have difficulty finishing planning so quickly. 
This is normal during the first several iterations. 
It will take a little while for you to learn your problem space, typical approaches to design problems, and how best to work together.If iteration planning still takes a long time after the first month or so, look for ways to speed it up. 
One common problem is spending too much time doing release planning during the iteration planning meeting. 
Most release planning should happen during the previous iteration, primarily among customers, while the programmers work on stories. 
Picking stories for the iteration plan should be a simple matter of taking stories from the front of the release plan. 
It should only take a few minutes because you won't estimate or discuss priorities.Long planning sessions also result from spending a lot of time trying to break the stories down into engineering tasks. 
This may be a result of doing too much design work. 
Although iteration planning is a design activity, it's a very high level one. 
Most of the real design work will happen during the iteration as pairs work on specific tasks. 
If you spend much time discussing possible design details, ask yourself whether you really need to solve these problems in order to come up with good engineering tasks.If you find that team members don't understand the existing design, or if you have long discussions about how it works, you may lack shared design understanding. 
Remedy this problem with collective code ownership and more pair programming.If you find yourselves speculating about possible design choices, your problem may be a result of trying to make your design too general. 
Remember to keep the design simple. 
Focus on the requirements that you have today. 
Trust pairs doing test-driven development to make good decisions on their own.Design speculation can also occur when you don't understand the requirements well. 
Take advantage of the on-site customer in your meeting. 
Ask him to explain the details of each story and why the system needs to behave in a particular way.Like your release plan, your iteration plan should be a prominent part of your informative workspace. 
Put your stories and tasks on a magnetic whiteboard, as shown in Figure. 
When you start work on a task, take it off of the whiteboard and clip it to your workstation. 
(Mark your initials on the whiteboard so people know where the task went.)  As you finish each task, put it back on the board and circle it with a green marker.One of the difficulties in iteration planning is identifying that things are going wrong in time to fix them. 
I take a brief look at our progress every day. 
Is there a task that's been in progress for more than a day?  It might be a problem. 
If it's halfway through the iteration, are about half of the cards marked green?  If not, we might not finish everything on time.After the iteration ends, take your cards down from the board, add a card on top with some vital statistics (iteration number, dates, whatever else you think is relevant), clip them together, and file them in a drawer. 
Alternatively, you can just throw them away. 
You're unlikely to come back to the cards, but most teams prefer archiving them just in case.Does making a big deal out of commitment mean that you always deliver everything as promised?  No, of course not. 
Commitment is about working around problems and doing what's necessary to deliver the iteration's stories—but sometimes a problem comes up that you can't work around.When you discover a problem that threatens your iteration commitment, first see if there's any way you can change your plan so that you still meet your commitments. 
Would using some of your iteration slack help?  Is there an engineering task that you can simplify or postpone?  Discuss your options as a team and revise your plan.Sometimes, the problem will be to big to absorb. 
In this case, you'll usually need to reduce the scope of the iteration. 
Typically, this involves splitting or removing a story. 
As a team, discuss your options and make the appropriate choice.After changing the plan, the customers should re-establish trust with stakeholders by explaining what happened, why, and what the team is doing to prevent this sort of problem in the future.Despite your best efforts, you may have a bad week and end up with nothing at all to demonstrate to your stakeholders. 
Some teams declare a lost iteration when this happens. 
They roll back their code and use their previous velocity as if the lost iteration never happened. 
Every team makes mistakes, so this is a fine approach if it happens rarely (less than once per quarter). 
If it happens more often, something is wrong. 
Ask your mentor for help.At the end of the iteration, every story should be "done done". 
Partially-completed stories should be rare: they reflect a planning problem. 
That said, they will happen occasionally, particularly when you're new to XP.Some teams feel that the best way to handle partially-completed stories is to delete all of the code for uncompleted stories and deliver only what's completely done. 
This sounds harsh, but it's a good idea. 
"With true timeboxing, the software is either accepted or thrown away at the timebox deadline. 
That makes it clear that the quality level must be acceptable at all times. 
The success of timeboxing depends on being able to meet tight schedules by limiting the product's scope, not its quality." [McConnell, p.581]If you follow this practice, you probably won't throw away much code—the iteration is only a week long. 
Starting fresh may require you to rewrite code, but you'll retain everything you learned when you wrote it the first time. 
The second attempt will often produce better code and may even finish more quickly.If you think this is extreme, as long as you know you will definitely work on that story during the next iteration, it's okay to keep the code. 
However, if you will not immediately continue to work on that story, it's best to delete the code. 
Get it out of your way. 
It's technical debt and baggage. 
If you ever need it, you can always get it out of version control.It's inevitable. 
You're on the third day of the iteration, everything is going well, and a customer comes up to you and says, "Pat, we really need to get this story in."  What do you do?As a programmer, it's very tempting to tell your customer to take a hike—right over the edge of a cliff. 
Changing direction in the middle of an iteration, after all, means an interruption in concentration, delays, and lost work.On the other hand, responsiveness to business needs is a core agile value, so suppress that homicidal urge, smile, and provide your customer with options.You can change the iteration schedule under the condition that you take out as much work as you add. 
In other words, if you're adding a two-point story to the plan, a two-point story needs to come out of the plan.In addition, you may only replace stories that you haven't started yet. 
A two-point story that's half-done isn't a two-point story any more—but it's not a one-point story either. 
It's too difficult to tell how much work you have left on a story until it's "done done", and replacing it will lead to technical debt in the form of half-finished code.Before making any changes, however, remember that your next planning meeting is less than a week away. 
Rather than inserting chaos into the team's work, ask yourself how much of an emergency you're really facing. 
Maybe it can wait until the next planning meeting. 
After all, it's only a day or three away.Dealing with an emergency request every now and then is fine—it's a great way for the team to be responsive. 
On the other hand, an emergency in every iteration means that something is wrong.After the second or third iteration in a row with an emergency request, take a look at what's happening. 
Perhaps your on-site customers need to be more disciplined about release planning. 
Perhaps stakeholders need stronger reminders that requests can wait until the next iteration. 
Often the requests will die down as your organization adapts to the iteration heartbeat.In some cases, however, the team has a legitimate need to provide ongoing support for ad hoc requests. 
If this is true for your team, sacrifice a programmer to be the batman."Batman" is a military term as well as a comic book character: it refers to a soldier assigned to deal with chores so that officers can focus on officering. 
On an XP team, the batman deals with organizational emergencies and support requests so the other programmers can focus on programmering. 
The batman has no other duties: he doesn't work on stories or the iteration plan.Rotate a new programmer into the batman role every iteration to prevent burn-out. 
If the load is particularly high, you may need two or more batmen per iteration.Depending on your situation, you may be better off using daily iterations rather than a batman. 
A daily iteration allows you to postpone all emergencies until the next morning, which enables the team to focus better (see the "Daily Iterations" sidebar). 
It's appropriate for teams that primarily deal with small ad-hoc issues, such as bug-fixes and minor enhancements, and don't have a long-term release plan.You should fix bugs as soon as you find them, preferably as you work on each task. 
This time is part of the overhead of your iteration. 
Don't batch them up for fixing later, even if "later" is as soon as the end of the iteration.Some bugs will be too big to absorb into your iteration slack. 
Create story cards for these and schedule them as soon as possible—or decide that they aren't worth fixing at all.Estimate new stories as they appear throughout the iteration. 
If this is too much of an interruption, batch up the stories and estimate them at a particular time every day.If you have a lot of stories to estimate, as often happens near the beginning of a project, schedule time for estimation with a story.This happens less frequently than you might think; breaking stories into engineering tasks helps modularize the design. 
However, it does happen.It's okay to have two pairs working on the same class. 
In this case, discuss the issue and come to agreement about the names of the class and the public methods that both pairs will be using. 
Then pick one pair to write the class. 
The other pair creates the exact same class but, instead of writing real code, stubs in some hardcoded return values.When you integrate, replace the fake class with the real one and make sure the tests still pass.Whatever she likes, as long as she can easily put it aside when a support request comes in. 
Don't try to squeeze every last drop of efficiency out of the batman; doing so will likely slow the team down and make the batman's job even more tedious.When you use iterations well, your team has a consistent, predictable velocity. 
Stakeholders know what to expect from the team and trust that it will deliver on its commitments. 
The team discovers mistakes quickly and deals with them while still meeting its commitments. 
Rarely, the team meets its commitments by replanning, changing its commitments, and communicating these changes to stakeholders.XP's iterations assume the use of customer-centric stories. 
To successfully deliver software in such a short timescale, the team must use simultaneous phases as well as simple design and incremental design and architecture. 
If you don't use these practices, XP-style iterations probably won't work for you.In order to achieve a consistent velocity and deliver on commitments, your iteration must include slack. 
Never artificially inflate your velocity. 
Similarly, don't use commitment as a club. 
Never force team members to commit to a plan they don't agree with.Energized work is also important. 
Without it, the team will have trouble maintaining equilibrium and a stable velocity.Finally, there's little value to a strict iteration schedule unless you pay close attention to the feedback cycles of velocity, planning, and frequent releases. 
A disciplined iteration schedule may improve predictability and estimation, but you must notice and react to changes in order to take advantage of it.Some methods use longer iterations that release to real customers, not just to internal stakeholders, at the end of each iteration. 
Other methods use independent phases instead of simultaneous phases and iterations. 
Either of these approaches can work, but most of XP's practices assume the presence of short iterations. 
If you don't use XP-style iterations, talk with your mentor about whether XP will work in your situation.Some established XP teams don't use engineering tasks at all. 
Instead, they use very small stories that can each be finished in less than a day. 
This approach works best for established products. 
Other teams use engineering tasks but don't estimate them. 
Either approach can work well, but they're advanced techniques. 
I recommend explicitly creating and estimating engineering tasks to start.Throughout this book, I've assumed that your team uses one-week iterations. 
However, iterations may be of any length. 
Many teams prefer two-week iterations.I've found that shorter iteration lengths are better for teams new to XP. 
Teams seem to mature in their understanding of XP based on how many iterations they've undertaken rather than how many weeks they've experienced. 
As a result, shorter iterations means more rapid improvement for a team new to XP.Short iterations allow the team to practice core XP skills more frequently. 
A short iteration leads to more planning, more internal releases, more iteration demos, and more retrospectives. 
They reduce the team's ability to use overtime to cover up scheduling flaws, which helps the team learn to estimate and plan well.One-week iterations also make decisions easier by reducing schedule risk. 
If a discussion is taking too long, you can say, "This may not be perfect, but we'll review it again next week."  This makes planning and retrospectives easier.On the other hand, one-week iterations put more pressure on the team. 
This makes energized work more difficult and can limit refactoring. 
Velocity is less stable in one-week iterations, because even one holiday represents a big loss of time for the iteration.I prefer one-week iterations for new teams. 
For established teams that are comfortable with all of the XP practices, I prefer two-week iterations. 
Two-week iterations are a little less stressful and lead to a more stable velocity.Three and four-week iterations seem too long to me. 
They don't provide enough feedback to the team or the larger organization. 
However, if you think that a longer iteration would be useful for your team, please try it. 
Be careful: longer iterations are more sensitive to mistakes, because it takes longer to expose and to recover from those mistakes.Don't use longer iterations if you feel that you need more time to get your work done. 
Longer iterations won't change the amount of time you have; they only change how often you check your progress. 
If you have difficulty getting your work done, shorten your iteration length (to a minimum of one week) and look at what you're doing to support energized work. 
Be sure to reduce your workload in each iteration proportionally. 
Shortening your iteration length will reduce the amount of work you have to do each iteration and will help you identify problems in your process.Some teams base their iterations on a number of business days rather than a calendar. 
For example, rather than having a seven calendar-day iteration, the team's iterations are five business days long. 
This is helpful for one-week iterations because it reduces the impact of holidays on the team's velocity. 
However, I don't recommend business-day iterations because they're harder to schedule with stakeholders. 
The regular heartbeat of the XP team is an excellent way to generate trust, and business-day iterations don't have the same impact. 
It's nice to know that Wednesday is always the start of a new iteration.Agile Estimating and Planning [Cohn] and Planning Extreme Programming [Beck & Fowler] each provide alternative ways of approaching iteration planning.Imagine that the power cable for your workstation is just barely long enough to reach the wall receptacle. 
You can plug it in if you stretch it taut, but the slightest vibration will cause the plug to pop out of the wall and the power to go off. 
You'll lose everything you were working on.I can't afford to have my computer losing power at the slightest provocation. 
My work's too important for that. 
In this situation, I would move the computer closer to the outlet so that it could handle some minor bumps. 
(Then I would tape the cord to the floor so people couldn't trip over it, install an uninterruptable power supply, and invest in a continuous backup server.)Your project plans are also too important to be disrupted by the slightest provocation. 
Like the power cord, they need slack.The amount of slack you need doesn't depend on the number of problems you face. 
It depends on the randomness of problems. 
If you always experience exactly 20 hours of problems in each iteration, your velocity will automatically compensate. 
However, if you experience between 20 and 30 hours of problems in each iteration, your velocity will bounce up and down. 
You need ten hours of slack to stabilize your velocity and to ensure that you'll meet your commitments.These numbers are just for illustration. 
Instead of measuring the number of hours you spend on problems, take advantage of velocity's feedback loop (see Estimating later in this chapter for more about velocity). 
If your velocity bounces around a lot, stop signing up for more stories than your velocity allows. 
This will cause your velocity to settle at a lower number that incorporates enough slack for your team. 
On the other hand, if your velocity is rock solid, try reducing slack by committing to a small extra story next iteration.One way to introduce slack into your iterations would be to schedule no work on the last day or two of your iteration. 
This would give you slack, but it would be pretty wasteful. 
A better approach would be to schedule useful, important work that isn't time-critical—work you can set aside in case of an emergency. 
Paying down technical debt fits the bill perfectly.Even the best teams inadvertantly accumulate technical debt. 
Although you should always make your code as clean as you can, some technical debt will slip by unnoticed.Rather than doing the bare minimum necessary to keep your head above water, be generous in refactoring and cleaning up technical debt in existing code. 
Every iteration, look for opportunities to make existing code better. 
Make this part of your everyday work. 
Every time I find myself scratching my head over a variable or method name, I change it. 
If I see some code that's no longer in use, I delete it.In addition to these small improvements, look for opportunities to make larger changes. 
Perhaps the code uses primitives rather than introducing a new type, or perhaps a class needs to have some of its responsibilities extracted into a new class.Paying down technical debt directly increases team productivity, so I spend a lot of time on it throughout the iteration. 
I usually spend about eight hours per week paying down technical debt, but other members of my teams only spend a few hours. 
A good rule of thumb is to spend ten percent of the iteration on technical debt.Don't spend all your time on a single problem. 
Refactor throughout the iteration—an hour encapsulating a structure here, two hours fixing class responsibilities there. 
Each refactoring should address a specific, relatively small problem. 
Sometimes you'll fix only part of a larger problem—that's okay as long as it makes the code better. 
If your team pays down technical debt every week, you'll have the opportunity to see and fix remaining problems in the future.As you fix technical debt, focus on fixes that make your current work easier. 
Don't go looking for technical debt that's unrelated to stories you're currently working on. 
If you consistently relate your improvements to your current work, you'll automatically put the most effort into the most frequently-modified and most valuable parts of the system.To keep up with their constantly expanding field, programmers must continually improve their skills. 
In doing so, they will often learn things that enhance their work on the project.Dedicated research time is an excellent way to encourage learning and add additional slack into your iterations. 
To introduce it, set aside half a day for each programmer to conduct self-directed research on a topic of his choice. 
Be completely hands-off. 
I recommend only two rules: don't spend this time on project stories or tasks, and don't modify any project code.I've introduced this technique to several teams and it's paid dividends each time. 
Two weeks after introducing research time at one organization, the product manager told me that research time was the most valuable time the team spent, and suggested that we double it.Research time works because programmers are typically motivated by a desire to do good work, particularly when they're self-directed. 
Most programmers have a natural desire to make their lives easier and to impress their colleagues. 
As a result, the work done in research time often has a surprisingly high return for the project.Research time is particularly valuable for XP teams. 
The continuous drumbeat of iteration deadlines is great for reducing risk and motivating the team to excel, but it can lead to tunnel vision. 
Dedicated research time gives programmers a chance to widen their ranges, which often leads to insights about how to develop more effectively.For research time to be effective, you must focus. 
Half a day can go by very quickly. 
It's easy to think of research time as a catch-all for postponed meetings. 
Be strict about avoiding interruptions and enlist the help of the project manager. 
Ignore your email, turn off your IM, and use your web browser only for specific research.When you first adopt research time, you might have trouble deciding what to work on. 
Think about what has puzzled you recently. 
Would you like to learn more about the details of your UI framework?  Is there a programming language that you've wanted to try, but your organization doesn't use?  Has real-time networking always fascinated you?As you do your research, create spike solutions—small, standalone programs—that demonstrate what you've learned. 
Avoid trying to make software that's generally useful; that will reduce the amount of time available to pursue core ideas. 
If something turns out to deserve further work, create and schedule a story for it.Research time and paying down technical debt are important tasks that enhance your programmers' skills and allow them to deliver more quickly. 
Paying down technical debt, in particular, should be part of every iteration. 
However, if your iteration commitments are at risk, it's okay to set these two tasks aside temporarily in order to meet those commitments.Before starting a big refactoring to pay down technical debt, consider the iteration plan and think about the kinds of problems the team has encountered. 
If the iteration is going smoothly, go ahead and refactor. 
If you've encountered problems or are a little behind schedule, shrug your shoulders and work on meeting your iteration commitment instead. 
You'll have another opportunity to fix the problem later. 
By varying the amount of time you spend paying down technical debt, you can ensure that most iterations come in exactly on time.Every iteration experiences a few bumps. 
Varying the time you spend paying down technical debt is your first line of defense. 
Rarely, though, a problem sneaks past. 
In this case, if family commitments permit, I voluntarily work a few extra hours. 
Overtime is unusual in the companies I work with, so an extra hour a day once in a while isn't a big deal. 
Another programmer or two will often volunteer to do the same.Some problems will be too significant to clean up in overtime. 
In this case, consider cancelling research time in order to address the problem. 
First, though, take a second look at whether you need to re-plan the iteration instead (see Iteration Planning earlier in this chapter). 
When problems are big, even cancelling research time may not be enough.Slack is a wonderful tool. 
It helps you meet your commitments and gives you time to perform important, non-urgent tasks that improve your productivity.Be careful, though. 
Although slack is a wonderful way to handle transitory problems, it can also disguise systemic problems. 
If you rely on slack to finish your stories in every iteration, that time isn't really slack—it's required.To return to the power cord example, suppose that your workstation is on a jiggly desk. 
Every time you type, the desk jiggles a bit and the power cord pops out. 
You could add just enough slack to the cord to handle your typing, but that wouldn't really be slack; it's necessary to use the computer at all. 
If anything else happens while you're typing—if Bob from marketing gives you a friendly slap on the back—the cord will still pop out.If you work overtime, cancel research time, or don't pay down any technical debt for two or three iterations in a row, you've over-committed and have no slack. 
Congratulate yourself for delivering on your commitments anyway. 
Now add slack by reducing your velocity.Making enough time for these non-urgent tasks is difficult. 
With a deadline rushing head-first toward you, it's difficult to imagine spending any time that doesn't directly contribute to getting stories out the door. 
New XP teams especially struggle with this. 
Don't give up. 
Slack is essential to meeting commitments, and that is the key to successful XP.In my experience, there are two big sources of randomness on XP teams: customer unavailability and technical debt. 
Both of these lead to an unpredictable environment, make estimating difficult, and require you to have more slack in order to meet your commitments.If programmers have to wait for customer answers as they work, you can reduce the need for slack by making customers more available to answer programmer questions. 
They may not like that—customers are often surprised by the amount of time XP needs from them—but if you explain that it will help improve velocity, they may be more interested in helping.On the other hand, if programmers often encounter unexpected technical delays, such as surprising design problems, difficulty integrating, or unavailability of a key staging environment, then your need for slack comes from too much technical debt. 
Fortunately, using your slack to pay down technical debt will automatically reduce the amount of slack you need in the future.First, pair programming and test-driven development (TDD) should allow you to deliver more quickly, not more slowly. 
However, they do have a learning curve—particularly TDD—so it's true that avoiding TDD might allow you to meet your commitments in the early stages of your project.However, you shouldn't use them as slack. 
Pair programming, test-driven development, and similar practices maintain your capability to deliver high-quality code. 
If you don't do them, you will immediately incur technical debt and hurt your productivity. 
You may meet this iteration's commitments, but you'll do so at the expense of the next iteration. 
If your existing slack options aren't enough, you need to replan your iteration, as discussed in Iteration Planning earlier in this chapter.In contrast, paying down technical debt, going home on time, and conducting research enhance your capability to deliver. 
Using them as slack once in a while won't hurt.In my experience, research time pays dividends within a few months. 
It's worthwhile even without an explicit policy of encouraging professional development.However, if research time isn't appropriate for your organization, you can increase the amount of time you spend paying down technical debt instead.You can if you want, but you don't have to. 
Treat it as you would any other spike.The book Critical Chain [Goldratt 1997] argues for creating a single buffer at the end of a project rather than padding each estimate. 
It's good advice, and adding slack to each iteration might seem to contradict that.However, an XP team makes a commitment to deliver every week. 
In a sense, an XP team conducts a series of one-week projects, each with a commitment to stakeholders and its own small buffer protecting that commitment. 
This commitment is necessary for stakeholder trust, to take advantage of feedback, and to overcome the inherently high risk of software projects. 
Without it, the project would drift off course.XP avoids Parkinson's Law ("work expands to fill the time available") and Student Syndrome ("work is delayed until its deadline") by performing important but non-urgent tasks in the buffer.When you incorporate slack into your iterations, you consistently meet your iteration commitments. 
You rarely need overtime. 
In addition, by spending so much time paying down technical debt, your code steadily improves, increasing your productivity and making further enhancements easier.The danger of thinking of these tasks as slack is that you'll think they aren't important. 
They're actually vital, and a team that doesn't perform these tasks will slow down over time. 
They're just not time-critical like your iteration commitment is. 
Don't use slack as an excuse to set aside these tasks indefinitely.In addition, never incur technical debt in the name of slack. 
If you can't meet your iteration commitments while performing standard XP tasks, replan the iteration instead. 
Practices you should never use as slack include test-driven development, refactoring new code, pair programming, and making sure stories are "done done".Finally, don't use try to use iteration slack to meet release commitments. 
There isn't enough iteration slack to make a meaningful difference to your release commitments—in fact, removing slack from your iterations could easily add so much chaos to your process that productivity actually goes down. 
Use risk management to provide slack for your release commitments.Slack allows you to be "done done" and meet your iteration commitments. 
It enables a consistent velocity. 
It provides an opportunity for extra refactoring. 
It reduces technical debt and increases team capability. 
I'm not aware of any alternatives that provide all of these benefits.Rather than including slack at the iteration level, some teams add a safety buffer to every estimate. 
As [Goldratt 1992] explains, this leads to delays and often doesn't improve the team's ability to meet their commitments.Many organizations, however, choose not to include any slack in their plans. 
It's no coincidence that these organizations have trouble meeting commitments. 
They also waste a lot of time thrashing around as they attempt to achieve the unachievable.Some teams form a reading group rather than conducting research time. 
This is an excellent alternative to research time, particularly for teams that are interested in discussing fundamental ideas rather than exploring new technologies.One way to do so is to take turns reading sections of a book or an article out loud and then discussing them. This approach has the advantage of not requiring advance preparation, which promotes more active participation. 
One person should act as facilitator and inspire discussion by asking questions about each section.A reading group can easily spend a half a day per week in productive conversation. 
To take this book as an example, you could probably discuss one or two practice sections per session.Other teams schedule silver stories for slack. 
These are less-important stories that they can set aside if they need extra time. 
I prefer to pay down technical debt instead because teams often neglect this crucial task.The other difficulty with silver stories is that you need to set aside slack tasks at a moment's notice. 
If you do that to a silver story, you'll leave behind technical debt in the form of half-done code. 
If you use silver stories, be sure to delete all code related to the story if you have to put it aside.I've also observed that silver stories hurt programmer morale. 
If the team needs to use some slack, they don't finish all of the stories on the board. 
Even though the silver stories were bonus stories, it doesn't feel like that in practice. 
In contrast, any time spent paying down technical debt directly improves programmers' quality of life. 
That's a win.Slack: Getting Past Burnout, Busywork, and the Myth of Total Efficiency [DeMarco 2002] provides a compelling case for slack throughout the organization.The Goal and Critical Chain [Goldratt 1992] and [Goldratt 1997] are two business novels that makes the case for using slack (or "buffers"), instead of padding estimates, to protect commitments and increase throughput."Silicon Valley Patterns Group Study of Domain-Driven Design" (http://domaindrivendesign.org/study/siliconvalleypatterns/) is an interesting transcript of some meetings of a long-running and highly-successful reading group.Stories are for planning. 
They're simple, one-or-two line descriptions of work the team should produce. 
Alistair Cockburn calls them "promissory notes for future conversation"1. 
Everything that stakeholders want the team to produce should have a story:1http://c2.com/cgi/wiki?UserStory, accessed 30 May 2007.This isn't enough detail for the team to implement and release working software, nor is that the intent of stories. 
A story is a placeholder for a detailed discussion about requirements. 
Customers are responsible for having the requirements details available when the rest of the team needs them.Although stories are short, they still have two important characteristics.The following examples are not stories:Write stories on index cards.This isn't the result of some strange Ludditian urge on the part of XP's creators; it's a deliberate choice based on the strengths of the medium. 
You see, physical cards have one feature that no conglomeration of pixels has: they're tactile. 
You can pick them up and move them around. 
This gives them power.During release planning, customers and stakeholders gather around a big table to select stories for the next release. 
It's a difficult process of balancing competing desires. 
Index cards help prevent these disputes by visually showing priorities, making the scope of the work more clear, and directing conflicts towards the plan rather than personalities.Story cards also form an essential part of an informative workspace. 
After the planning meeting, move the cards to the release planning board—a big, six-foot whiteboard, placed prominently in the team's open workspace (see Figure). 
You can post hundreds of cards and still see them all clearly. 
For each iteration, place the story cards to finish during the iteration on the iteration planning board—another big whiteboard; see Figure—and move them around to indicate your status and progress. 
Both of these boards are clearly visible throughout the team room and constantly broadcast information to the team.Index cards also help you be responsive to stakeholders. 
When you talk with a stakeholder and she has a suggestion, invite her to write it down on a blank index card. 
I always carry cards with me for precisely this purpose. 
Afterwards, take the stakeholder and her card to the product manager. 
They can walk over to the release planning board and discuss the story's place in the overall vision. 
Again, physical cards focus the discussion on relative priorities rather than contentious "approved / disapproved" decisions.If the product manager and stakeholder decide to add the story to the release plan, they can take it to the programmers right away. 
A brief discussion allows the programmers to estimate the story. 
Developers write their estimate—and possibly a few notes—on the card, and then the stakeholder and product manager place the card into the release plan.Physical index cards enable these ways of working in a very easy and natural way that's surprisingly difficult to replicate with computerized tools. 
Although you can use software, index cards just work better: they're easier to set up and manipulate, make it easier to see trends and the big picture, and allow you to change your process with no configuration or programming.Most people are skeptical about the value of index cards at first, so if you feel that way, you're not alone. 
The best way to evaluate the value of physical story cards is to try them for a few months, then decide whether to keep them.Stories need to be customer-centric. 
Write them from the on-site customers' point of view and make sure that they provide something that customers care about. 
On-site customers are in charge of priorities in the planning game, so if a story has no customer value, your customers won't—and shouldn't—include it in the plan.One practical result of customer-centric stories is that you won't have stories for technical issues. 
There should be no "Create a build script" story, for example—customers wouldn't know how to prioritize it. 
Although programmers can tell customers where the technical stories belong in the plan, that disrupts the balance of power over the scope of the project and can leave customers feeling disenfranchised.Instead, include any technical considerations in the estimate for each story. 
If a story requires that the programmers create or update a build script, for example, include that cost when estimating for that story.Customer-centric stories aren't necessarily always valuable to the end-user, but they should always be valuable to the on-site customers. 
For example, a story to produce a trade-show demo doesn't help end-users, but it helps the customers sell the product.Stories can start at any size, but it is difficult to estimate stories that are too large or too small. 
Split large stories; combine small ones.The right size for a story depends on your velocity. 
You should be able to complete four to ten stories in each iteration. 
Split and combine stories to reach this goal. 
For example, a team with a velocity of ten days per iteration might split stories with estimates of more than two days, and combine stories that are less than half a day.Combining stories is easy. 
Take several similar stories, staple their cards together, and write your new estimate on the front.Splitting stories is more difficult, because it tempts you away from vertical stripes and releasable stories. 
It's easiest to just create a new story for each step in the previous story. 
Unfortunately, this approach leads to story clumps. 
Instead, consider the essence of the story. 
Peel away all of the other issues and write them as new stories. 
[Cohn] has an excellent chapter on how to do this in his book Agile Estimating and Planning. 
He summarizes various options for splitting stories:Most stories will add new capabilities to your software, but any action that requires the team's time but is not a part of normal work needs a story.XP teams need very little documentation to do their work (see Documentation), but you may need the team to produce documentation for other reasons. 
Create documentation stories just like any other: make them customer-centric and make sure you can identify specific completion criteria. 
An example of a documentation story is "Produce user manual".Performance, scalability, and stability—so-called non-functional requirements—should be scheduled with stories too. 
Be sure that these stories have precise completion criteria. 
See Performance Optimization for more.Ideally, your team will fix bugs as soon as they find them, before declaring a story as "done done". 
Nobody's perfect, though, and you will miss some bugs. 
Schedule these bugs with story cards: "Fix multiple-user editing bug". 
Schedule them as soon as possible to keep your code clean and reduce your need for bug-tracking software.Bug stories can be difficult to estimate. 
Often, the biggest timesink in debugging is figuring out what's wrong, and you usually can't estimate how long that will take. 
Instead, provide a time-boxed estimate. 
"We'll spend up to a day investigating this bug. 
If we haven't fixed it by then, we'll schedule another story."Sometimes programmers won't be able to estimate a story because they don't know enough about the technology required to implement the story. 
In this case, create a story to research that technology. 
An example of a research story is "Figure out how to estimate 'Send HTML' story". 
Programmers will often use a spike solution (see Spike Solutions) to research the technology, so these sorts of stories are often called spike stories.Word these stories in terms of the goal, not the research that needs to be done. 
When programmers work on a research story, they only need to do enough work to make their estimate for the real story. 
They shouldn't try to figure out all of the details or solve the entire problem.Programmers can usually estimate how long it will take to research a technology even if they don't know the technology in question. 
If they can't even estimate how long the research will take, timebox the story as you do with bug stories. 
I find that a day is plenty of time for most spike stories, and half a day is sufficient for most.Other than spike stories, you normally don't need to schedule time for the programmers to estimate stories—you can just ask them for an estimate at any time. 
It's part of the overhead of the iteration, as are support requests and other unscheduled interruptions. 
If your programmers feel that estimating is too much of an interruption, try putting new story cards in a pile for the programmers to estimate when it's convenient.Sometimes you'll have a large number of new stories to estimate. 
In this case, it might be worth creating a story card for estimating those stories.Like estimating, most meetings are part of the normal overhead of the iteration. 
If you have an unusual time commitment, such as training or an day-long off-site, you can reserve time for it with a story.Don't create stories for technical details. 
Technical tasks are part of the cost of implementing stories and should be part of the estimates. 
Use incremental design and architecture to break large technical requirements into small pieces that you can implement incrementally.See our discussion of reporting (Reporting) for more information.If you exercise reasonable care, you're unlikely to lose cards. 
The only time I've seen a team lose their cards was when they created them and then left them in an ignored pile somewhere on their boss's desk for six months.That said, there's always the possibility of fire or other disaster. 
If the risk of losing cards is too great for you, consider taking a digital photo of the release planning board every week or so. 
An eight megapixel camera has sufficient resolution to capture a six-foot whiteboard and all of the detail on the cards.If you only have a few stories in an iteration, it's harder to see that you're making progress, which increases the risk that you won't see problems in time to correct them. 
In addition, if something goes wrong and you can't finish a story, it will represent a large percentage of your work. 
Too many stories, on the other hand, increases your planning and estimating burden. 
Each story becomes smaller, making it harder to see the big picture.An average of six stories per iteration leads to a three-month release plan (the maximum I recommend) consisting of 78 stories. 
That's a nice number. 
It gives you flexibility in planning without overwhelming you with details.It's much more difficult to create customer-centric stories than programmer-centric stories, so it's tempting to find excuses for avoiding them. 
"Our customers don't mind if we have programmer-centric stories" is one such excuse. 
Try to avoid it.Even if your customers really do have the ability to prioritize programmer-centric stories, customer-centric stories lead to better plans. 
Remember, your goal is to create stories that allow you to release the software at any time. 
Programmer-centric stories usually don't have that property.If your customers are programmers—if you're writing software for programmers, such as a library or framework—then your stories should use a programmer-centric language (see Ubiquitous Language). 
Even so, they should reflect your customers' perspective. 
Create stories about your customers' needs, not your plans to fulfill their needs.When a stakeholder asks you for a feature, take out an index card and invite him to write it down so it can be scheduled. 
For electronic requests, a customer should follow up, either by speaking to the requester in person or creating the story card himself.If stakeholders refuse to use stories, the product manager can manage this relationship by providing stakeholders with what they want to see and translating stakeholders wishes into stories for the team.When you use stories well, the on-site customers understand all of the work that they approve and schedule. 
You work on small, manageable, and independent pieces and can deliver complete features frequently. 
The project always represents the best possible value to the customer at any point in time.Stories are no replacement for requirements. 
You need another way of getting details, whether through expert customers on-site (the XP way) or a requirements document (the traditional way).Be very cautious of using customer-centric stories without also using most of the XP development practices. 
Customer-centric stories depend on the ability to implement infrastructure incrementally with incremental design and architecture. 
Without this ability, you're likely to incur greater technical debt.Physical index cards are only appropriate if the team sits together, or at least has a common area in which they congregate. 
Experienced distributed teams often keep physical index cards at the main site and copy the cards into the electronic system. 
This is an administrative headache, but for these teams, the benefits of physical cards make the added work worthwhile.Some organizations are skittish about using informal planning tools. 
If important members of your organization require a formal Gantt chart, you may need to provide it. 
Your project manager can help you make this decision. 
As with a distributed team, you may find it valuable to use physical cards as well, then duplicate the information into the tool.For teams that don't use stories, the main distinction between stories and the line items in most plans is that stories are customer-centric. 
If you can't use customer-centric stories for some reason, customers cannot participate effectively in the planning game. 
This will eliminate one of its primary benefits: the ability to create better plans by blending information from both customers and programmers. 
Unfortunately, no alternative practice will help.Another distinctive feature of stories is the use of index cards. 
Physical cards offer many benefits over electronic tools, but you can still use an electronic tool if necessary. 
Some teams track their stories using spreadsheets and others use dedicated agile planning tools. 
None of these approaches, however, provide the benefits of physical index cards.Agile Estimating and Planning, by [Cohn], discusses options for splitting stories in Chapter 12.Programmers often consider estimating to be a black art—one of the most difficult things they must do. 
Many programmers find that they consistently estimate too low. 
To counter this problem, they pad their estimates (multiplying by three is a common approach) but sometimes even these rough guesses are too low.Are good estimates possible?  Of course!  You just need to focus on your strengths.Part of the reason estimating is so difficult is that programmers can rarely predict how they will spend their time. 
A task that requires eight hours of uninterrupted concentration can take two or three days if the programmer must deal with constant interruptions. 
It can take even longer if the programmer works on another task at the same time.Part of the secret to good estimates is to predict the effort, not the calendar time that a project will take. 
Make your estimates in terms of ideal engineering days (often called story points): the number of days a task would take if you focused entirely on it and experienced no interruptions.Ideal time alone won't lead to accurate estimates. 
I've asked some of the teams I've worked with to measure exactly how long each task takes them. 
One team gave me 18 months of data, and even though we estimated in ideal time, the estimates were never accurate.Still, they were consistent. 
For example, one team always estimated their stories at about 60% of the time they actually needed. 
This may not sound very promising. 
How useful can inaccurate estimates be, especially if they don't correlate to calendar time?  Velocity holds the key.Although estimates are almost never accurate, they are consistently inaccurate. 
While the estimate accuracy of individual estimates is all over the map—one estimate might be half the actual time, another might be 20 percent more than the actual time—the estimates are consistent in aggregate. 
Additionally, although each iteration experiences a different set of interruptions, the amount of time required for the interruptions also tends to be consistent from iteration to iteration.As a result, you can reliably convert estimates to calendar time if you aggregate all the stories in an iteration. 
A single scaling factor is all you need.This is where velocity comes in. 
Your velocity is the number of story points you can complete in an iteration. 
It's a simple, yet surprisingly sophisticated tool. 
It uses a feedback loop: every iteration's velocity reflects what the team actually achieved in the previous iteration.This feedback leads to a magical effect. 
When the team underestimates their workload, they are unable to finish all of their stories by the iteration deadline. 
This causes their velocity to go down, which in turn reduces the team's workload, allowing them to finish everything on time the following week.Similarly, if the team overestimates their workload, they find themselves able to finish more stories by the iteration deadline. 
This causes their velocity to go up, which increases the team's workload to match their capacity.Velocity is an extremely effective way of balancing the team's workload. 
In a mature XP team, velocity is stable enough to predict schedules with a high degree of accuracy (see Risk Management earlier in this chapter).Velocity relies upon a strict iteration timebox. 
To make velocity work, never count stories that aren't "done done" at the end of the iteration. 
Never allow the iteration deadline to slip, not even by a few hours.You may be tempted to cheat a bit and work longer hours, or to slip the iteration deadline, in order to finish your stories and make your velocity a little bit higher. 
Don't do that!  Artificially raising velocity sabotages the equilibrium of the feedback cycle. 
If continued, your velocity will gyrate out of control, which will likely reduce your capacity for energized work in the process. 
This will further damage your equilibrium and your ability to meet your commitments.Velocity tends to be unstable at the beginning of a project. 
Give it three or four iterations to stabilize. 
After that point, you should achieve the same velocity every iteration, unless there's a holiday during the iteration. 
Use your iteration slack to ensure that you consistently meet your commitments every iteration. 
I look for deeper problems if the team's velocity changes more than one or twice per quarter.There's a secret to estimating. 
Experts automatically make consistent estimates1. 
All you have to do use a consistent estimating technique. 
When you estimate, pick a single, optimistic value. 
How long will the story take if you experience no interruptions, can pair with anyone else on the team, and everything goes well?  There's no need to pad your estimates or provide a probabilistic range with this approach. 
Velocity automatically applies the appropriate amount of padding for short-term estimates and risk management adds padding for long-term estimates.1Unless you have a lot of technical debt. 
If you do, add more iteration slack (see Slack earlier in this chapter) to compensate for the inconsistency.There are two corollaries to this secret. 
First, if you're an expert but you don't trust your ability to make estimates, relax. 
You automatically make good estimates. 
Just imagine the work you're going to do and pick the first number that comes into your head. 
It won't be right, but it will be consistent with your other estimates. 
That's sufficient.Second, if you're not an expert, the way to make good estimates is to become an expert. 
This isn't as hard as it sounds. 
An expert is just a beginner with lots of experience. 
To become an expert, make a lot of estimates with relatively short timescales and pay attention to the results. 
In other words, follow the XP practices.All of the programmers should participate in estimating. 
At least one customer should be present to answer questions. 
Once the programmers have all the information they need to make an estimate, one programmer will suggest an estimate. 
Allow this to happen naturally. 
The person who is most comfortable will speak first. 
Typically this is the person who has the most expertise.If the suggested estimate doesn't sound right, or if you don't understand where it came from, ask for details. 
Alternatively, if you're a programmer, provide your own estimate and explain your reasoning. 
The ensuing discussion will clarify the estimate. 
When all of the programmers confirm an estimate, write it on the card.At first, different team members will have differing ideas of how long something should take. 
This will lead to inconsistent estimates. 
If the team makes estimates as a group, programmers will automatically synchronize their estimates within the first several iterations.Comparing your estimates to the actual time required for each story or task may also give you the feedback you need to become more consistent. 
To do so, track your time as described in Reporting in Chapter 6.Estimate stories in story points. 
When you start estimating stories, think of a story point as an ideal day.When you start estimating stories, imagine the engineering tasks you will need to implement it. 
Ask your on-site customers about their expectations, focusing on those things that would affect your estimate. 
If you encounter something that seems expensive, provide the customers with less costly alternatives.As you gain experience with the project, you will begin to make estimates intuitively rather than mentally breaking them down into engineering tasks. 
Often, you'll think in terms of similar stories rather than ideal days. 
For example, you might think that a story is a typical report, and typical reports are one point. 
Or you might think that a story is a complicated report, but not twice as difficult as a regular report, and estimate it at one and a half points.Sometimes you will need more information to make an accurate estimate. 
In this case, make a note on the card. 
If you need more information from your customers, write "??" (for "unanswered questions") in place of the estimate. 
If you need to research a piece of technology further, write "Spike" in place of the estimate and create a spike solution story (see Stories earlier in this chapter).During iteration planning, programmers will create engineering tasks that allow them to deliver the stories planned for the iteration. 
Each engineering task is a concrete, technical task such as "update build script" or "implement domain logic".Estimate engineering tasks in ideal hours rather than ideal days. 
When you think about the estimate, be sure to include everything necessary for the task to be "done done"—testing, customer review, and so forth. 
See Iteration Planning earlier in this chapter for more information.If the programmers understand the requirements and are experts in the required technology, they should be able to estimate a story in less than a minute. 
If the programmers need to discuss the technology, or if they need to ask questions of the customers, then estimating may take longer. 
I look for ways to bring discussions to a close if an estimate takes longer than five minutes, and I look for deeper problems if every story involves detailed discussion.One common cause of slow estimates is inadequate customer preparation. 
To make their estimates, programmers often ask questions the customers haven't considered. 
In some cases, customers will disagree on the answer and need to work it out.A customer huddle—in which the customers briefly discuss the issue, come to a decision, and return—is one way to handle this. 
Another way is to write "??" in place of the estimate and move on to the next story. 
The customers then work out the details at their own pace, and the programmers estimate the story later.Expect the customers to be unprepared for programmer questions during the first several iterations. 
Over time, they will learn to anticipate most of the questions programmers will ask.Programmer inexperience can also cause slow estimating. 
If the programmers don't understand the problem domain well, they will need to ask a lot of questions before they can make an estimate. 
As with inadequate customer preparation, this problem will go away in time. 
If the programmers don't understand the technology, however, immediately create a spike story and move on.Some programmers try to figure out all the details of the requirements before making an estimate. 
However, only those issues which would change the estimate by a half-point or more are relevant. 
It takes practice to figure out which details are important and which you can safely ignore.This sort of over-attention to detail sometimes occurs when a programmer is reluctant to make estimates. 
A programmer who worries that someone will use her estimate against her in the future will spend too much time trying to make her estimate perfect rather than settling on her first impression.Programmer reluctance may be a sign of organizational difficulties or excessive schedule pressure, or it may also stem from past experiences that have nothing to do with the current project. 
In the latter case, the programmers will usually come to trust the team over time.To help address these estimation issues, ask leading questions. 
For example:It's almost a law of physics: customers and stakeholders are invariably disappointed with the amount of features their teams can provide. 
Sometimes they express that disappointment out loud. 
The best way to deal with this is to ignore the tone and treat the customers' questions as honest requests for information.In fact, a certain amount of back-and-forth is healthy: it helps the team focus on the high-value, low-cost elements of the customers' ideas. 
(See The Planning Game earlier in this chapter.)One common challenge is to say, "Why does that cost so much?"  Resist the immediate urge to defend yourself and your sacred honor, pause a moment to collect your thoughts, then list the issues you considered when coming up with the estimate. 
Suggest options for reducing the cost of the story by reducing scope.Your explanation will usually satisfy your customers. 
In some cases, they'll ask for more information. 
Again, treat these questions as simple requests. 
If there's something you don't know, admit it, and explain why you made your estimate anyway. 
If a question reveals something that you haven't considered, change your estimate.Be careful, though: the questions may cause you to doubt your estimate. 
Your initial, gut-feel estimate is most likely correct. 
Only change your estimate if you learn something genuinely new. 
Don't change it just because you feel pressured. 
As the programmers who will be implementing the stories, you are the most qualified to make the estimates. 
Be polite, but firm:If a customer reacts with disbelief or browbeats you, he may not realize how disrespectful he's being. 
Sometimes making him aware of his behavior can help.Customers and stakeholders may also be confused by the idea of story points. 
I start by providing a simplified explanation:If the questioner pushes for more information, I explain all of the details of ideal days and velocity. 
That often inspires concern. 
"If our velocity is ten ideal days and we have six programmers, shouldn't our velocity be 30?"You can try to explain that ideal days aren't the same as developer days, but that has never worked for me. 
Now I just offer to provide detailed information.(Reporting in Chapter 6 discusses time usage reports in detail.)These questions often dissipate as customers and stakeholders gain trust in the team's ability to deliver. 
If they don't, or if the problems are particularly bad, enlist the help of your project manager to defuse the situation. 
Trust in chapter 6 has further suggestions.Your velocity can suffer for many reasons. 
The following options might allow you to improve your velocity:The most common technical problem I see is excessive technical debt. 
This has a bigger impact on team productivity than any other factor does. 
Make code quality a priority and your velocity will improve dramatically. 
However, this isn't a quick fix. 
Teams with excessive technical debt often have months—or even years—of cleanup ahead of them. 
Rather than stopping work to pay down technical debt, fix it incrementally. 
Iteration slack is the best way to do so, although you may not see a noticeable improvement for several months.If your customers aren't available to answer questions when programmers need them, programmers either have to wait or make guesses about the answers. 
Both of these hurt velocity. 
To improve your velocity, make sure that a customer is always available to answer programmer questions.Tired, burned-out programmers make costly mistakes and don't put forth their full effort. 
If your organization has been putting pressure on the team, or if programmers have worked a lot of extra hours, shield the programmers from organizational pressure and consider instituting a no-overtime policy.If programmers are the constraint for your team—as this book assumes—then hand any work that other people can do to other people. 
Find ways to excuse programmers from unnecessary meetings, shield them from interruptions, and have somebody else to take care of organizational bureaucracy such as time sheets and expense reports. 
You could even hire an administrative assistant for the team to handle all non-project-related matters.Most programming teams have all of the resources they need. 
However, if your programmers complain about slow computers, insufficient RAM, or inavailability of key materials, get it for them. 
It's always a surprise when a company nickle-and-dimes its software teams. 
Does it make sense to save $5,000 in equipment costs if it costs your team half an hour per programmer every day?  A team of six programmers will recoup that cost within a month. 
And what about the opportunity costs of delivering fewer features?Velocity is related to the number of programmers on your team, but unless your project is woefully understaffed and experienced personnel are readily available, adding people won't make an immediate difference. 
As [Brooks] famously said, "adding people to a late project only makes it later."  Expect the new employees to take a month or two to be productive. 
Pair programming, collective code ownership, and a shared workspace will help reduce that time, though adding junior programmers to the team can actually decrease productivity.Likewise, adding to large teams can cause communication challenges that decrease productivity. 
Six programmers is my preferred size for an XP team and I readily add good programmers to reach that number. 
Past six, I am very cautious about adding programmers, and I avoid team sizes greater than ten programmers.If you add or remove only one person, try leaving your velocity unchanged and see what happens. 
Another option is to adjust your velocity proportionally to the change. 
Either way, your velocity will adjust to the correct number after another iteration.Your iteration slack should handle minor variations in people's availability. 
If a large percentage of the team is away, as during a holiday, your velocity may go down for an iteration. 
This is normal. 
Your velocity should recover in the next iteration.If you have a small number of programmers—four or fewer—you may find that even one day of absence is enough to affect your velocity. 
In this case, you may wish to use two-week iterations. 
See Iteration Planning earlier in this chapter for a discussion of the trade-offs.For your first iteration, just make your best guess. 
Set your velocity for the next iteration based on what you actually complete. 
Expect your velocity to take three or four iterations to stabilize.Your organization may want you to make release commitments before your velocity has stabilized. 
The best approach is to say, "I don't have a schedule estimate yet, but we're working on it. 
I can promise you an estimate in three or four weeks. 
We need the time to calibrate the developers' estimates to what they actually produce."It does take a lot of programmer-hours for all of the programmers to estimate together, but this isn't wasted time. 
Estimating sessions are not just for estimation—they're also a crucial first step in communicating and clarifying requirements. 
Programmers ask questions and clarify details, which often leads to ideas that the customers haven't considered. 
Sometimes this collaboration reduces the overall cost of the project. 
(See The Planning Game earlier in this chapter.)All of the programmers need to be present to ensure that they all understand what they will be building. 
Having the programmers together also increases estimate accuracy.Sometimes you will miss something important. 
If Joe hadn't asked Mary about the age column, the team would have missed a major problem and their estimate would have been wrong. 
These mistakes happen. 
Information obvious to customers isn't always obvious to programmers.Although you cannot prevent these mistakes, you can reduce them. 
If all of the programmers estimate together, they're more likely to ask the right questions. 
Mistakes will also decrease as the team becomes more experienced in making estimates. 
Customers will learn what details to provide and programmers will learn which questions to ask.In the meantime, don't worry about it unless you encounter these surprises frequently. 
Address unexpected details when they come up. 
(See Iteration Planning earlier in this chapter.)If unexpected details frequently surprise you, and the problem doesn't improve with experience, ask your mentor for help.Story estimates don't need to be accurate, just self-consistent. 
As a result, you only need to re-estimate stories when your understanding of the story changes in a way that affects its difficulty or scope.XP uses incremental design and architecture, so the whole design gradually improves over time. 
As a result, your estimates will usually remain consistent with each other.With proper incremental design and architecture, technical dependencies should be rare, although they can happen. 
I typically make a note in the estimate field: "six (four if foo story done first)."If you find yourself making more than a few of these notes, something is wrong with your approach to incremental design. 
Ask your mentor for help.[DeMarco 2002] has argued that organizations set project deadlines based on the value of the project. 
In other words, the project is only worth doing if you can complete it before the deadline. 
(Some organizations play games with deadlines in order to compensate for expected overruns, but assume the deadline is accurate in this discussion.)If this is true—and it coincides with my experience—then the estimate for the project isn't as important as whether or not you can finish it before the deadline.To judge whether a project is worth pursuing, gather the project visionary, a seasoned project manager, and a senior programmer or two (preferably ones that would be on the project). 
Ask the visionary to describe the project goals and its deadline, then ask the project manager and programmers if they think it's possible. 
If it is, then you should gather the team and perform some real estimating, release planning, and risk management by conducting the first three or four iterations of the project.This approach takes about four weeks and yields a release date that you can commit to. 
[McConnell 2005] provides additional options that are faster but less reliable.When you estimate well, your velocity is consistent and predictable with each iteration. 
You make commitments and meet them reliably. 
Estimation is fast and easy, and you can estimate most stories in a minute or two.This approach to estimating assumes that programmers are the constraint (see XP Concepts in Chapter 3 for more about the Theory of Constraints). 
It also depends on fixed-length iterations, small stories, and small tasks. 
If these conditions aren't present, you need to use a different approach to estimating.This approach also requires trust: developers need to believe that they can give accurate estimates without being attacked, and customers and stakeholders needto believe the developers are providing honest estimates. 
That trust may not be present at first, but if it doesn't develop, you'll run into trouble.Regardless of your approach to estimating, never use missed estimates to attack developers. 
This is a quick and easy way to destroy trust.There are many approaches to estimating. 
This one has the benefit of being both accurate and simple. 
However, its dependency on release planning for long-term estimates makes it labor intensive for initial project estimates. 
See [McConnell 2005] for other options.Agile Estimating and Planning [Cohn] describes a variety of approaches to agile estimation.Software Estimation: Demystifying the Black Art [McConnell] provides a comprehensive look at traditional approaches to estimation.A limo takes you from your chartered plane to the venue and you and your cooks walk confidently into a kitchen... only to stop in shock. 
The kitchen is a mess: rotting food, unwashed cooking implements, standing water in the sinks. 
It's the morning of the event. 
You have twelve hours.What do you do?  You roll up your sleeves and start cleaning. 
As soon as the first space is clear, a few cooks begin the longest, most complicated food preparation. 
A few more head to the market to get fresh ingredients. 
The rest keep cleaning. 
Working around the mess will take too long.It's a lesson you've learned from software over and over again.Software development requires the cooperation of everyone on the team. 
Programmers are often called "developers", but in reality everyone on the team is part of the development effort. 
When you share the work, customers identify the next requirements while programmers work on the current ones. 
Testers help the team figure out how to stop introducing bugs. 
Programmers spread the cost of technical infrastructure over the entire life of the project. 
Above all, everyone helps to keep everything clean.The best way I know of to reduce the cost of writing software is to improve the internal quality of its code and design. 
I've never seen high quality on a well-managed project fail to repay its investment. 
It always reduces the cost of development in the short term as well as the long term. 
On a successful XP project, There's an amazing feeling—the feeling of being absolutely safe to change absolutely anything without worry.Here are nine practices keep the code clean and allow the entire team to contribute to development:A team using an up-front requirements phase keeps their requirements in a requirements document. 
An XP team doesn't have a requirements phase and story cards aren't miniature requirements documents, so where do requirements come from?In XP, the on-site customers sit with the team. 
They're expected to have all of the information about requirements at their fingertips. 
When somebody needs to know something about the requirements for the project, she asks one of the on-site customers rather than looking up the information in a document.Face-to-face communication is much more effective than written communication, as [Cockburn] discusses, and it allows XP to save time by eliminate a long requirements analysis phase. 
However, requirements work is still necessary. 
The on-site customers need to understand the requirements for the software before they can explain it.The key to successful requirements analysis in XP is expert customers. 
Involve real customers, an experienced product manager, and experts in your problem domain (see The XP Team in Chapter 3 and Real Customer Involvement in Chapter 6). 
Many of the requirements for your software will be intuitively obvious to the right customers.Some requirements will require even expert customers to consider a variety of options or do some research before making a decision. 
Customers, you can and should include other team members in your discussions if it helps clarify your options. 
For example, you may wish to include a programmer in your discussion of user interface options so you can strike a balance between an impressive UI and low implementation cost.Write down any requirements you might forget. 
These notes are primarily for your use as customers so you can easily answer questions in the future and to remind you of the decisions you made. 
They don't need to be detailed or formal requirements documents; keep them simple and short. 
When creating screen mock-ups, for example, I often prefer to create a sketch on a whiteboard and take a digital photo. 
I can create and photograph a whiteboard sketch in a fraction of the time it me to make a mock-up in an electronic tool.Work on requirements incrementally, in parallel with the rest of the team's work. 
This makes your work easier and ensures that the rest of the team won't have to wait to get started. 
Your work will typically parallel your release planning horizons, discussed in Release Planning in Chapter 8.Start by clarifying your project vision, then identify features and stories as described in Release Planning in Chapter 8. 
These initial ideas will guide the rest of your requirements work.Figure out what a story means to you and how you'll know it's finished slightly before you ask programmers to estimate it. 
As they estimate, programmers will ask questions about your expectations; try to anticipate those questions and have answers ready. 
(Over time, you'll learn what sorts of questions your programmers will ask.)  A rough sketch of the visible aspects of the story might help.Figure out the details for each story just before programmers start implementing it. 
Create rough mock-ups that show what you expect the work to look like when it's done. 
Prepare customer tests that provide examples of tricky domain concepts and describe what "done done" means for each story. 
You'll typically wait for the corresponding iteration begins to do most of this work.While stories are under development, before they're "done done," review each story to make sure it works as you expected. 
You don't need to exhaustively test the application—you can rely on the programmers to test their work—but you should check those areas where programmers might think differently than you do. 
These areas include terminology, screen layout, and interactions between screen elements.Some of your findings will reveal errors due to miscommunication or misunderstanding. 
Others, while meeting your requirements, won't work as well in practice as you had hoped. 
In either case, the solution is the same: talk with the programmers about making changes. 
You can even pair with programmers as they work on the fixes.Many changes will be minor and the programmers will be able to fix them as part of their iteration slack. 
If there are major changes, however, the programmers may not have time to fix them in the current iteration. 
(This can happen even when the change seems minor from the customer's perspective.)  Create story cards for these changes. 
Before scheduling such a story into your release plan, consider whether the value of the change is worth its cost.Over time, programmers will learn about your expectations for the application. 
Expect the number of issues you discover to decline each iteration.1See Exploratory Testing later in this chapter.Do you have a clear, compelling vision?  If so, your customers should know where to start. 
If you don't, you may not have the right customers on your team. 
In this case, you can use traditional requirements gathering techniques (see "Further Reading" at the end of this section) to determine the software's requirements, but you're better off involving real experts (see The XP Team in Chapter 3 and Real Customer Involvement in Chapter 6).This is most likely to happen at the beginning of the project, before programmers have learned what the customers like. 
If this happens to you, spend more time with programmers so that your perspective is captured sooner and more accurately. 
In some cases, customers should pair with programmers as they work on error-prone areas.Things that can seem nitpicky to programmers—such as the color of the screen background, or a few pixels of alignment in the UI—represent polish and professionalism to customers. 
This goes both ways: some things that seem important to programmers, such as quality code and refactoring, often seem like unnecessary perfectionism to customers.Rather than getting upset about these differences of perspective, try to learn what your customers care about and why. 
As you learn, you will anticipate your customers' needs better, which will reduce the need to make changes.When customers work out requirements incrementally, programmers are able to work on established stories while customers figure out the details for future stories. 
Customers have ready answers to requirements questions, allowing estimation and iteration planning to proceed quickly and smoothly. 
By the time a story is "done done," it reflects the customers' expectations, and customers experience no unpleasant surprises.In order to incrementally define requirements, the team must include on-site customers who are dedicated to working out requirements details throughout the entire project. 
Without this dedicated resource, your team will struggle with insufficient and unclear requirements.When performing customer reviews, think of them as tools for conveying the customers' perspective rather than as bug-hunting sessions. 
The programmers should be able to produce code that's nearly bug-free (see No Bugs in Chapter 7); the purpose of the review is to bring customers' expectations and programmers' work into alignment.The traditional approach to requirements definition is to perform requirements analysis sessions and document the results. 
This requires an up-front requirements gathering phase, which takes extra time and introduces communication errors.You can use an up-front analysis phase with XP, but good on-site customers should make that unnecessary.Software Requirements [Wiegers 1999] is a good resource for classic approaches to requirements gathering.The following text is excerpted from The Art of Agile Development by James Shore and Shane Warden, published by O'Reilly. Copyright &copy 2008 the authors. All rights reserved.We implement tricky domain concepts correctly.Customers have specialized expertise, or domain knowledge, that programmers don't have. 
Some areas of the application—what programmers call domain rules—require this expertise. 
You need to make sure that the programmers understand the domain rules well enough to code them properly in the application. 
Customer tests help customers communicate their expertise.Don't worry; this isn't as complicated as it sounds. 
Customer tests are really just examples. 
Your programmers turn them into automated tests, which they then use to check that they've implemented the domain  rules correctly. 
Once the tests are passing, the programmers will include them in their ten-minute build, which will inform the programmers if they ever do anything to break the tests.To create customer tests, follow the Describe, Demonstrate, Develop process outlined in the next section. 
Use this process during the iteration in which you develop the corresponding stories.At the beginning of the iteration, look at your stories and decide whether there are any aspects that programmers might misunderstand. 
You don't need to provide examples for everything. 
Customer tests are for communication, not for proving that the software works. 
(See No Bugs in Chapter 7).For example, if one of your stories is "Allow invoice deleting", you don't need to explain how invoices are deleted. 
Programmers understand what it means to delete something. 
However, you might need examples that show when it's okay to delete an invoice, especially if there are complicated rules to ensure that invoices aren't deleted inappropriately.Once you've identified potential misunderstandings, gather the team at a whiteboard and summarize the story in question. 
Briefly describe how the story should work and the rules you're going to provide examples for. 
It's okay to take questions, but don't get stuck on this step.For example, if you decided to discuss invoice deletion, you might say:After a brief discussion of the rules, provide concrete examples that illustrate the scenario. 
Tables are often the most natural way to describe this information, but you don't need to worry about formatting. 
Just get the examples on the whiteboard.Your discussion probably won't be as smooth and clean as this example. 
As you discuss business rules, you'll jump back and forth between describing the rules and demonstrating them with examples. 
You'll probably discover special cases that you hadn't considered. 
In some cases, you might even discover whole new categories of rules that you need customer tests for.One particularly effective way to work is to elaborate on a theme. 
Start by discussing the most basic case and providing a few examples. 
Next, describe a special case or additional detail and provide a few more examples. 
Continue in this way, working from simplest to most complicated, until you have described all aspects of the rule.You don't need to show all possible examples. 
Remember, the purpose here is to communicate, not to exhaustively test the application. 
You only need enough examples to show the differences in the rules. 
A handful of examples per case is usually enough, and sometimes just one or two is sufficient.When you've covered enough ground, document your discussion so the programmers can start working on implementing your rules. 
This is also a good time to evaluate whether the examples are in a format that works well for automated testing. 
If not, discuss alternatives with the programmers.Don't formalize your examples too soon. 
While you're brainstorming, it's often easiest to work on the whiteboard. 
Wait until you've worked out all the examples around a particular business rule (or part of a business rule) before formalizing it. 
This will help you focus on the business rule rather than formatting details.In some cases, you may discover that you have more examples and rules to discuss than you realized. 
The act of creating specific examples often reveals scenarios you hadn't considered. 
Testers are particularly good at finding these. 
If you have a lot of issues to discuss, consider letting some or all of the programmers get started on the examples you have while you figure out the rest of the details.Programmers, once you have some examples, you can start implementing the code using normal test-driven development. 
Don't use the customers' tests as a substitute for writing your own tests. 
Although it's possible to drive your development with customer tests—in fact, this can feel quite natural and productive—the tests don't provide the fine-grained support that TDD does. 
Over time, you'll discover holes in your implementation and regression suite. 
Instead, pick a business rule, implement it with TDD, then confirm that the associated customer tests pass.One of the most common mistakes in creating customer tests is describing what happens in the user interface rather than providing examples of business rules. 
For example, to show that an account rep must not delete a mailed invoice, you might make the mistake of writing this:What happened to the core idea?  It's too hard to see. 
Compare that to the previous approach:When an invoice has been emailed, an account rep may not delete it... or, as you might draw it on the whiteboard:Good examples focus on the essence of your rules. 
Rather than imagining how those rules might work in the application, just think about what the rules are. 
If you weren't creating an application at all, how would you describe those rules to a colleague?  Talk about things rather than actions. 
Sometimes it helps to think in terms of a template: "When (scenario X), then (scenario Y)."It takes a bit of practice to think this way, but the results are worth it. 
The tests become more compact, easier to maintain, and (when implemented correctly) faster to run.Team members, watch out for a common pitfall in customer testing: no customers!  Some teams have programmers and testers do all the work of customer testing, and some teams don't involve their customer at all. 
In others, a customer is present only as a mute observer. 
Don't forget the "customer" in "customer tests."  The purpose of these activities to bring the customers' knowledge and perspective to the team's work. 
If programmers or testers take the reins, you've lost that benefit and missed the point.In some cases, customers may not be willing to take the lead. 
Programmers and testers may be able to solve this problem by asking the customers for their help. 
When programmers need domain expertise, they can ask customers to join the team as they discuss examples. 
One particularly effective technique is to ask for an explanation of a business rule, pretend to be confused, then hand a customer the whiteboard marker and ask him to draw an example on the board.Programmers may use any tool they like to turn the customers' examples into automated tests. 
Ward Cunningham's Fit (Framework for Integrated Test)1, is specifically designed for this purpose. 
It allows you to use HTML to mix descriptions and tables, just as in my invoice auditing example, then runs the tables through programmer-created fixtures to execute the tests.1Available free at http://fit.c2.com/.Fit is a great tool for customer tests because it allows customers to review, run, and even expand on their own tests. 
Although programmers have to write the fixtures, customers can easily add to or modify existing tables to check an idea. 
Testers can also modify the tests as an aid to exploratory testing. 
Because the tests are written in HTML, they can use any HTML editor to modify the tests, including Microsoft Word.Programmers, don't make Fit too complicated. 
It's a deceptively simple tool. 
Your fixtures should work like unit tests, focusing on just a few domain objects. 
For example, the invoice auditing example would use a custom ColumnFixture. 
Each column in the table corresponds to a variable or method in the fixture. 
The code is almost trivial (see Example 9-1).Example 9-1. Example fixture (C#)Using Fit in this way requires a ubiquitous language and good design. 
A dedicated domain layer with Whole Value objects2 works best. 
Without it, you may have to write end-to-end tests, with all the challenges that entails. 
If you have trouble using Fit, talk to your mentor about whether your design needs work.2See Domain-Driven Design [Evans] for a discussion of domain layers and http://c2.com/ppr/checks.html#1 [Cunningham] for information about Whole Value.Once the tests are passing, make them a standard part of your ten-minute build. 
Like programmers' tests, you should fix them immediately if they ever break.Absolutely!  Often, the tests will continue to pass. 
That's good news; leave the new scenario in place to act as documentation for future readers. 
If the new test doesn't pass, talk with the programmers about whether they can fix it with iteration slack or whether you need a new story.Automated acceptance tests tend to be brittle and slow. 
I've replaced acceptance tests with customer reviews (see "Customer Reviews" later in this chapter) and a variety of other techniques (see "A Little Lie" in Chapter 3).When you use customer tests well, you reduce the number of mistakes in your domain logic. 
You discuss domain rules in concrete, unambiguous terms and often discover special cases that you hadn't considered. 
The examples influence the design of the code and help promote a ubiquitous language. 
When written well, the customer tests run quickly and require no more maintenance than unit tests do.Don't use customer tests as a substitute for test-driven development. 
Customer tests are a tool to help communicate challenging business rules, not a comprehensive automated testing tool. 
In particular, Fit doesn't work well as a test scripting tool—it doesn't have variables, loops, or subroutines. 
(Some people have attempted to add these things to Fit, but it's not pretty.)  Real programming tools, such as xUnit or Watir, are better for test scripting.In addition, customer tests require domain experts. 
The real value of the process is the conversation that explores and exposes the customers' business requirements and domain knowledge. 
If your customers are unavailable, those conversations won't happen.Finally, because Fit tests are written in HTML, Fit carries more of a maintenance burden than xUnit frameworks do. 
Automated refactorings won't extend to your Fit tests. 
To keep your maintenance costs down, avoiding creating customer tests for every business rule. 
Focus on the tests that will help improve programmer understanding, and avoid further maintenance costs by refactoring your customer tests regularly. 
Similar stories will have similar tests: consolidate your tests whenever you have the opportunity.Some teams have testers, not customers, write customer tests. 
Although this introduces another barrier between the customers' knowledge and the programmers' code, I have seen it succeed. 
It may be your best choice when customers aren't readily available.Customer tests don't have to use Fit or FitNesse. 
Theoretically, you can write them in any testing tool, including xUnit, although I haven't seen anybody do this.Fit for Developing Software [Mugridge & Cunningham] is the definitive reference for Fit."Agile Requirements" [Shore 2005a], online at http://www.jamesshore.com/Blog/Agile-Requirements.html, is a series of essays about agile requirements, customer testing, and Fit."What programming languages really need is a 'DWIM' instruction," the joke goes. 
"Do what I mean, not what I say."Programming is demanding. 
It requires perfection, consistently, for months and years of effort. 
At best, mistakes lead to code that won't compile. 
At worst, they lead to bugs that lie in wait and pounce at the moment that does the most damage.People aren't so good at perfection. 
No wonder, then, that software is buggy.Wouldn't it be cool if there was a tool that alerted you to programming mistakes moments after you made them—a tool so powerful that it virtually eliminated the need for debugging?There is such a tool—or rather, a technique. 
It's test-driven development, and it actually delivers these results.Test-driven development, or TDD, is a rapid cycle of testing, coding, and refactoring. 
When adding a feature, a pair may perform dozens of these cycles, implementing and refining the software in baby steps until there is nothing left to add and nothing left to take away. 
Research shows that TDD substantially reduces the incidence of defects [Janzen & Saiedian]. 
When used properly, it also helps improve your design, documents your public interfaces, and guards against future mistakes.TDD isn't perfect, of course. 
(Is anything?)  TDD is difficult to use on legacy codebases. 
Even with green-field systems, it takes a few months of steady use to overcome the learning curve. 
Try it anyway—although TDD benefits from other XP practices, it doesn't require them. 
You can use it on almost any project.Back in the days of punchcards, programmers laboriously hand-checked their code to make sure it would compile. 
A compile error could lead to failed batch jobs and intense debugging sessions to look for the misplaced character.Getting code to compile isn't such a big deal anymore. 
Most IDEs check your syntax as you type, and some even compile every time you save. 
The feedback loop is so fast that errors are easy to find and fix. 
If something doesn't compile, there isn't much code to check.Test-driven development applies the same principle to programmer intent. 
Just as modern compilers provide more feedback on the syntax of your code, TDD cranks up the feedback on the execution of your code. 
Every few minutes—as often as every 20 or 30 seconds—TDD verifies that the code does what you think it should do. 
If something goes wrong, there are only a few lines of code to check. 
Mistakes are easy to find and fix.TDD uses an approach similar to double-entry bookkeeping. 
You communicate your intentions twice, stating the same idea in different ways: first with a test, then with production code. 
When they match, it's likely they were both coded correctly. 
If they don't, there's a mistake somewhere.In TDD, the tests are written from the perspective of a class's public interface. 
They focus on the class' behavior, not its implementation. 
Programmers write each test before the corresponding production code. 
This focuses their attention on creating interfaces that are easy to use rather than easy to implement, which improves the design of the interface.After TDD is finished, the tests remain. 
They're checked in with the rest of the code, and they act as living documentation of the code. 
More importantly, programmers run all of the tests with (nearly) every build, ensuring that code continues to work as originally intended. 
If someone accidentally changes the code's behavior—for example, with a misguided refactoring—the tests fail, signalling the mistake.You can start using TDD today. 
It's one of those things that takes moments to learn and a lifetime to master.Imagine TDD as a small, fast-spinning motor. 
It operates in a very short cycle that repeats over and over again. 
Every few minutes, this cycle ratchets your code forward a notch, providing code that—although it may not be finished—has been tested, designed, coded, and is ready to check in.To use TDD, follow the "red, green, refactor" cycle illustrated in Figure. 
With experience, unless you're doing a lot of refactoring, each cycle will take fewer than five minutes. 
Repeat the cycle until your work is finished. 
You can stop and integrate whenever all your tests pass, which should be every few minutes.TDD uses small tests to force you to write your code—you only write enough code to make the tests pass. 
The XP saying is, "Don't write any production code unless you have a failing test."Your first step, therefore, is to engage in an a rather odd thought process. 
Imagine what behavior you want your code to have, then think of a small increment that will require fewer than five lines of code. 
Next, think of a test—also a few lines of code—that will fail unless that behavior is present.In other words, think of a test that will force you to add the next few lines of production code. 
This is the hardest part of TDD because the concept of tests driving your code seems backwards, and because it can be difficult to think in small increments.Pair programming helps. 
While the driver tries to make the current test pass, the navigator should stay a few steps ahead, thinking of tests that will drive the code to the next increment.Now write the test. 
Write only enough code for the current increment of behavior—typically fewer than five lines of code. 
If it takes more, that's okay, just try for a smaller increment next time.Code in terms of the class behavior and its public interface, not how you think you will implement the internals of the class. 
Respect encapsulation. 
In the first few tests, this often means that you write your test to use method and class names that don't exist yet. 
This is intentional—it forces you to design your class's interface from the perspective of a user of the class, not as its implementer.After the test is coded, run your entire suite of tests and watch the new test fail. 
In most TDD testing tools, this will result in a red progress bar.This is your first opportunity to compare your intent with what's actually happening. 
If the test doesn't fail, or if it fails in a different way than you expected, something is wrong. 
Perhaps your test is broken, or it doesn't test what you thought it did. 
Troubleshoot the problem; you should always be able to predict what's happening with the code.Next, write just enough production code to get the test to pass. 
Again, you should usually need less than five lines of code. 
Don't worry about design purity or conceptual elegance—just do what you need to do to make the test pass. 
Sometimes you can just hardcode the answer. 
This is okay because you'll be refactoring in a moment.Run your tests again, and watch all the tests pass. 
This will result in a green progress bar.This is your second opportunity to compare your intent with reality. 
If the test fails, get back to known-good code as quickly as you can. 
Often, you or your pairing partner can see the problem by taking a second look at the code you just wrote. 
If you can't see the problem, consider erasing the new code and trying again. 
Sometimes it's best to delete the new test (it's only a few lines of code, after all) and start the cycle over with a smaller increment.With all your tests passing again, you can now refactor without worrying about breaking anything. 
Review the code and look for possible improvements. 
Ask your navigator if he's made any notes.For each problem you see, refactor the code to fix it. 
Work in a series of very small refactorings—a minute or two each, certainly not longer than five minutes—and run the tests after each one. 
They should always pass. 
As before, if the test doesn't pass and the answer isn't immediately obvious, undo the refactoring and get back to known-good code.Refactor as many times as you like. 
Make your design as good as you can, but limit it to the code's existing behavior. 
Don't anticipate future needs, and certainly don't add any behavior. 
Remember, refactorings aren't supposed to change behavior. 
New behavior requires a failing test.When you're ready to add new behavior, start the cycle over again.Each time you finish the TDD cycle, you add a tiny bit of well-tested, well-designed code. 
The key to success with TDD is small increments. 
Typically, you'll run through several cycles very quickly, then spend more time on refactoring for a cycle or two, then speed up again.With practice, you can finish more than twenty cycles in an hour. 
Don't focus too much on how fast you go, though. 
That might tempt you to skip refactoring and design, which is too important to skip. 
Instead, take very small steps, run the tests frequently, and minimize the time you spend with a red bar.I recently recorded how I used TDD to solve a sample problem. 
The increments are very small—they may even seem ridiculously small—but this makes finding mistakes trivially easy, and that helps me go faster.As you read, keep in mind that it takes far longer to explain an example like this than to program it. 
I completed each step in a matter of seconds.Imagine that you need to program a Java class to parse an HTTP query string.1  You've decided to use TDD to do so.1Pretend you're in an alternate reality without a gazillion libraries that already do this.Step 1: Think. 
The first step is to imagine the features you want the code to have. 
My first thought was, "I need my class to separate name/value pairs into a HashMap."  Unfortunately, this would take more than five lines to code, so I needed to think of a smaller increment.Often, the best way to make the increments smaller is to start with seemingly trivial cases. 
"I need my class to put one name/value pair into a HashMap," I thought, which sounded like it would do the trick.Step 2: Red Bar. 
The next step is to write the test. 
Remember, this is partially an exercise in interface design. 
In this example, my first temptation was to call the class QueryStringParser, but that's not very object-oriented. 
I settled on QueryString.As I wrote the test, I realized that a class named QueryString wouldn't return a HashMap; it would encapsulate the HashMap. 
It would provide a method such as valueFor(name) to access the name-value pairs.Building that seemed like it would require too much code for one increment, so I decided to have this test to drive the creation of a count() method instead. 
I decided that the count() method would return the total number of name/value pairs. 
My test checked that it would work when there was just one pair.The code didn't compile, so I wrote a do-nothing QueryString class and count() method.That gave me the red bar I expected.Green Bar. 
To make this test pass, I hardcoded the right answer. 
I could have programmed a better solution, but I wasn't sure how I wanted the code to work. 
Would it count the number of equal signs?  Not all query strings have equal signs. 
I decided to punt.Green bar.Step 4: Refactor. 
I didn't like the QueryString name, but I had another test in mind and I was eager to get to it. 
I made a note to fix the name on an index card—perhaps HttpQuery would be better. 
I'd see how I felt next time through the cycle.Step 5: Repeat. 
Yup.Think. 
I wanted to force myself to get rid of that hard-coded return 1, but I didn't want to have to deal with multiple query strings yet. 
My next increment would probably be about the valueFor() method, and I wanted to avoid the complexity of multiple queries. 
I decided that testing an empty string would require me to code count() properly without making future increments too difficult.Red Bar. 
New test.Red bar. 
Expected: <0> but was: <1>. 
No surprise there.This inspired two thoughts. 
First, I should test the case of a null argument to the QueryString constructor. 
Second, I was starting to see duplication in my tests that needed refactoring. 
I added both notes to my index card.Green Bar. 
Now I had to stop and think. 
What was the fastest way for me to get back to a green bar?  I decided to check if the query string was blank.Refactor. 
I doublechecked my to-do list. 
I needed to refactor the tests, but I decided to wait for another test to demonstrate the need. 
"Three strikes and you refactor," as the saying goes.It was time to do the cycle again.Think. 
My list included testNull(), which meant I needed to test the case when the query string is null. 
I decided to put that in.Red Bar. 
This test forced me to think about what behavior I wanted when the value was null. 
I've always been a fan of code that fails fast, so I decided that a null value was illegal. 
This meant the code should throw an exception telling callers not to use null values. 
(Simple Design, later in this chapter, discusses failing fast in detail.)Green Bar. 
Piece of cake.Refactor. 
I still needed to refactor my tests, but the new test didn't have enough in common with the old tests to make me feel it was necessary. 
The production code looked okay, too, and there wasn't anything significant on my index card. 
No refactorings this time.Think. 
Okay, now what?  The easy tests were done. 
I decided to put some meat on the class and implement the valueFor() method. 
Given a name of a name/value pair in the query string, this method would return the associated value.As I thought about this test, I realized I also needed a test to show what happens when the name doesn't exist. 
I wrote that on my index card for later.Red Bar. 
To make the tests fail, I added a new assertion at the end of my existing testOneNameValuePair() test.Green Bar. 
This test made me think for a moment. 
Could the split() method work?  I thought it would.This code passed the tests, but it was incomplete. 
What if there were more than one equal sign, or no equal signs?  It needed proper error handling. 
I made a note to add tests for those scenarios.Refactor. 
The names were bugging me. 
I decided that QueryString was okay, if not perfect. 
The qs in the tests was sloppy, so I renamed it query.Think. 
I had a note reminding me to take care of error handling in valueFor(), but I wanted to tackle something more meaty. 
I decided to add a test for multiple name/value pairs.Red Bar. 
When dealing with a variable number of items, I usually test the case of zero items, one item, and three items. 
I had already tested zero and one, so now I tested three.I could have written an assertion for count() rather than valueFor(), but the real meat was in the valueFor() method. 
I made a note to test count() next.Green Bar. 
My initial thought was that the split() technique would work again.Ewww... that felt like a hack—but the test passed!Refactor: The additions to valueFor() felt hackish. 
I took a second look. 
The two issues that bothered me the most were the nameAndValue array and the RuntimeException. 
An attempt to refactor nameAndValue led to worse code, so I backed out the refactoring and decided to leave it alone for another cycle.The RuntimeException was worse; it's better to throw a custom exception. 
In this case, though, the Java convention is to return null rather than throw an exception. 
I already had a note that I should test the case where name isn't found; I revised it to say that the correct behavior was to return null.Reviewing further, I saw that my duplicated test logic had reached three duplications. 
Time to refactor... or so I thought. 
After a second look, I realized that the only duplication between the tests was that I was constructing a QueryString object each time. 
Everything else was different, including QueryString's constructor parameters. 
The tests didn't need refactoring after all. 
I scratched that note off of my list.In fact, the code was looking pretty good... better than I initially thought, at least. 
I'm too hard on myself.Think. 
After reviewing my notes, I realized that I should probably test degenerate uses of the ampersand, such as two ampersands in a row. 
I made a note to add tests for that. 
At the time, though, I wanted to get the count() method working properly with multiple name/value pairs.Red Bar. 
I added the count() assertion to my test. 
It failed as expected.Green Bar. 
To get this test to pass, I stole my existing code from valueFor() and modified it. 
This was blatant duplication, but I planned to refactor as soon as I saw the green bar.I was able to delete more of the copied code than I expected. 
To my surprise, however, it didn't pass!  The test failed in the case of an empty query string: expected: <0> but was: <1>. 
I had forgotten that split() returned the original string when the split character isn't found. 
My code expected it to return an empty array when no split occurred.I added a guard clause that took care of the problem. 
It felt like a hack, so I planned to take a closer look after the tests passed.Refactor. 
This time I definitely needed to refactor. 
The duplication between count() and valueFor() wasn't too strong—it was just one line—but they both parsed the query string, which was a duplication of function if not code. 
I decided to fix it.At first, I wasn't sure how to fix the problem. 
I decided to try to parse the query string into a HashMap, as I had originally considered. 
To keep the refactoring small, I left count() alone at first and just modified valueFor(). 
It was a small change.This code parsed the query string during every call to valueFor(), which wasn't a great idea. 
I had kept the code in valueFor() to keep the refactoring simple. 
Now I wanted to move it out of valueFor() into the constructor. 
This required a sequence of refactorings, described in Refactoring later in this chapter.I reran the tests after each of these refactorings to make sure that I hadn't broken anything... and in fact, one refactoring did break the tests. 
When I called the parser from the constructor, testNoNameValuePairs()—the empty query test—bit me again, causing an exception in the parser. 
I added a guard clause as before, which solved the problem.After all that refactoring, the tests and production code were nice and clean.The class wasn't done—it still needed to handle degenerate uses of the equals and ampersand signs, and it didn't fully implement the query string specification yet, either2. 
In the interest of space, though, I leave the remaining behavior as an exercise for you to complete yourself. 
As you try it, remember to take very small steps and to check for refactorings every time through the cycle.2For example, the semicolon works like the ampersand in query strings.In order to use TDD, you need a testing framework. 
The most popular are the open-source xUnit tools, such as JUnit (for Java) and NUnit (for .NET). 
Although these tools have different authors, they typically share the basic philosophy of Kent Beck's pioneering SUnit.If your platform doesn't have an xUnit tool, you can build your own. 
Although the existing tools often provide GUIs and other fancy features, none of that is necessary. 
All you need is a way to run all of your test methods as a single suite, a few assert methods, and an unambiguous pass or fail result when the test suite is done.As with programming itself, TDD has myriad nuances to master. 
The good news is that the basic steps alone—red, green, refactor—will lead to very good results. 
Over time, you'll fine-tune your approach.One of the nuances of TDD is test speed—not the frequency of each increment, which is also important, but how long it takes to run all of the tests. 
In TDD, you run the tests as often as one or two times every minute. 
They must be fast. 
If they aren't, they'll be a distraction rather than a help. 
You won't run them as frequently, which reduces the primary benefit of TDD: micro-increments of proof.[Nielsen] reports that users lose interest and switch tasks when the computer makes them wait more than ten seconds. 
Computers only seem "fast" when they make users wait less than a second.Although this research explored the area of user interface design, I've found it to be true when running tests as well. 
If they take more than ten seconds, I'm tempted to check my email, surf the web, or otherwise distract myself. 
Then it takes several minutes for me to get back to work. 
To avoid this, make sure your tests take under ten seconds to run. 
Less than a second is even better.An easy way to keep your test times down is to run a subset of tests during the TDD cycle. 
Periodically run the whole test suite to make sure you haven't accidentally broken something, particularly before integrating and during refactorings that affect multiple classes.Running a subset does incur the risk that you'll make a mistake without realizing it, leading to annoying debugging problems later. 
Advanced practitioners design their tests to run quickly. 
This requires that they make trade-offs between three basic types of automated tests:The vast majority of your tests should be unit tests. 
A small fraction should be integration tests, and only a handful should be end-to-end tests.Unit tests focus just on the class or method at hand. 
They run entirely in memory, which makes them very fast. 
Depending on your platform, your testing tool should be able to run at least 100 unit tests per second.[Feathers] provides an excellent definition of a unit test:Tests that do these things are integration tests, not unit tests.Creating unit tests requires good design. 
A highly-coupled system—a big ball of mud, or spaghetti software—makes it difficult to write unit tests. 
If you have trouble doing this, or if Feathers' definition seems impossibly idealistic, it's a sign of problems in your design. 
Look for ways to decouple your code so that each class, or set of related classes, may be tested in isolation. 
See Simple Design later in this chapter for ideas, and consider asking your mentor for help.Unit tests aren't enough. 
At some point, your code has to talk to the outside world. 
You can use TDD for that code, too.A test that causes your code to talk to a database, communicate across the network, touch the file system, or otherwise leave the bounds of its own process is an integration test. 
The best integration tests are focused integration tests that test just one interaction with the outside world.One of the challenges of integration tests is the need to prepare the external dependency to be tested. 
Tests should run exactly the same way every time, regardless of which order you run them in or the state of the machine prior to running them. 
This is easy with unit tests but harder with integration tests. 
If you're testing your ability to select data from a database table, that data needs to be in the database.Make sure each integration test can run entirely on its own. 
It should set up the environment it needs and then restore the previous environment afterwards. 
Be sure to do so even if the test fails or an exception is thrown. 
Nothing is more frustrating than a test that intermittently fails. 
Integration tests that don't set up and tear down their test environment properly are common culprits.You shouldn't need many integration tests. 
The best integration tests have a tight focus; each checks just one aspect of your program's ability to talk to the outside world. 
The number of focused integration tests in your test suite should be proportional to the types of external interactions your program has, not the overall size of the program. 
(In contrast, the number of unit tests you have is proportional to the overall size of the program.)If you need a lot of integration tests, it's a sign of design problems. 
It may mean that the code that talks to the outside world isn't cohesive. 
For example, if all your business objects talk directly to a database, you'll need integration tests for each one. 
A better design would be to have just one class that talks to the database. 
The business objects would talk to that class.3  In this scenario, only the database class would need integration tests. 
The business objects could use ordinary unit tests.3A still better design might involve a persistence layer.In a perfect world, the unit tests and focused integration tests mesh perfectly to give you total confidence in your tests and code. 
You should be able to make any changes you want without fear, comfortable in the knowledge that if you make a mistake, your tests will catch them.How can you be sure that your unit tests and integration tests mesh perfectly?  One way is to write end-to-end tests. 
End-to-end tests exercise large swaths of the system, starting at (or just behind) the user interface, passing through the business layer, touching the database, and returning. 
Acceptance tests and functional tests are common examples of end-to-end tests. 
Some people also call them integration tests, although I reserve that term for focused integration tests.End-to-end tests can give you more confidence in your code, but they suffer from many problems. 
They're difficult to create because they require error-prone and labor-intensive setup and teardown procedures. 
They're brittle and tend to break whenever any part of the system or its setup data changes. 
They're very slow—they run in seconds or even minutes per test, rather than multiple tests per second. 
They provide a false sense of security, by exercising so many branches in the code that it's difficult to say which parts of the code are actually covered.Instead of end-to-end tests, use exploratory testing to check the effectiveness of your unit and integration tests. 
When your exploratory tests find a problem, use that information to improve your approach to unit and integration testing, rather than introducing end-to-end tests.In some cases, limitations in your design may prevent unit and integration tests from testing your code sufficiently. 
This is particularly true when you have legacy code. 
In that case, end-to-end tests are a necessary evil. 
Think of them as technical debt: strive to make them unnecessary, and replace them with unit and integration tests whenever you have the opportunity.[Feathers] says legacy code is "code without tests."  I think of it as "code you're afraid to change."  This is usually the same thing.The challenge of legacy code is that, because it was created without tests, it usually isn't designed for testability. 
In order to introduce tests, you need to change the code. 
In order to change the code with confidence, you need to introduce tests. 
It's this kind of chicken-and-egg problem that makes legacy code so difficult to work with.To make matters worse, legacy code has often accumulated a lot of technical debt. 
(It's hard to remove technical debt when you're afraid to change existing code.)  You may have trouble understanding how everything fits together. 
Methods and functions may have side effects that aren't apparent.One way to approach this problem is to introduce end-to-end smoke tests. 
These tests exercise common usage scenarios involving the component you want to change. 
They aren't sufficient to give you total confidence in your changes, but they at least alert you when you make a big mistake.With the smoke tests in place, you can start introducing unit tests. 
The challenge here is finding isolated components to test, as legacy code is often tightly coupled code. 
Instead, look for ways for your test to strategically interrupt program execution. 
[Feathers] calls these opportunities seams. 
For example, in an object-oriented language, if a method has a dependency you want to avoid, your test can call a test-specific subclass that overrides and stubs out the offending method.Finding and exploiting seams often leads to ugly code. 
It's a case of temporarily making the code worse so you can then make it better. 
Once you've introduced tests, refactor the code to make it test-friendly, then improve your tests so they aren't so ugly. 
Then you can proceed with normal TDD.Adding tests to legacy code is a complex subject that deserves its own book. 
Fortunately, [Feathers]' Working Effectively with Legacy Code is exactly that book.The saying is, "Test everything that can possibly break."  To determine if something could possibly break, I think, "Do I have absolute confidence that I'm doing this correctly, and that nobody in the future will inadvertently break this code?"I've learned through painful experience that I can break nearly anything, so I test nearly everything. 
The only exception is code without any logic, such as simple accessors and mutators (getters and setters), or a method that only calls another method.You don't need to test third-party code unless you have some reason to distrust it.As in my extended QueryString example, start by testing public methods. 
As you refactor, some of that code will move into private methods, but the existing tests will still thoroughly test its behavior.If your code is so complex that you need to test a private method directly, this may be a sign to refactor. 
You may benefit from moving the private methods into their own class and providing a public interface. 
The "Replace Method with Method Object" refactoring [Fowler 1999] (p. 135) may help.TDD is particularly difficult with user interfaces because most UI frameworks weren't designed with testability in mind. 
Many people compromise by writing a very thin, untested translation layer that only forwards UI calls to a presentation layer. 
They keep all of their UI logic in the presentation layer and use TDD on that layer as normal.There are some tools that allow you to test a UI directly, perhaps by making HTTP calls (for web-based software), or by pressing buttons or simulating window events (for client-based software). 
These are essentially integration tests and they suffer similar speed and maintainability challenges as other integration tests. 
Despite the challenges, these tools can be helpful.Yes. 
I do, and everybody should. 
Tests are just code. 
The normal rules of good development apply: avoid duplication, choose good names, factor and design well.I've seen otherwise-fine projects go off of the rails because of brittle and fragile test suites. 
By making TDD a central facet of development, you've committed to maintaining your test code just as much as you've committed to maintaining the rest of the code. 
Take it just as seriously.When you use TDD properly, you find that you spend little time debugging. 
Although you continue to make mistakes, you find those mistakes very quickly and have little difficulty fixing them. 
You have total confidence that the whole codebase is well-tested, which allows you to aggressively refactor at every opportunity, confident in the knowledge that the tests will catch any mistakes.Although TDD is a very valuable tool, it does have a two-or-three month learning curve. 
It's easy to apply to toy problems such as the QueryString example, but translating that experience to larger systems takes time. 
Legacy code, proper unit test isolation, and integration tests are particularly difficult to master. 
On the other hand, the sooner you start using TDD, the sooner you'll figure it out, so don't let these challenges stop you.Be careful when applying TDD without permission. 
Learning TDD could slow you down temporarily. 
This could backfire and cause your organization to reject TDD without proper consideration. 
I've found that combining testing time with development time when providing estimates helps alleviate pushback for dedicated developer testing.Also be cautious about being the only one to use TDD on your team. 
You may find that your teammates break your tests and don't fix them. 
It's better to get the whole team to agree to try it together.TDD is the heart of XP's programming practices. 
Without it, all of XP's other technical practices will be much harder to use.A common misinterpretation of TDD is to design your entire class first, then write all of its test methods, then write the code. 
This approach is frustrating and slow, and it doesn't allow you to learn as you go.Another misguided approach is to write your tests after you write your production code. 
This is very difficult to do well—production code must be designed for testability, and it's hard to do so unless you write the tests first. 
It doesn't help that writing tests after the fact is boring. 
In practice, the temptation to move on to the next task usually overwhelms the desire for well-tested code.Although you can use these alternatives to introduce tests to your code, TDD isn't just about testing. 
It's really about using very small increments to produce high-quality, known-good code. 
I'm not aware of any alternatives that provide TDD's ability to catch and fix mistakes quickly.Test-driven development is one of the most heavily-explored aspects of Extreme Programming. 
There are several excellent books on various aspects of TDD. 
Most are focused on Java and JUnit, but their ideas are applicable to other languages as well.Test-Driven Development: By Example [Beck 2002] is a good introduction to TDD. 
If you liked the QueryString example, you'll like the extended examples in this book. 
The TDD patterns in Part III are particularly good.Test-Driven Development: A Practical Guide [Astels] provides a larger example that covers a complete project. 
Reviewers praise its inclusion of UI testing.JUnit Recipes [Rainsberger] is a comprehensive book discussing a wide variety of testing problems, including a thorough discussion of testing J2EE.Working Effectively with Legacy Code [Feathers] is a must-have for anybody working with legacy code.Entropy always wins. 
Eventually, chaos turns your beautifully imagined and well-designed code into a big mess of spaghetti.At least, that's the way it used to be, before refactoring. 
Refactoring is the process of changing the design of your code without changing its behavior—what it does stays the same, but how it does it changes. 
Refactorings are also reversible; sometimes one form is better than another for certain cases. 
Just as you can change the expression x2 - 1 to (x + 1)(x - 1) and back, you can change the design of your code—and once you can do that, you can keep entropy at bay.Refactoring enables an approach to design I call reflective design. 
In addition to creating a design and coding it, you can now analyze the design of existing code and improve it. 
One of the best ways to identify improvements is with code smells: condensed nuggets of wisdom that help you identify common problems in design.Code smells don't necessarily mean that there's a problem with the design. 
It's like a funky smell in the kitchen: it could indicate that it's time to take out the garbage, or it could just mean that Uncle Gabriel is cooking with a particularly pungent cheese. 
Either way, when you smell something funny, take a closer look.[Fowler 1999], writing with Kent Beck, has an excellent discussion of code smells. 
It's well worth reading. 
Here are a summary of the smells I find most often, along with smells that Fowler and Beck didn't mention.11Wannabee Static, Time Dependency, Half-Baked Objects, and Coddling Nulls are new in this book.These two smells help you identify cohesion problems in your code. 
Divergent Change occurs when unrelated changes affect the same class. 
It's an indication that your class involves too many concepts. 
Split it, and give each concept its own home.Shotgun Surgery is just the opposite: it occurs when you have to modify multiple classes to support changes to a single idea. 
This indicates that the concept is represented in many places throughout your code. 
Find or create a single home for the idea.Some implementations represent high-level design concepts with primitive types. 
For example, a decimal might represent dollars. 
This is the Primitive Obsession code smell. 
Fix it by encapsulating the concept in a class.Data Clumps are similar. 
They occur when several primitives represent a concept as a group. 
For example, several strings might represent an address. 
Rather than being encapsulated in a single class, however, the data just clumps together. 
When you see batches of variables consistently passed around together, you're probably facing a Data Clump. 
As with Primitive Obsession, encapsulate the concept in a single class.One of the most common mistakes I see in object-oriented design is when programmers put their data and code in separate classes. 
This often leads to duplicate data-manipulation code. 
When you have a class that's little more than instance variables combined with accessors and mutators (getters and setters), you have a Data Class. 
Similarly, when you have a class that contains methods but no meaningful per-object state, you have a Wannabee Static Class.Ironically, one of the primary strengths of object-oriented programming is its ability to combine data with the code that operates on that data. 
Data Classes and Wannabee Statics are twins separated at birth. 
Reunite them by combining instance variables with the methods that operate on those variables.Null references pose a particular challenge to programmers: they're occasionally useful, but they most often indicate invalid states or other error conditions. 
Programmers are usually unsure of what to do when they receive a null reference; their methods often check for null references and then return null themselves.Coddling Nulls like this leads to complex methods and error-prone software. 
Errors suppressed with null cascade deeper into the application, causing unpredictable failures later in the execution of the software. 
Sometimes the null makes it into the database, leading to recurring application failures.Instead of Coddling Nulls, adopt a fail fast strategy (see "Simple Design" later in this chapter). 
Don't allow null as a parameter to any method, constructor, or property unless it has explicitly-defined semantics. 
Throw exceptions to signify errors rather than returning null. 
Make sure that any unexpected null reference will cause the code to throw an exception, either as a result of being dereferenced or by hitting an explicit assertion.Time Dependencies occur when a class' methods must be called in a specific order. 
Half-Baked Objects are a special case of Time Dependency: they must first be constructed, then initialized with a method call, then used.Time Dependencies often indicate an encapsulation problem. 
Rather than managing its state itself, the class expects its callers to manage some of its state. 
This results in bugs and duplicate code in callers. 
Look for ways to encapsulate the class' state more effectively. 
In some cases, you may find that your class has too many responsibilities and would benefit from being split into multiple classes.Reflective design requires that you understand the design of existing code. 
The easiest way to do so is to ask someone else on the team. 
A conversation around a whiteboard design sketch is a great way to learn.In some cases, no one on the team will understand the design, or you may wish to dive into the code yourself. 
When you do, focus on the responsibilities and interactions of each major component. 
What is the purpose of this package or namespace?  What does this class represent?  How does it interact with other packages, namespaces, and classes?For example, NUnitAsp is a tool for unit testing ASP.NET code-behind logic. 
One of its classes is HttpClient, which you might infer makes calls to an HTTP (web) server—presumably an ASP.NET web server.To confirm that assumption, look at the names of the class's methods and instance variables. 
HttpClient has methods named GetPage, FollowLink, SubmitForm, and HasCookie, along with some USER_AGENT constants and several related methods and properties. 
In total, it seems pretty clear that HttpClient emulates a web browser.Now expand that understanding to related elements. 
Which classes does this class depend on?  Which classes depend on this one?  What are their responsibilities?  As you learn, diagram your growing understanding on a whiteboard.Creating a UML sequence diagram2 can be helpful for understanding how individual methods interact with the rest of the system. 
Start with a method you want to understand and look at each line in turn, recursively adding each called method to your sequence diagram. 
This is fairly time-consuming, so I only recommend it if you're confused about how or why a method works.2[Fowler & Scott] is a good resource for learning more about UML.Although these techniques are useful for understanding the design of unfamiliar code, you shouldn't need them often. 
You should already have a good understanding of most parts of your software, or be able to talk to someone on the team who does. 
Unless you're dealing with a lot of legacy code, you should rarely have trouble understanding the design of existing code: your team wrote it, after all. 
If you find yourself needing these techniques often, something is wrong—ask your mentor for help.Reflective design helps you understand what to change; refactoring gives you the ability to make those changes. 
When you refactor, proceed in a series of small transformations. 
(Confusingly, each type of transformation is also called a refactoring.)  Each refactoring is like making a turn on a Rubik's cube. 
To achieve anything significant, you have to string together several individual refactorings, just as you have to string together several turns to solve the cube.The fact that refactoring is a sequence of small transformations sometimes gets lost on people new to refactoring. 
Refactoring isn't rewriting. 
You don't just change the design of your code; to refactor well, you need to make that change in a series of controlled steps. 
Each step should only take a few moments, and your tests should pass after each one.There are a wide variety of individual refactorings. 
[Fowler 1999]'s Refactoring is the classic work on the subject. 
It contains an in-depth catalog of refactorings, and is well worth studying—I learned more about good code from reading that book than from any other.You don't need to memorize all the individual refactorings. 
Instead, try to learn the mindset behind the refactorings. 
Work from the book in the beginning. 
Over time, you'll learn how to refactor intuitively, creating each refactoring as you need it.Any significant design change requires a sequence of refactorings. 
Learning how to change your design through a series of small refactorings is a valuable skill. 
 Once you've mastered it, you can make dramatic design changes without breaking anything. 
You can even do so in pieces, fixing part of the design one day and the rest of it another day.To illustrate this point, I'll show each step in the simple refactoring from my TDD example (see the TDD example in Test-Driven Development earlier in this chapter). 
Note how small each step is. 
Working in small steps enables you to remain in control of the code, preventing confusion, and allows you to work very quickly.The purpose of this example was to create an HTTP query string parser. 
At this point, I had a working, bare-bones parser. 
Here are the tests:The code worked—it passed all the tests—but it was ugly. 
Both the count() and valueFor() methods had duplicate parsing code. 
I wanted to eliminate this duplication and put parsing in just one place.To eliminate the duplication, I needed a single method that could do all the parsing. 
The other methods would work with the results of the parse rather than doing any parsing of their own. 
I decided that this parser, called from the constructor, would parse the data into a HashMap.Although I could have done this in one giant step by moving the body of valueFor() into a parseQueryString() method and then hacking until the tests passed again, I knew from hard-won experience that it was faster to proceed in small steps.My first step was to introduce HashMap() into valueFor(). 
This would make valueFor() look just like the parseQueryString() method I needed. 
Once it did, I could extract out parseQueryString() easily.After making this refactoring, I ran the tests. 
They passed.Now I could extract the parsing logic into its own method. 
I used my IDE's built-in Extract Method refactoring to do so.The tests passed again, of course. 
With such small steps, I'd be surprised if they didn't. 
That's the point; by taking small steps, I remain in complete control of my changes, which reduces surprises.I now had a parseQueryString() method, but it was only available to valueFor(). 
My next step was to make it available to all methods. 
To do so, I created a _map instance variable and had parseQueryString() use it.This is a trickier refactoring than it seems. 
Whenever you switch from a local variable to an instance variable, the order of operations can get confused. 
That's why I continued to have parseQueryString() return _map, even though it was now available as an instance variable. 
I wanted to make sure this first step passed its tests before proceeding to my next step, which was to get rid of the unnecessary return.The tests passed again.Because parseQueryString() stood entirely on its own, its only relationship to valueFor() was that it had to be called before valueFor()'s return statement. 
I was finally ready to achieve my goal of calling parseQueryString() from the constructor.This seemed like a simple refactoring. 
After all, I only moved one line of code. 
Yet when I ran my tests, they failed. 
My parse method didn't work with an empty string—a degenerate case that I hadn't yet implemented in valueFor(). 
 It wasn't a problem as long as only valueFor() ever called parseQueryString(), but it showed up now that I called parseQueryString() in the constructor.The problem was easy to fix with a guard clause.At this point, I was nearly done. 
The duplicate parsing in the count() method caused all of this mess, and I was ready to refactor it to use the _map variable rather than do its own parsing. 
It went from:... to:I love it when I can delete code.I reviewed the code and saw just one loose end remaining: the _query instance variable that stored the unparsed query string. 
I no longer needed it anywhere but parseQueryString(), so I demoted it from a instance variable to a parseQueryString() parameter.When you compare the initial code to this code, it seems like a big change. 
Yet this change took place as a series of small, careful refactorings. 
Although it took me a long time to describe the steps, each individual refactoring took a matter of seconds in practice. 
The whole series occurred in a few minutes.Constantly. 
Perform little refactorings as you use TDD and bigger refactorings every week. 
Every week, your design should be better than it was the week before. 
(See Incremental Design and Architecture later in this chapter.)If it were possible to design your code perfectly from the beginning, then refactoring would be rework. 
As anybody who's worked with large systems knows, mistakes always creep in. 
It isn't possible to design software perfectly. 
That's why refactoring is important. 
Rather than bemoan the errors in the design, celebrate your ability to fix them.You can refactor databases too. 
Just as with normal refactorings, the trick is to proceed in small, behavior-preserving steps. 
For example, to rename a table, you can create a new table, copy the data from one to the next, update constraints, update stored procedures and applications, then delete the old table.3  See Further Reading for more.3These steps assume that the database isn't live during the refactoring. 
A live refactoring would have a few more steps.Take advantage of communication and continuous integration. 
Before taking on a refactoring that will touch a bunch of code, check in your existing code and let people know what you're about to do. 
Sometimes other pairs can reduce the chance of integration conflicts by mirroring any renaming you're planning to do. 
(IDEs with refactoring support make such renaming painless.)During the refactoring, I like to use the distributed version control system SVK, built atop Subversion. 
It allows me to commit my changes to a local repository one at a time, then push all of them to the main repository when I reach the point of integration. 
This doesn't prevent conflicts with other pairs, but it allows me to checkpoint locally, which reduces my need to disturb anyone else before I finish.It's possible you made a mistake in refactoring, but if not, it could be a sign of poor tests. 
They might test the code's implementation rather than its behavior. 
Undo the refactoring and take a look at the tests; you may need to refactor them.Absolutely!  Too many developers forget to do this and find themselves maintaining tests that are brittle and difficult to change. 
Tests have to be maintained just as much as production code, so they're valid targets for refactoring too.Exercise caution when refactoring tests. 
It's easier to unwittingly break a test than it is to break production code, because you can make the test pass when it shouldn't. 
I like to temporarily change my production code to make the tests fail just to show that they still can. 
Pair programming also helps.Sometimes it's valuable to leave more duplication in your tests than you would in the code itself. 
Tests have a documentation value, and reducing duplication and increasing abstraction can sometimes obscure the intent of the tests. 
This can be a tough judgement call—error on the side of eliminating duplication.When you use refactoring as an everyday part of your toolkit, the code constantly improves. 
You make significant design changes safely and confidently. 
Every week, the code is at least slightly better than it was the week before.Refactoring requires good tests. 
Without it, it's dangerous because you can't easily tell whether your changes have modified behavior. 
When I have to deal with untested legacy code, I often write a few end-to-end tests first to provide a safety net for refactoring.Refactoring also requires collective code ownership. 
Any significant design changes will require that you change all parts of the code. 
Collective code ownership makes this possible. 
Similarly, refactoring requires continuous integration. 
Without it, each integration will be a nightmare of conflicting changes.It's possible to spend too much time refactoring. 
You don't need to refactor code that's unrelated to your present work. 
Similarly, balance your need to deliver stories with the need to have good code. 
As long as the code is better than it was when you started, you're doing enough. 
In particular, if you think the code could be better, but you're not sure how to improve it, it's okay to leave it for someone else to improve later.There is no real alternative to refactoring. 
No matter how carefully you design, all code accumulates technical debt. 
Without refactoring, that technical debt will eventually overwhelm you, leaving you to choose between rewriting the software (at great expense and risk) or abandoning it entirely."Clean Code: Args—A Command-line Argument Parser" [Martin 2005] is a rare treasure: a detailed walkthrough of an extended refactoring. 
If you liked my refactoring example but wanted more, read this article. 
It's online at http://www.objectmentor.com/resources/articles/Clean_Code_Args.pdf.Refactoring: Improving the Design of Existing Code [Fowler 1999] is the definitive reference for refactoring. 
It's also a great read. 
Buy it.Refactoring to Patterns [Kerievsky] takes Fowlers' work one step higher, showing how refactorings can string together to achieve significant design changes. 
It's a good way to learn more about how to use individual refactorings to achieve big results.Refactoring Databases: Evolutionary Database Design [Ambler & Sadalage] shows how refactoring can apply to database schemas.When writing code, agile developers often stop to ask themselves, "What is the simplest thing that could possibly work?"  They seem to be obssessed with simplicity. 
Rather than anticipating changes and providing extensibility hooks and plug-in points, they create a simple design that anticipates as little as possible, as cleanly as possible. 
Unintuitively, this results in designs that are ready for any change, anticipated or not.This may seem silly. 
How can a design be ready for any change?  Isn't the job of a good designer or architect to anticipate future changes and make sure the design can be extended to support them?  Doesn't Design Patterns: Elements of Reusable Software say that the key to maximizing reuse is to anticipate changes and design accordingly?I'll let Erich Gamma, coauthor of Design Patterns, answer these questions. 
In the following excerpt, Gamma is interviewed by Bill Venners.11"Erich Gamma on Flexibility and Reuse: A Conversation with Erich Gamma, Part II", http://www.artima.com/lejava/articles/reuse.html, accessed 6 April 2007.Simple doesn't mean simplistic. 
Don't make boneheaded design decisions in the name of reducing the number of classes and methods. 
A simple design is clean and elegant, not something you throw together with the least thought possible. 
Here are some points to keep in mind as you strive for simplicity:This pithy XP saying sums up an important aspect of simple design: avoid speculative coding. 
Whenever you're tempted to add something to your design, ask yourself if it supports the stories and features you're currently delivering. 
If not, well... you aren't gonna need it. 
Your design could change. 
Your customers' minds could change.Similarly, remove code that's no longer in use. 
You'll make the design smaller, simpler, and easier to understand. 
If you need it again in the future, you can always get it out of version control. 
For now, it's a maintenance burden you don't need.We do this because excess code makes change difficult. 
Speculative design, added to make specific changes easy, often turns out to be wrong in some way, which actually makes changes more difficult. 
It's usually easier to add to a design than to fix a design that's wrong. 
The incorrect design has code that depends on it, sometimes locking bad decisions in place.Once and only once is a surprisingly powerful design guideline. 
As Martin Fowler said:22http://www.artima.com/intv/principlesP.html.There's even more to this idea than removing duplication. 
Think of it this way:Express every concept once. 
(And only once).33Thanks to Andrew Black for this insight.In other words, don't just eliminate duplication; make sure that every important concept has an explicit representation in your design. 
As [Hunt & Thomas] phrase their DRY Principle: "Every piece of knowledge must have a single, unambiguous, authoritative representation within a system."An effective way to make your code express itself once (and only once) is to be explicit about core concepts. 
Rather than expressing these concepts with an primitive data type, create a new type. 
For example, rather than representing dollar amounts with a decimal data type, create a Dollars class. 
(See Example.)Although using basic data types may seem simpler—it's one less class, after all—it actually makes your design more complex. 
Your code doesn't have a place for the concept. 
As a result, every time someone works with that concept, the code may need to reimplement basic behavior, such as string parsing and formatting, which results in widespread duplication. 
This duplication will likely be only fragments of code, but the net weight of those fragments will make your code hard to change. 
For example, if you decide to change the display of dollar amounts—perhaps you want negative amounts to be red—you must find every little fragment of formatting code and fix it.Instead, make sure that every concept has a home. 
Don't generalize; just make sure the basic concept has an explicit representation. 
Over time, as needed, add code (such as formatting and parsing) to your type. 
By starting with a simple but explicit representation of the concept, you provide a location for those future changes to congregate. 
Without it, they will tend to accumulate in other methods and lead to duplication and complex code.Simplicity is in the eye of the beholder. 
It doesn't matter much if you think the design is simple; if the rest of your team or future maintainers of your software find it too complicated, then it is.To avoid this problem, use idioms and patterns that are common for your language and team. 
It's okay to introduce new ideas, too, but run them past other team members first. 
Be sure to use names that clearly reflect the intent of your variables, methods, classes, and other entities.Pair programming will help you create simple code. 
If you have trouble understanding something your partner wrote, discuss the situation and try to find a better way to express the concept. 
Before you use a comment to explain something, ask your partner how to make the code express its intent without needing a comment.A hidden source of duplication lies in calls to third-party components. 
When you have these calls spread throughout your code, replacing or augmenting that component becomes difficult. 
If you discover that the component isn't sufficient for your needs, you could be in trouble.To prevent this problem, isolate your third-party components behind an interface that you control. 
Ask yourself, "When I need to upgrade or change this component, how hard will it be?"  In object-oriented languages, consider using the Adapter pattern [Gamma et. al.] rather than instantiating third-party classes directly. 
For frameworks that require that you extend their classes, create your own base classes that extend the framework classes, rather than extending the classes directly.Create your adapter incrementally. 
Rather than supporting every feature of the component in your adapter, support only what you need today. 
Write the adapter's interface to match your needs, not the interface of the component. 
This will make it easier to use and to replace when necessary.Isolating third-party components reduces duplication at the cost of making your code slightly more complex. 
Some components, such as Java's J2SE or the .NET framework, are so pervasive that isolating them makes little sense. 
 Make the decision to isolate common components according to the risk that you'll need to replace or augment that component. 
For example, I would use the Java or .NET String class directly, without an adapter, but I might consider isolating .NET's cryptography libraries or elements of the J2EE framework.Published interfaces limit your ability to make changes. 
Once the public interface to a class or other module is published so that people outside the team may use it, changing it requires great expense and effort. 
Because other people might be using the interface, changing it can sabotage their efforts.Some teams approach design as if every public interface were also a published interface. 
This internal publication assumes that, once defined, a public interface should never change. 
This is a bad idea—it prevents you from improving your design over time. 
A better approach is to change your nonpublished interfaces whenever you need, updating callers accordingly.If your code is used outside your team, then you do need published interfaces. 
Each one, however, is a commitment to a design decision that you may wish to change in the future. 
Minimize the number of interfaces you expose to the outside world and ask if the benefit of having other teams use your code is really worth the cost.4  (Sometimes it is, but don't automatically assume so.)  Postpone publishing interfaces as long as possible to allow your design to improve and settle.4[Brooks] estimated that making code externally reusable increases costs threefold. 
That estimate probably doesn't apply to modern development, but there's still a nontrivial cost associated with creating reusable components. 
"Object-oriented" doesn't mean "automatic reuse," despite early claims to the contrary.In some cases, as with teams creating a library for third-party use, the entire purpose of the project is to create a published interface. 
In that case, your API is your product. 
Still, the smaller the interface, the better—it's much easier to add new elements to your API than to remove or change incorrect elements. 
Make the interface as small as is practical.As Erich Gamma said in his interview, "When it comes to exposing more API [in Eclipse, the open source Java IDE], we do that on demand. We expose API gradually... when we see the need, then we say, okay, we'll make the investment of publishing this as an API, make it a commitment. So I really think about it in smaller steps, we do not want to commit to an API before its time. "One of the pitfalls of simple design is that your design will be incomplete. 
Some elements won't work because no one has needed them before.To prevent these gaps from being a problem, write your code to fail fast. 
Use assertions to signal the limits of your design; if someone tries to use something that isn't implemented, the assertion will cause his tests to fail.In XP, the plan can change every week. 
Unless you're implementing the feature that very week, don't put the hook in. 
The plan could change, leaving you stuck with unneeded code.Plus, if you're using incremental design and architecture properly, your code will actually be easier to modify in the future than it is today. 
Saving the change for later will save time and money.A simple design should make arbitrary changes possible by reducing duplication and limiting the scope of changes. 
If ignoring a potential feature could make it more difficult, you should look for ways of eliminating that risk without explicitly coding support for the feature. 
Incremental Design And Architecture, later in this chapter, has more about risk-driven architecture.When you create simple designs, you avoid adding support for any features other than the ones you're working on in the current iteration. 
You finish work more quickly as a result. 
When you use simple design well, your design supports arbitrary changes easily. 
Although new features might require a lot of new code, changes to existing code are localized and straightforward.Simple design requires continuous improvement through refactoring and incremental design and architecture. 
Without it, your design will fail to evolve with your requirements.Don't use simple design as an excuse for poor design. 
Simplicity requires careful thought. 
As the Einstein quote at the beginning of this section says, it's a lot easier to create complex designs than simple ones. 
Don't pretend "simple" means "fastest" or "easiest."Pair programming and collective code ownership, though not strictly necessary for simple design, will help your team devote the brainpower needed to create truly simple designs.Until recently, the accepted best practice in design followed the advice Erich Gamma now disavows: "The key to maximizing reuse lies in anticipating new requirements and changes to existing requirements, and in designing your systems so they can evolve accordingly."  A team can have success with this approach, but it depends on how well they anticipate new requirements. 
If the team's expectations are too far off, they might need to rewrite a lot of code that was based on bad assumptions. 
Some changes may affect so much code that they're just considered impossible. 
If you follow this approach, it's best to hire designers who have a lot of experience in your specific industry. 
They're more likely to correctly anticipate changes.Martin Fowler has a collection of his excellent IEEE Design columns online at http://www.martinfowler.com/articles.html#IDAOPDBC. 
Many of these columns discuss core concepts that help in creating a simple design.The Pragmatic Programmer: From Journeyman to Master [Hunt & Thomas] contains a wealth of design information that will help you create simple, flexible designs. 
Practices of an Agile Developer [Subramaniam & Hunt] is its spiritual successor, offering similarly pithy advice, although with less emphasis on design and coding.Prefactoring [Pugh] also has good advice for creating simple, flexible designs."Fail Fast" [Shore 2004b] discusses that concept in more detail. 
It is available at http://www.martinfowler.com/ieeeSoftware/failFast.pdf.XP makes challenging demands of its programmers: every week, programmers should finish four to ten customer-valued stories. 
Every week, customers may revise the current plan and introduce entirely new stories—with no advance notice. 
This regimen starts with the first week of the project.In other words, as a programmer you must be able to produce customer value, from scratch, in a single week. 
No advance preparation is possible. 
You can't set aside several weeks for building a domain model or persistence framework: your customers need you to deliver completed stories.Fortunately, XP provides a solution for this dilemma: incremental design (also called evolutionary design) allows you to build technical infrastructure (such as domain models and persistence frameworks) incrementally, in small pieces, as you deliver stories.Incremental design applies the concepts introduced in test-driven development to all levels of design. 
Like test-driven development, developers work in small steps, proving each before moving to the next. 
This takes place in three parts: start by creating the simplest design that could possibly work, incrementally add to it as the needs of the software evolve, and continuously improve the design by reflecting on its strengths and weaknesses.To be specific, when you first create a design element—whether it's a new method, a new class, or a new architecture—be completely specific. 
Create a simple design that solves only the problem you face at the moment, no matter how easy it may seem to solve more general problems.This is difficult!  Experienced programmers think in abstractions. 
In fact, the ability to think in abstractions is often a sign of a good programmer. 
Coding for one specific scenario will seem strange, even unprofessional.Do it anyway. 
The abstractions will come. 
Waiting to make them will enable you to create designs that are simpler and more powerful.The second time you work with a design element, modify the design to make it more general—but only general enough to solve the two problems it needs to solve. 
Next, review the design and make improvements. 
Simplify and clarify the code.The third time you work with a design element, generalize it further—but again, just enough to solve the three problems at hand. 
A small tweak to the design is usually enough. 
It will be pretty general at this point. 
Again, review the design, simplify, and clarify.Continue this pattern. 
By the fourth or fifth time you work with a design element—be it a method, a class, or something bigger—you'll typically find that its abstraction is perfect for your needs. 
Best of all, because you allowed practical needs to drive your design, it will be simple yet powerful.Incremental design initially creates every design element—method, class, namespace, or even architecture—to solve a specific problem. 
Additional customer requests guide the incremental evolution of the design. 
This requires continuous attention to the design, albeit at different time-scales. 
Methods evolve in minutes; architectures evolve over months.No matter what level of design you're looking at, the design tends to improve in bursts. 
Typically, you'll implement code into the existing design for several cycles, making minor changes as you go. 
Then something will give you an idea for a new design approach, requiring a series of refactorings to support it. 
[Evans] calls this a breakthrough (see Figure). 
Breakthroughs happen at all levels of the design, from methods to architectures.Breakthroughs are the result of important insights and lead to substantial improvements to the design. 
(If they don't, they're not worth implementing.)  You can see a small, method-scale breakthrough at the end of "A TDD Example" earlier in this chapter.You've seen this level of incremental design before: it's test-driven development. 
While the driver implements, the navigator thinks about the design. 
She looks for overly complex code and missing elements, which she writes on her notecard. 
She thinks about which features the code should support next, what design changes might be necessary, and which tests may guide the code in the proper direction. 
During the refactoring step of TDD, both members of the pair look at the code, discuss opportunities for improvements, and review the navigator's notes.Method refactorings happen every few minutes. 
Breakthroughs may happen several times per hour and could take ten minutes or more to complete.When TDD is done well, the design of individual classes and methods is beautiful: they're simple, elegant, and easy to use. 
This isn't enough. 
Without attention to the interaction between classes, the overall system design will be muddy and confusing.During TDD, the navigator should also consider the wider scope. 
Ask yourself these questions: are there similarities between the code you're implementing and other code in the system?  Are class responsibilities clearly defined and concepts clearly represented?  How well does this class interact with other classes?When you see a problem, jot it down on your card. 
During one of the refactoring steps of TDD—usually, when you're not in the middle of something else—bring up the issue, discuss solutions with your partner, and refactor. 
If you feel that your design change will significantly affect other members of the team, take a quick break to discuss it around a whiteboard.Class-level refactorings happen several times per day. 
Depending on your design, breakthroughs may happen a few times per week and can take several hours to complete. 
(Nonetheless, remember to proceed in small, test-verified steps.)  Use your iteration slack to complete breakthrough refactorings. 
In some cases, you'll won't have time to finish all of the refactorings you identify. 
That's okay; as long as the design is better at the end of the week than it was at the beginning, you're doing enough.Large programs use overarching organizational structures called architecture. 
For example, many programs segregate user interface classes, business logic classes, and persistence classes into their own namespaces; this is a classic three-layer architecture. 
Other designs have the application pass the flow of control from one machine to the next in an n-tier architecture.These architectures are implemented through the use of recurring patterns. 
These aren't design patterns in the formal Gang of Four1 sense. 
Instead, they're standard conventions specific to your codebase. 
For example, in a three-layer architecture, every business logic class will probably be part of a "business logic" namespace, may inherit from a generic "business object" base class, and probably interfaces with its persistence layer counterpart in a standard way.1The "Gang of Four" is a common nickname for the authors of Design Patterns, a book that introduced design patterns to the mainstream.These recurring patterns embody your application architecture. 
Although they lead to consistent code, they're also a form of duplication, which makes changes to your architecture more difficult.Fortunately, you can design architectures incrementally. 
As with other types of continuous design, use TDD and pair programming as your primary vehicle. 
While your software grows, be conservative in introducing new architectural patterns: introduce just what you need to for the amount of code you have and the features you support at the moment. 
Before introducing a new pattern, ask yourself if the duplication is really necessary. 
Perhaps there's some language feature you can use that will reduce your need to rely on the pattern.In my experience, breakthroughs in architecture happen every few months. 
(This estimate will vary widely depending on your team members and code quality.)  Refactoring to support the breakthrough can take several weeks or longer because of the amount of duplication involved. 
Although changes to your architecture may be tedious, they usually aren't difficult once you've identified the new architectural pattern. 
Start by trying out the new pattern in just one part of your design. 
Let it sit for a while—a week or two—to make sure the change works well in practice. 
Once you're sure that it does, bring the rest of the system into compliance with the new structure. 
Refactor each class that you touch as you perform your everyday work and use some of your slack in each iteration to fix other classes.Keep delivering stories while you refactor. 
Although you could take a break from new development to refactor, that would disenfranchise your customers. 
Balance technical excellence with delivering value. 
Neither can take precedence over the other. 
This may lead to inconsistencies within the code during the changeover, but fortunately, that's mostly an aesthetic problem—more annoying than problematic.Architecture may seem too essential not to design up front. 
Some problems do seem too expensive to solve incrementally, but I've found that nearly everything is easy to change if you eliminate duplication and embrace simplicity. 
Common thought is that distributed processing, persistence, internationalization, security, and transaction structure are so complex that you must consider them from the start of your project. 
I disagree; I've dealt with all of them incrementally [Shore 2004a].Of course, no design is perfect. 
Even with simple design, some of your code will contain duplication, and some will be too complex. 
There's always more refactoring to do than time to do it. 
That's where risk-driven architecture comes in.Although I've emphasized designing for the present, it's okay to think about future problems. 
Just don't implement any solutions to stories that you haven't yet scheduled.What do you do when you see a hard problem coming?  For example, what if you know that internationalizing your code is expensive and only going to get more expensive?  Your power lies in your ability to chooose which refactorings to work on. 
Although it would be inappropriate to implement features your customers haven't asked for, you can direct your refactoring efforts towards reducing risk. 
Anything that improves the current design is okay—so choose improvements that also reduce future risk.To apply risk-driven architecture, consider what it is about your design that concerns you and eliminate duplication around those concepts. 
For example, if your internationalization concern is that you always format numbers, dates, and other variables in the local style, look for ways to reduce duplication in your variable formatting. 
One way to do so would be to make sure every concept has its own class (as described in Simple Design earlier in this chapter), then condense all formatting around each concept into a single method within each class, as shown in Figure. 
If there's still a lot of duplication, the Strategy pattern would allow you to condense the formatting code even further.Limit your efforts to improving your existing design. 
Don't actually implement support for localized formats until your customers ask for them. 
Once you've eliminated duplication around a concept—for example, once there's only one method in your entire system that renders numbers as strings—changing its implementation will be just as easy later as it is now.2We did have to make it thread-safe, so it wasn't entirely trivial.Although incremental design focuses heavily on test-driven development and refactoring as an enabler, it isn't about coding. 
When you use TDD, incremental design, and pair programming well, every pairing session involves a lot of conversation about design. 
In fact, that's what all the (relevant) conversations are about. 
As Ron Jeffries likes to say, design is so important in XP that we do it all the time. 
Some of the design discussions are very detailed and nitpicky, such as "What should we name this method?"  Others are much higher level, such as "These two classes share some responsibilities. 
We should split them apart and make a third class."Have design discussions away from the keyboard as often as you think is necessary, and use whatever modelling techniques you find helpful. 
Try to keep them informal and collaborative; sketches on a whiteboard work well. 
Some people like to use CRC (Class, Responsibility, Collaborator) cards.Some of your discussions will be predictive: you'll discuss how you can change your design to support some feature that you haven't yet added to the code. 
Others will be reflective: you'll discuss how to change your design to better support existing features.Reflective design (discussed in more detail in Refactoring earlier in this chapter) is always helpful in XP. 
I like to sketch UML diagrams on a whiteboard to illustrate problems in the current design and possible solutions. 
When my teams discover a breakthrough refactoring at the class or architecture level, we gather around a whiteboard to discuss it and sketch our options.Predictive design is less helpful in XP, but it's still a useful tool. 
As you're adding a new feature, use it to help you decide which tests to write next in TDD. 
At a larger scale, use predictive design to consider risks and perform risk-driven architecture.The trick to using predictive design in XP is keeping your design simple and focusing only on the features it currently supports. 
Although it's okay to predict how your design will change when you add new features, you shouldn't actually implement those changes until you're working on the stories in question. 
When you do, you should keep your mind open to other ways of implementing those features. 
Sometimes the act of coding with test-driven development will reveal possibilities you hadn't considered.Given these caveats, I find that I use predictive design less and less as I become more experienced with incremental design. 
That's not because it's "against the rules"—I'm perfectly happy breaking rules—but because working incrementally and reflectively has genuinely yielded better results for me.Try it yourself, and find the balance between predictive and reflective design that works best for you.Just the opposite, actually, in my experience. 
There are two reasons for this. 
First, because incremental design implements just enough code to support the current requirements, you start delivering features much more quickly with incremental design. 
Second, when a predicted requirement changes, you haven't coded any parts of the design to support it, so you haven't wasted any effort.Even if requirements never changed, incremental design would still be more effective, as it leads to design breakthroughs on a regular basis. 
Each breakthrough allows you to see new possibilities and eventually leads to another breakthrough—sort of like walking through a hilly forest in which the top of each hill reveals a new, higher hill that you couldn't see before. 
This ongoing series of breakthroughs substantially improves your design.Sometimes a breakthrough will lead you to see a completely new way of approaching your design. 
In this case, refactoring may seem like backtracking. 
This happens to everyone and is not a bad thing. The nature of breakthroughs—especially at the class and architectural level—is that you usually don't see them until after you've lived with the current design for a while.Ask them to schedule it with a story, then estimate and deliver it as you would any other story. 
Remind them that the design will change over time. 
The most effective option is to schedule documentation stories for the last iteration.If your organization requires up-front design documentation, the only way to provide it is to engage in up-front design. 
Try to keep your design efforts small and simple. 
If you can, use incremental design once you actually start coding.When you use incremental design well, every iteration advances the software's features and design in equal measure. 
You have no need to skip coding for an iteration for refactoring or design. 
Every week, the quality of the software is better than it was the week before. 
As time goes on, the software becomes increasingly easy to maintain and extend.Incremental design requires simple design and constant improvement. 
Don't try to use incremental design without a commitment to continuous daily improvement (in XP terms, merciless refactoring.)  This requires self-discipline and a strong desire for high-quality code from at least one team member. Because nobody can do that all the time, pair programming, collective code ownership, energized work, and slack are important support mechanisms.Test-driven development is also important for incremental design. 
Its explicit refactoring step, repeated every few minutes, gives pairs continual opportunities to stop and make design improvements. 
Pair programming helps in this area, too, by making sure that half of the team's programmers—as navigators—always have an opportunity to consider design improvements.Be sure your team sits together and communicates well if you're using incremental design. 
Without constant communication about class and architectural refactorings, your design will fragment and diverge. 
Agree on coding standards so that everyone follows the same patterns.Anything that makes continuous improvement difficult will make incremental design difficult. 
Published interfaces are an example; because they are difficult to change after publication, incremental design may not be appropriate for published interfaces. 
(You can still use incremental design for the implementation of those interfaces.)  Similarly, any language or platform that makes refactoring difficult will also inhibit your use of incremental design.Finally, some organizations place organizational rather than technical impediments on refactoring, as with organizations that require up-front design documentation or have rigidly controlled database schemas. 
Incremental design may not be appropriate in these situations.If you are uncomfortable with XP's approach to incremental design, you can hedge your bets by combining it with up-front design. 
Start with an up-front design stage and then commit completely to XP-style incremental design. 
Although it will delay the start of your first iteration (and may require some up-front requirements work, too), this approach has the advantage of providing a safety net without incurring too much risk.There are other alternatives to XP's approach to incremental design, but I don't think they would work well with XP. 
One option is to use another type of incremental design, one more like up-front design, that does some up-front design at the beginning of every iteration, rather than relying on simple design and refactoring to the extent that XP does.I haven't tried another incremental design approach with XP because they seem to interact clumsily with XP's short iterations. 
The design sessions seem like they would be too short and small to create a cohesive architecture on their own. 
Without XP's focus on simple design and merciless refactoring, a single design might not evolve.Another alternative is to design everything up-front. 
This could work in an environment with very few requirements changes (or a prescient designer), but it's likely to break down with XP's adaptive plans and tiered planning horizons."Is Design Dead?" [Fowler 2000], online at http://www.martinfowler.com/articles/designDead.html, discusses evolutionary design from a slightly skeptical perspective."Continuous Design" [Shore 2004a] discusses my experiences with difficult challenges in incremental design, such as internationalization and security. 
It is available at http://www.martinfowler.com/ieeeSoftware/continuousDesign.pdf.You've probably noticed by now that XP values concrete data over speculation. 
Whenever you're faced with a question, don't speculate about the answer—conduct an experiment!  Figure out how you can use real data to make progress.That's what spike solutions are for, too.A spike solution, or spike, is a technical investigation. 
It's a small experiment to research the answer to a problem. 
For example, a programmer might not know whether Java throws an exception on arithmetic overflow. 
A quick ten-minute spike will answer the question.The best way to implement a spike is usually to create a small program or test that demonstrates the feature in question. 
You can read as many books and tutorials as you like, but it's my experience that nothing helps me understand a problem more than writing working code. 
It's important to work from a practical point of view, not just a theoretical one.Writing code, however, often takes longer than reading a tutorial. 
Reduce that time by writing small, standalone programs. 
You can ignore all the complexities necessary to write production code—just focus on getting something working. 
Run from the command-line or your test framework. 
Hardcode values. 
Ignore user input, unless absolutely necessary. 
I often end up with programs a few dozen lines long that run almost everything from main().Of course, this approach means that you can't reuse this code in your actual production codebase, as you didn't develop it with your the normal discipline and care. 
That's fine. 
It's an experiment. 
When you finish, throw it away, check it in as documentation, or share it with your colleagues, but don't treat it as anything other than an experiment.Most spikes are performed on the spur of the moment. 
You see a need to clarify a small technical issue, and you write a quick spike to do so. 
If the spike takes more than a few minutes, your iteration slack absorbs the cost.If you anticipate the need for a spike when estimating a story, include the time in your story estimate. 
Sometimes, you won't be able to estimate a story at all until you've done your research; in this case, create a spike story and estimate that instead (see Stories in Chapter 8).Exactly. 
Production code should never catch Throwable, but a spike isn't production code. 
Spike solutions are the one time that you can forget about writing good code and just focus on short-term results. 
(That said, for larger spikes, you may find that code that's too sloppy is hard to work with and slows you down.)It's up to you. 
Because spikes aren't production code, even teams with strict pair programming rules (see Pair Programming) don't require writing spikes in pairs.One very effective way to pair on a spike is to have one person research the technology while the other person codes. 
Another option is for both people to work independently on separate approaches, each doing their own research and coding, then coming back together to review progress and share ideas.Unless you think someone will refer to it again later, toss it. 
Remember, the purpose of a spike solution is to give you the information and experience to know how to solve a problem, not to produce the code that solves it.Perform a spike whenever you have a question about if or how some piece of technology will work.That's good; it gives you more information. 
Perhaps the customer will reconsider the value of the feature, or perhaps you need to think of another way to accomplish what you want.Once, a customer asked me for a feature I thought might work in a certain way, but my spike demonstrated that the relevant Internet standard actually prohibited the desired behavior. 
We came up with a different design for implementing the larger feature.When you clarify technical questions with well-directed, isolated experiments, you spend less time speculating about how your program will work. 
You focus your debugging and exploratory programming on the problem at hand rather than the ways in which your production code might be interacting with your experiment.Avoid the temptation to create useful or generic programs out of your spikes. 
Focus your work on answering a specific technical question, and stop working on the spike as soon as it answers that question. 
Similarly, there's no need to create a spike when you already understand a technology well.Don't use spikes as an excuse to avoid disciplined test-driven development and refactoring. 
Never copy spike code into production code. 
Even if it is exactly what you need, rewrite it using test-driven development so that it meets your production code standards.Spike solutions are a learning technique based on performing small, concrete experiments. 
Some people perform these experiments in their production code, which can work well for small experiments (such as the arithmetic overflow example), but it increases the scope of possible error. 
If something doesn't work as expected, is it because your understanding of the technology is wrong?  Is it due to an unseen interaction with the production code or test framework?  Standalone spikes eliminate this uncertainty.For stories that you can't estimate accurately, an alternative to scheduling a spike story is to provide a high estimate. 
This is risky, because some stories will take longer than your highest estimate, and some may not be possible at all.Another option is to research problems by reading about the underlying theory and finding code snippets in books or online. 
This is often a good way to get started on a spike, but the best way to really understand what's going on is to create your own spike. 
Simplify and adapt the example. 
Why does it work?  What happens when you change default parameters?  Use the spike to clarify your understanding.